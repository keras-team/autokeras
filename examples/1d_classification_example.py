import numpy
from autokeras.image1D_supervised import Image1DClassifier
import urllib.request
import os

# fix random seed for reproducibility
numpy.random.seed(7)

# load the widely-used pima indians dataset example file

pima_url="https://gist.githubusercontent.com/ktisha/c21e73a1bd1700294ef790c56c8aec1f/raw/819b69b5736821ccee93d05b51de0510bea00294/pima-indians-diabetes.csv"
pimafile = "./pima-indians-diabetes.csv"

#Contributed by Werner G. Krebs, Ph.D. of Acculation Inc in the hopes that it will be useful and 
#inspire others to contribute a better 1D auto-classification module.

#(C)2018 Acculation Inc

#This is FREE SOFTWARE Contributed under MIT's software license. AS SUCH IT PROVIDED WITHOUT WARRANTY
#OF ANY KIND. See the MIT LICENSE file for complete legal details.

#Example Demonstration of classic Pima classification problem using the contributed naive
#1D automatic classifier.

#On this particular example, the very simple 3 layer 1D Dense classifier from https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/
#and other places
#will EASILY beat the models generated by Image1DClassifier (which is ultimately using
#network topologies optimized for 2D images, which don't work that well on 1D data.)

#The dense network topology optimized for 1D from the example website above
#does at least 10% better than typical results from this
#example run with a 300 second timelimit.

#On the other hand, someone did need to manually find even that very simple dense network.
#With the example below, autokeras does automatically find and optimize various network topologies.

#This class is contributed as a working 1D classification baseline in the hopes that it will 
#inspire others to improve upon autokeras and this 1D baseline implementation. Some hints for low-hanging
#fruit to further improve 1D classification are given in the comments to the Image1DClassifier

#And note a generalized classifier can now be created from this by having code that temporarily remaps
#string classification labels into auto-assigned numeric equivalents, then calls this 1D classification class.

#The idea of AutoML is a fast test to see if there are other things that could be tried to
#beat a classifier that has been manually tuned by a data scientist. In that sense,
#the classifiers in autokeras currently fall short because they are limited primarily to 
#neural network classifiers (whereas you'd want to test regression classifiers, Bayesian Networks,
#Decision Trees, etc.)

if not os.path.exists(pimafile):
    with urllib.request.urlopen(pima_url) as response:
        with open(pimafile,'b+w') as outfile:
            outfile.write(response.read())

dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
numpy.random.shuffle(dataset) #For OOS testing. data are often not in random order.
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]

x_train = X[0:-70,:]
y_train = Y[0:-70]

#out of sample testing, roughly 10% of data, randomly selected via the shuffle above.
x_test = X[-70:,:]
y_test = Y[-70:]

del X
del Y

if __name__ == '__main__':
    clf = Image1DClassifier(verbose=True)
    clf.fit(x_train, y_train, time_limit=300)
    clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)
#export model if desired.
#    clf.load_searcher().load_best_model().produce_keras_model().save('test_save.h5')
    y = clf.evaluate(x_test, y_test)
    print(y)
