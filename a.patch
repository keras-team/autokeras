diff --git a/autokeras/test_utils.py b/autokeras/test_utils.py
index 951b203..1cf4eeb 100644
--- a/autokeras/test_utils.py
+++ b/autokeras/test_utils.py
@@ -13,14 +13,46 @@
 # limitations under the License.
 
 import inspect
+import os
 
-import keras
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 import autokeras as ak
 
 SEED = 5
+COLUMN_NAMES = [
+    "sex",
+    "age",
+    "n_siblings_spouses",
+    "parch",
+    "fare",
+    "class",
+    "deck",
+    "embark_town",
+    "alone",
+]
+COLUMN_TYPES = {
+    "sex": "categorical",
+    "age": "numerical",
+    "n_siblings_spouses": "categorical",
+    "parch": "categorical",
+    "fare": "numerical",
+    "class": "categorical",
+    "deck": "categorical",
+    "embark_town": "categorical",
+    "alone": "categorical",
+}
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+TRAIN_CSV_PATH = keras.utils.get_file(
+    fname=os.path.basename(TRAIN_DATA_URL), origin=TRAIN_DATA_URL
+)
+TEST_CSV_PATH = keras.utils.get_file(
+    fname=os.path.basename(TEST_DATA_URL), origin=TEST_DATA_URL
+)
 
 
 def generate_data(num_instances=100, shape=(32, 32, 3), dtype="np"):
@@ -67,6 +99,26 @@ def generate_text_data(num_instances=100):
     )
 
 
+def generate_data_with_categorical(
+    num_instances=100,
+    num_numerical=10,
+    num_categorical=3,
+    num_classes=5,
+    dtype="np",
+):
+    categorical_data = np.random.randint(
+        num_classes, size=(num_instances, num_categorical)
+    )
+    numerical_data = np.random.rand(num_instances, num_numerical)
+    data = np.concatenate((numerical_data, categorical_data), axis=1)
+    if data.dtype == np.float64:
+        data = data.astype(np.float32)
+    if dtype == "np":
+        return data
+    if dtype == "dataset":
+        return tf.data.Dataset.from_tensor_slices(data)
+
+
 def build_graph():
     keras.backend.clear_session()
     image_input = ak.ImageInput(shape=(32, 32, 3))
diff --git a/autokeras/tuners/greedy_test.py b/autokeras/tuners/greedy_test.py
index ba47e4c..947425c 100644
--- a/autokeras/tuners/greedy_test.py
+++ b/autokeras/tuners/greedy_test.py
@@ -14,8 +14,8 @@
 
 from unittest import mock
 
-import keras
 import keras_tuner
+from tensorflow import keras
 
 import autokeras as ak
 from autokeras import test_utils
diff --git a/autokeras/tuners/task_specific.py b/autokeras/tuners/task_specific.py
index 5e2a17c..6d5de89 100644
--- a/autokeras/tuners/task_specific.py
+++ b/autokeras/tuners/task_specific.py
@@ -73,13 +73,84 @@ IMAGE_CLASSIFIER = [
 
 TEXT_CLASSIFIER = [
     {
+        "text_block_1/block_type": "vanilla",
+        "classification_head_1/dropout": 0,
+        "text_block_1/max_tokens": 5000,
+        "text_block_1/conv_block_1/separable": False,
+        "text_block_1/text_to_int_sequence_1/output_sequence_length": 512,
+        "text_block_1/embedding_1/pretraining": "none",
+        "text_block_1/embedding_1/embedding_dim": 64,
+        "text_block_1/embedding_1/dropout": 0.25,
+        "text_block_1/conv_block_1/kernel_size": 5,
+        "text_block_1/conv_block_1/num_blocks": 1,
+        "text_block_1/conv_block_1/num_layers": 1,
+        "text_block_1/conv_block_1/max_pooling": False,
+        "text_block_1/conv_block_1/dropout": 0,
+        "text_block_1/conv_block_1/filters_0_0": 256,
+        "text_block_1/spatial_reduction_1/reduction_type": "global_max",
+        "text_block_1/dense_block_1/num_layers": 1,
+        "text_block_1/dense_block_1/use_batchnorm": False,
+        "text_block_1/dense_block_1/dropout": 0.5,
+        "text_block_1/dense_block_1/units_0": 256,
+        "optimizer": "adam",
+        "learning_rate": 1e-3,
+    },
+    {
+        "text_block_1/block_type": "transformer",
+        "classification_head_1/dropout": 0,
+        "optimizer": "adam",
+        "learning_rate": 1e-3,
+        "text_block_1/max_tokens": 20000,
+        "text_block_1/text_to_int_sequence_1/output_sequence_length": 200,
+        "text_block_1/transformer_1/pretraining": "none",
+        "text_block_1/transformer_1/embedding_dim": 32,
+        "text_block_1/transformer_1/num_heads": 2,
+        "text_block_1/transformer_1/dense_dim": 32,
+        "text_block_1/transformer_1/dropout": 0.25,
+        "text_block_1/spatial_reduction_1/reduction_type": "global_avg",
+        "text_block_1/dense_block_1/num_layers": 1,
+        "text_block_1/dense_block_1/use_batchnorm": False,
+        "text_block_1/dense_block_1/dropout": 0.5,
+        "text_block_1/dense_block_1/units_0": 20,
+    },
+    {
+        "text_block_1/block_type": "bert",
         "classification_head_1/dropout": 0,
         "optimizer": "adam_weight_decay",
         "learning_rate": 2e-5,
         "text_block_1/bert_block_1/max_sequence_length": 512,
+        "text_block_1/max_tokens": 20000,
     },
 ]
 
+STRUCTURED_DATA_CLASSIFIER = [
+    {
+        "structured_data_block_1/normalize": True,
+        "structured_data_block_1/dense_block_1/num_layers": 2,
+        "structured_data_block_1/dense_block_1/use_batchnorm": False,
+        "structured_data_block_1/dense_block_1/dropout": 0,
+        "structured_data_block_1/dense_block_1/units_0": 32,
+        "structured_data_block_1/dense_block_1/units_1": 32,
+        "classification_head_1/dropout": 0.0,
+        "optimizer": "adam",
+        "learning_rate": 0.001,
+    }
+]
+
+STRUCTURED_DATA_REGRESSOR = [
+    {
+        "structured_data_block_1/normalize": True,
+        "structured_data_block_1/dense_block_1/num_layers": 2,
+        "structured_data_block_1/dense_block_1/use_batchnorm": False,
+        "structured_data_block_1/dense_block_1/dropout": 0,
+        "structured_data_block_1/dense_block_1/units_0": 32,
+        "structured_data_block_1/dense_block_1/units_1": 32,
+        "regression_head_1/dropout": 0.0,
+        "optimizer": "adam",
+        "learning_rate": 0.001,
+    }
+]
+
 
 class ImageClassifierTuner(greedy.Greedy):
     def __init__(self, **kwargs):
@@ -89,3 +160,13 @@ class ImageClassifierTuner(greedy.Greedy):
 class TextClassifierTuner(greedy.Greedy):
     def __init__(self, **kwargs):
         super().__init__(initial_hps=TEXT_CLASSIFIER, **kwargs)
+
+
+class StructuredDataClassifierTuner(greedy.Greedy):
+    def __init__(self, **kwargs):
+        super().__init__(initial_hps=STRUCTURED_DATA_CLASSIFIER, **kwargs)
+
+
+class StructuredDataRegressorTuner(greedy.Greedy):
+    def __init__(self, **kwargs):
+        super().__init__(initial_hps=STRUCTURED_DATA_REGRESSOR, **kwargs)
diff --git a/autokeras/tuners/task_specific_test.py b/autokeras/tuners/task_specific_test.py
index f23b1cd..ffafee4 100644
--- a/autokeras/tuners/task_specific_test.py
+++ b/autokeras/tuners/task_specific_test.py
@@ -59,7 +59,7 @@ def test_img_clf_init_hp2_equals_hp_of_a_model(tmp_path):
     assert set(init_hp.keys()) == set(hp._hps.keys())
 
 
-def test_txt_clf_init_hp0_equals_hp_of_a_model(tmp_path):
+def test_txt_clf_init_hp2_equals_hp_of_a_model(tmp_path):
     clf = ak.TextClassifier(directory=tmp_path)
     clf.inputs[0].shape = (1,)
     clf.inputs[0].batch_size = 6
@@ -67,10 +67,69 @@ def test_txt_clf_init_hp0_equals_hp_of_a_model(tmp_path):
     clf.outputs[0].in_blocks[0].shape = (10,)
     clf.tuner.hypermodel.epochs = 1000
     clf.tuner.hypermodel.num_samples = 20000
+    init_hp = task_specific.TEXT_CLASSIFIER[2]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
+    clf.tuner.hypermodel.build(hp)
+
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_txt_clf_init_hp1_equals_hp_of_a_model(tmp_path):
+    clf = ak.TextClassifier(directory=tmp_path)
+    clf.inputs[0].shape = (1,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
+    init_hp = task_specific.TEXT_CLASSIFIER[1]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
+    clf.tuner.hypermodel.build(hp)
+
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_txt_clf_init_hp0_equals_hp_of_a_model(tmp_path):
+    clf = ak.TextClassifier(directory=tmp_path)
+    clf.inputs[0].shape = (1,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
     init_hp = task_specific.TEXT_CLASSIFIER[0]
     hp = keras_tuner.HyperParameters()
     hp.values = copy.copy(init_hp)
 
+    clf.tuner.hypermodel.build(hp)
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_sd_clf_init_hp0_equals_hp_of_a_model(tmp_path):
+    clf = ak.StructuredDataClassifier(
+        directory=tmp_path,
+        column_names=["a", "b"],
+        column_types={"a": "numerical", "b": "numerical"},
+    )
+    clf.inputs[0].shape = (2,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
+    init_hp = task_specific.STRUCTURED_DATA_CLASSIFIER[0]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
+    clf.tuner.hypermodel.build(hp)
+
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_sd_reg_init_hp0_equals_hp_of_a_model(tmp_path):
+    clf = ak.StructuredDataRegressor(
+        directory=tmp_path,
+        column_names=["a", "b"],
+        column_types={"a": "numerical", "b": "numerical"},
+    )
+    clf.inputs[0].shape = (2,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
+    init_hp = task_specific.STRUCTURED_DATA_REGRESSOR[0]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
     clf.tuner.hypermodel.build(hp)
 
     assert set(init_hp.keys()) == set(hp._hps.keys())
diff --git a/autokeras/utils/data_utils.py b/autokeras/utils/data_utils.py
index 06a0630..94c25c6 100644
--- a/autokeras/utils/data_utils.py
+++ b/autokeras/utils/data_utils.py
@@ -12,15 +12,13 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import numpy as np
 import tensorflow as tf
-import tree
-from keras import ops
+from tensorflow import nest
 
 
 def batched(dataset):
-    shape = tree.flatten(dataset_shape(dataset))[0]
+    shape = nest.flatten(dataset_shape(dataset))[0]
     return len(shape) > 0 and shape[0] is None
 
 
@@ -62,23 +60,23 @@ def dataset_shape(dataset):
 
 
 def unzip_dataset(dataset):
-    return tree.flatten(
+    return nest.flatten(
         [
-            dataset.map(lambda *a: tree.flatten(a)[index])
-            for index in range(len(tree.flatten(dataset_shape(dataset))))
+            dataset.map(lambda *a: nest.flatten(a)[index])
+            for index in range(len(nest.flatten(dataset_shape(dataset))))
         ]
     )
 
 
 def cast_to_string(tensor):
-    if keras.backend.standardize_dtype(tensor.dtype) == "string":
-        return tensor  # pragma: no cover
+    if tensor.dtype == tf.string:
+        return tensor
     return tf.strings.as_string(tensor)
 
 
 def cast_to_float32(tensor):
-    if keras.backend.standardize_dtype(tensor.dtype) == "float32":
+    if tensor.dtype == tf.float32:
         return tensor
-    if keras.backend.standardize_dtype(tensor.dtype) == "string":
+    if tensor.dtype == tf.string:
         return tf.strings.to_number(tensor, tf.float32)
-    return ops.cast(tensor, "float32")  # pragma: no cover
+    return tf.cast(tensor, tf.float32)
diff --git a/autokeras/utils/io_utils_test.py b/autokeras/utils/io_utils_test.py
index 111ff3c..453908f 100644
--- a/autokeras/utils/io_utils_test.py
+++ b/autokeras/utils/io_utils_test.py
@@ -15,9 +15,9 @@
 import os
 import shutil
 
-import keras
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 
 from autokeras import test_utils
 from autokeras.utils import io_utils
diff --git a/autokeras/utils/layer_utils.py b/autokeras/utils/layer_utils.py
index 8245bf4..8da71f3 100644
--- a/autokeras/utils/layer_utils.py
+++ b/autokeras/utils/layer_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras import layers
+from tensorflow.keras import layers
 
 
 def get_global_average_pooling(shape):
@@ -44,7 +44,7 @@ def get_conv(shape):
 
 
 def get_sep_conv(shape):
-    return [  # pragma: no cover
+    return [
         layers.SeparableConv1D,
         layers.SeparableConv2D,
         layers.Conv3D,
diff --git a/autokeras/utils/types.py b/autokeras/utils/types.py
index b3bc674..b256e2f 100644
--- a/autokeras/utils/types.py
+++ b/autokeras/utils/types.py
@@ -19,8 +19,8 @@ from typing import Union
 
 import numpy as np
 import tensorflow as tf
-from keras.losses import Loss
-from keras.metrics import Metric
+from tensorflow.keras.losses import Loss
+from tensorflow.keras.metrics import Metric
 
 DatasetType = Union[np.ndarray, tf.data.Dataset]
 LossType = Union[str, Callable, Loss]
diff --git a/autokeras/utils/utils.py b/autokeras/utils/utils.py
index 26c2b25..bb85a63 100644
--- a/autokeras/utils/utils.py
+++ b/autokeras/utils/utils.py
@@ -13,15 +13,16 @@
 # limitations under the License.
 
 import re
+import warnings
 
-import keras
 import keras_tuner
 import tensorflow as tf
-import tree
+from packaging.version import parse
+from tensorflow import nest
 
 
 def validate_num_inputs(inputs, num):
-    inputs = tree.flatten(inputs)
+    inputs = nest.flatten(inputs)
     if not len(inputs) == num:
         raise ValueError(
             "Expected {num} elements in the inputs list "
@@ -35,6 +36,32 @@ def to_snake_case(name):
     return insecure
 
 
+def check_tf_version() -> None:
+    if parse(tf.__version__) < parse("2.7.0"):
+        warnings.warn(
+            "The Tensorflow package version needs to be at least 2.7.0 \n"
+            "for AutoKeras to run. Currently, your TensorFlow version is \n"
+            f"{tf.__version__}. Please upgrade with \n"
+            "`$ pip install --upgrade tensorflow`. \n"
+            "You can use `pip freeze` to check afterwards "
+            "that everything is ok.",
+            ImportWarning,
+        )
+
+
+def check_kt_version() -> None:
+    if parse(keras_tuner.__version__) < parse("1.1.0"):
+        warnings.warn(
+            "The Keras Tuner package version needs to be at least 1.1.0 \n"
+            "for AutoKeras to run. Currently, your Keras Tuner version is \n"
+            f"{keras_tuner.__version__}. Please upgrade with \n"
+            "`$ pip install --upgrade keras-tuner`. \n"
+            "You can use `pip freeze` to check afterwards "
+            "that everything is ok.",
+            ImportWarning,
+        )
+
+
 def contain_instance(instance_list, instance_type):
     return any(
         [isinstance(instance, instance_type) for instance in instance_list]
@@ -122,10 +149,24 @@ def add_to_hp(hp, hps, name=None):
 
 
 def serialize_keras_object(obj):
-    return keras.utils.serialize_keras_object(obj)  # pragma: no cover
+    if hasattr(tf.keras.utils, "legacy"):
+        return tf.keras.utils.legacy.serialize_keras_object(
+            obj
+        )  # pragma: no cover
+    else:
+        return tf.keras.utils.serialize_keras_object(obj)  # pragma: no cover
 
 
-def deserialize_keras_object(config, module_objects=None, custom_objects=None):
-    return keras.utils.deserialize_keras_object(
-        config, custom_objects, module_objects
-    )
+def deserialize_keras_object(
+    config, module_objects=None, custom_objects=None, printable_module_name=None
+):
+    if hasattr(tf.keras.utils, "legacy"):
+        return (
+            tf.keras.utils.legacy.deserialize_keras_object(  # pragma: no cover
+                config, custom_objects, module_objects, printable_module_name
+            )
+        )
+    else:
+        return tf.keras.utils.deserialize_keras_object(  # pragma: no cover
+            config, custom_objects, module_objects, printable_module_name
+        )
diff --git a/autokeras/utils/utils_test.py b/autokeras/utils/utils_test.py
index 668a102..c589547 100644
--- a/autokeras/utils/utils_test.py
+++ b/autokeras/utils/utils_test.py
@@ -27,6 +27,32 @@ def test_validate_num_inputs_error():
     assert "Expected 2 elements in the inputs list" in str(info.value)
 
 
+def test_check_tf_version_error():
+    utils.tf.__version__ = "2.1.0"
+
+    with pytest.warns(ImportWarning) as record:
+        utils.check_tf_version()
+
+    assert len(record) == 1
+    assert (
+        "Tensorflow package version needs to be at least"
+        in record[0].message.args[0]
+    )
+
+
+def test_check_kt_version_error():
+    utils.keras_tuner.__version__ = "1.0.0"
+
+    with pytest.warns(ImportWarning) as record:
+        utils.check_kt_version()
+
+    assert len(record) == 1
+    assert (
+        "Keras Tuner package version needs to be at least"
+        in record[0].message.args[0]
+    )
+
+
 def test_run_with_adaptive_batch_size_raise_error():
     def func(**kwargs):
         raise tf.errors.ResourceExhaustedError(0, "", None)
diff --git a/benchmark/experiments/image.py b/benchmark/experiments/image.py
index 8f2f377..67a32b4 100644
--- a/benchmark/experiments/image.py
+++ b/benchmark/experiments/image.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras import datasets
+from tensorflow.keras import datasets
 
 import autokeras as ak
 from benchmark.experiments import experiment
diff --git a/benchmark/experiments/structured_data.py b/benchmark/experiments/structured_data.py
new file mode 100644
index 0000000..27a3675
--- /dev/null
+++ b/benchmark/experiments/structured_data.py
@@ -0,0 +1,116 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import numpy as np
+import pandas as pd
+import sklearn
+import tensorflow as tf
+
+import autokeras as ak
+from benchmark.experiments import experiment
+
+
+class StructuredDataClassifierExperiment(experiment.Experiment):
+    def get_auto_model(self):
+        return ak.StructuredDataClassifier(
+            max_trials=10, directory=self.tmp_dir, overwrite=True
+        )
+
+
+class Titanic(StructuredDataClassifierExperiment):
+    def __init__(self):
+        super().__init__(name="Titanic")
+
+    @staticmethod
+    def load_data():
+        TRAIN_DATA_URL = (
+            "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+        )
+        TEST_DATA_URL = (
+            "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+        )
+        x_train = tf.keras.utils.get_file("titanic_train.csv", TRAIN_DATA_URL)
+        x_test = tf.keras.utils.get_file("titanic_eval.csv", TEST_DATA_URL)
+
+        return (x_train, "survived"), (x_test, "survived")
+
+
+class Iris(StructuredDataClassifierExperiment):
+    def __init__(self):
+        super().__init__(name="Iris")
+
+    @staticmethod
+    def load_data():
+        # Prepare the dataset.
+        TRAIN_DATA_URL = (
+            "https://storage.googleapis.com/"
+            "download.tensorflow.org/data/iris_training.csv"
+        )
+        x_train = tf.keras.utils.get_file("iris_train.csv", TRAIN_DATA_URL)
+
+        TEST_DATA_URL = (
+            "https://storage.googleapis.com/"
+            "download.tensorflow.org/data/iris_test.csv"
+        )
+        x_test = tf.keras.utils.get_file("iris_test.csv", TEST_DATA_URL)
+
+        return (x_train, "virginica"), (x_test, "virginica")
+
+
+class Wine(StructuredDataClassifierExperiment):
+    def __init__(self):
+        super().__init__(name="Wine")
+
+    @staticmethod
+    def load_data():
+        DATASET_URL = (
+            "https://archive.ics.uci.edu/ml/"
+            "machine-learning-databases/wine/wine.data"
+        )
+
+        # save data
+        dataset = tf.keras.utils.get_file("wine.csv", DATASET_URL)
+
+        data = pd.read_csv(dataset, header=None).sample(frac=1, random_state=5)
+        split_length = int(data.shape[0] * 0.8)  # 141
+
+        return (data.iloc[:split_length, 1:], data.iloc[:split_length, 0]), (
+            data.iloc[split_length:, 1:],
+            data.iloc[split_length:, 0],
+        )
+
+
+class StructuredDataRegressorExperiment(experiment.Experiment):
+    def get_auto_model(self):
+        return ak.StructuredDataRegressor(
+            max_trials=10, directory=self.tmp_dir, overwrite=True
+        )
+
+
+class CaliforniaHousing(StructuredDataRegressorExperiment):
+    @staticmethod
+    def load_data():
+        house_dataset = sklearn.datasets.fetch_california_housing()
+        (
+            x_train,
+            x_test,
+            y_train,
+            y_test,
+        ) = sklearn.model_selection.train_test_split(
+            house_dataset.data,
+            np.array(house_dataset.target),
+            test_size=0.2,
+            random_state=42,
+        )
+        return (x_train, y_train), (x_test, y_test)
diff --git a/benchmark/experiments/text.py b/benchmark/experiments/text.py
index 7a510eb..14e6ef5 100644
--- a/benchmark/experiments/text.py
+++ b/benchmark/experiments/text.py
@@ -14,8 +14,8 @@
 
 import os
 
-import keras
 import numpy as np
+import tensorflow as tf
 from sklearn.datasets import load_files
 
 import autokeras as ak
@@ -33,7 +33,7 @@ class IMDB(experiment.Experiment):
 
     @staticmethod
     def load_data():
-        dataset = keras.utils.get_file(
+        dataset = tf.keras.utils.get_file(
             fname="aclImdb.tar.gz",
             origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",  # noqa: E501
             extract=True,
diff --git a/benchmark/performance.py b/benchmark/performance.py
index 77ad170..678b805 100644
--- a/benchmark/performance.py
+++ b/benchmark/performance.py
@@ -13,17 +13,17 @@
 # limitations under the License.
 import os
 
-import keras
 import numpy as np
-from keras.datasets import cifar10
-from keras.datasets import mnist
+import tensorflow as tf
 from sklearn.datasets import load_files
+from tensorflow.keras.datasets import cifar10
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
 
 def imdb_raw(num_instances=100):
-    dataset = keras.utils.get_file(
+    dataset = tf.keras.utils.get_file(
         fname="aclImdb.tar.gz",
         origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
         extract=True,
@@ -75,3 +75,21 @@ def test_imdb_accuracy_over_92(tmp_path):
     clf.fit(x_train, y_train, batch_size=6, epochs=1)
     accuracy = clf.evaluate(x_test, y_test)[1]
     assert accuracy >= 0.92
+
+
+def test_titaninc_accuracy_over_77(tmp_path):
+    TRAIN_DATA_URL = (
+        "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+    )
+    TEST_DATA_URL = (
+        "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+    )
+
+    train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+    test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+    clf = ak.StructuredDataClassifier(max_trials=10, directory=tmp_path)
+
+    clf.fit(train_file_path, "survived")
+
+    accuracy = clf.evaluate(test_file_path, "survived")[1]
+    assert accuracy >= 0.77
diff --git a/docs/autogen.py b/docs/autogen.py
index b97c9cf..16a02a6 100644
--- a/docs/autogen.py
+++ b/docs/autogen.py
@@ -34,6 +34,20 @@ PAGES = {
         "autokeras.TextRegressor.evaluate",
         "autokeras.TextRegressor.export_model",
     ],
+    "structured_data_classifier.md": [
+        "autokeras.StructuredDataClassifier",
+        "autokeras.StructuredDataClassifier.fit",
+        "autokeras.StructuredDataClassifier.predict",
+        "autokeras.StructuredDataClassifier.evaluate",
+        "autokeras.StructuredDataClassifier.export_model",
+    ],
+    "structured_data_regressor.md": [
+        "autokeras.StructuredDataRegressor",
+        "autokeras.StructuredDataRegressor.fit",
+        "autokeras.StructuredDataRegressor.predict",
+        "autokeras.StructuredDataRegressor.evaluate",
+        "autokeras.StructuredDataRegressor.export_model",
+    ],
     "auto_model.md": [
         "autokeras.AutoModel",
         "autokeras.AutoModel.fit",
@@ -50,11 +64,13 @@ PAGES = {
     "node.md": [
         "autokeras.ImageInput",
         "autokeras.Input",
+        "autokeras.StructuredDataInput",
         "autokeras.TextInput",
     ],
     "block.md": [
         "autokeras.ConvBlock",
         "autokeras.DenseBlock",
+        "autokeras.Embedding",
         "autokeras.Merge",
         "autokeras.ResNetBlock",
         "autokeras.RNNBlock",
@@ -62,9 +78,13 @@ PAGES = {
         "autokeras.TemporalReduction",
         "autokeras.XceptionBlock",
         "autokeras.ImageBlock",
+        "autokeras.StructuredDataBlock",
         "autokeras.TextBlock",
         "autokeras.ImageAugmentation",
         "autokeras.Normalization",
+        "autokeras.TextToIntSequence",
+        "autokeras.TextToNgramVector",
+        "autokeras.CategoricalToNumerical",
         "autokeras.ClassificationHead",
         "autokeras.RegressionHead",
     ],
diff --git a/docs/ipynb/customized.ipynb b/docs/ipynb/customized.ipynb
index da250aa..c323f2e 100644
--- a/docs/ipynb/customized.ipynb
+++ b/docs/ipynb/customized.ipynb
@@ -19,10 +19,9 @@
    },
    "outputs": [],
    "source": [
-    "import keras\n",
     "import numpy as np\n",
-    "import tree\n",
-    "from keras.datasets import mnist\n",
+    "import tensorflow as tf\n",
+    "from tensorflow.keras.datasets import mnist\n",
     "\n",
     "import autokeras as ak"
    ]
@@ -135,19 +134,24 @@
     "the validation data, please refer to the Validation Data section of the\n",
     "tutorials of [Image\n",
     "Classification](/tutorial/image_classification/#validation-data), [Text\n",
-    "Classification](/tutorial/text_classification/#validation-data),\n",
+    "Classification](/tutorial/text_classification/#validation-data), [Structured\n",
+    "Data\n",
+    "Classification](/tutorial/structured_data_classification/#validation-data),\n",
     "[Multi-task and Multiple Validation](/tutorial/multi/#validation-data).\n",
     "\n",
     "## Data Format\n",
     "You can refer to the documentation of\n",
     "[ImageInput](/node/#imageinput-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[RegressionHead](/block/#regressionhead-class),\n",
     "[ClassificationHead](/block/#classificationhead-class),\n",
     "for the format of different types of data.\n",
     "You can also refer to the Data Format section of the tutorials of\n",
     "[Image Classification](/tutorial/image_classification/#data-format),\n",
-    "[Text Classification](/tutorial/text_classification/#data-format).\n",
+    "[Text Classification](/tutorial/text_classification/#data-format),\n",
+    "[Structured Data\n",
+    "Classification](/tutorial/structured_data_classification/#data-format).\n",
     "\n",
     "## Implement New Block\n",
     "\n",
@@ -173,8 +177,8 @@
     "class SingleDenseLayerBlock(ak.Block):\n",
     "    def build(self, hp, inputs=None):\n",
     "        # Get the input_node from inputs.\n",
-    "        input_node = tree.flatten(inputs)[0]\n",
-    "        layer = keras.layers.Dense(\n",
+    "        input_node = tf.nest.flatten(inputs)[0]\n",
+    "        layer = tf.keras.layers.Dense(\n",
     "            hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
     "        )\n",
     "        output_node = layer(input_node)\n",
@@ -228,6 +232,7 @@
     "**Nodes**:\n",
     "[ImageInput](/node/#imageinput-class),\n",
     "[Input](/node/#input-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[TextInput](/node/#textinput-class).\n",
     "\n",
     "**Preprocessors**:\n",
@@ -235,10 +240,13 @@
     "[ImageAugmentation](/block/#imageaugmentation-class),\n",
     "[LightGBM](/block/#lightgbm-class),\n",
     "[Normalization](/block/#normalization-class),\n",
+    "[TextToIntSequence](/block/#texttointsequence-class),\n",
+    "[TextToNgramVector](/block/#texttongramvector-class).\n",
     "\n",
     "**Blocks**:\n",
     "[ConvBlock](/block/#convblock-class),\n",
     "[DenseBlock](/block/#denseblock-class),\n",
+    "[Embedding](/block/#embedding-class),\n",
     "[Merge](/block/#merge-class),\n",
     "[ResNetBlock](/block/#resnetblock-class),\n",
     "[RNNBlock](/block/#rnnblock-class),\n",
@@ -246,6 +254,7 @@
     "[TemporalReduction](/block/#temporalreduction-class),\n",
     "[XceptionBlock](/block/#xceptionblock-class),\n",
     "[ImageBlock](/block/#imageblock-class),\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class),\n",
     "[TextBlock](/block/#textblock-class).\n"
    ]
   }
diff --git a/docs/ipynb/export.ipynb b/docs/ipynb/export.ipynb
index 5a31d1a..34b7199 100644
--- a/docs/ipynb/export.ipynb
+++ b/docs/ipynb/export.ipynb
@@ -19,9 +19,9 @@
    },
    "outputs": [],
    "source": [
-    "import numpy as np\n",
-    "from keras.datasets import mnist\n",
-    "from keras.models import load_model\n",
+    "import tensorflow as tf\n",
+    "from tensorflow.keras.datasets import mnist\n",
+    "from tensorflow.keras.models import load_model\n",
     "\n",
     "import autokeras as ak"
    ]
@@ -32,8 +32,7 @@
     "colab_type": "text"
    },
    "source": [
-    "You can easily export your model the best model found by AutoKeras as a Keras\n",
-    "Model.\n",
+    "You can easily export your model the best model found by AutoKeras as a Keras Model.\n",
     "\n",
     "The following example uses [ImageClassifier](/image_classifier) as an example.\n",
     "All the tasks and the [AutoModel](/auto_model/#automodel-class) has this\n",
@@ -48,6 +47,7 @@
    },
    "outputs": [],
    "source": [
+    "print(tf.__version__)\n",
     "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
     "\n",
     "# Initialize the image classifier.\n",
@@ -69,7 +69,7 @@
     "\n",
     "loaded_model = load_model(\"model_autokeras\", custom_objects=ak.CUSTOM_OBJECTS)\n",
     "\n",
-    "predicted_y = loaded_model.predict(np.expand_dims(x_test, -1))\n",
+    "predicted_y = loaded_model.predict(tf.expand_dims(x_test, -1))\n",
     "print(predicted_y)"
    ]
   }
diff --git a/docs/ipynb/image_classification.ipynb b/docs/ipynb/image_classification.ipynb
index 190373e..c48dbdb 100644
--- a/docs/ipynb/image_classification.ipynb
+++ b/docs/ipynb/image_classification.ipynb
@@ -21,7 +21,7 @@
    "source": [
     "import numpy as np\n",
     "import tensorflow as tf\n",
-    "from keras.datasets import mnist\n",
+    "from tensorflow.keras.datasets import mnist\n",
     "\n",
     "import autokeras as ak"
    ]
@@ -33,8 +33,7 @@
    },
    "source": [
     "## A Simple Example\n",
-    "The first step is to prepare your data. Here we use the MNIST dataset as an\n",
-    "example\n"
+    "The first step is to prepare your data. Here we use the MNIST dataset as an example\n"
    ]
   },
   {
@@ -194,13 +193,13 @@
     "colab_type": "text"
    },
    "source": [
-    "The usage of AutoModel is similar to the functional API of Keras. Basically, you\n",
-    "are building a graph, whose edges are blocks and the nodes are intermediate\n",
-    "outputs of blocks. To add an edge from input_node to output_node with\n",
-    "output_node = ak.[some_block]([block_args])(input_node).\n",
+    "The usage of AutoModel is similar to the functional API of Keras. Basically, you are\n",
+    "building a graph, whose edges are blocks and the nodes are intermediate outputs of\n",
+    "blocks. To add an edge from input_node to output_node with output_node =\n",
+    "ak.[some_block]([block_args])(input_node).\n",
     "\n",
-    "You can even also use more fine grained blocks to customize the search space\n",
-    "even further. See the following example.\n"
+    "You can even also use more fine grained blocks to customize the search space even\n",
+    "further. See the following example.\n"
    ]
   },
   {
diff --git a/docs/ipynb/image_regression.ipynb b/docs/ipynb/image_regression.ipynb
index 7a2cfe1..6288f7e 100644
--- a/docs/ipynb/image_regression.ipynb
+++ b/docs/ipynb/image_regression.ipynb
@@ -20,7 +20,7 @@
    "outputs": [],
    "source": [
     "import tensorflow as tf\n",
-    "from keras.datasets import mnist\n",
+    "from tensorflow.keras.datasets import mnist\n",
     "\n",
     "import autokeras as ak"
    ]
diff --git a/docs/ipynb/load.ipynb b/docs/ipynb/load.ipynb
index 1294927..32bdc67 100644
--- a/docs/ipynb/load.ipynb
+++ b/docs/ipynb/load.ipynb
@@ -22,7 +22,6 @@
     "import os\n",
     "import shutil\n",
     "\n",
-    "import keras\n",
     "import numpy as np\n",
     "import tensorflow as tf\n",
     "\n",
@@ -55,7 +54,7 @@
    "outputs": [],
    "source": [
     "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"  # noqa: E501\n",
-    "local_file_path = keras.utils.get_file(\n",
+    "local_file_path = tf.keras.utils.get_file(\n",
     "    origin=dataset_url, fname=\"image_data\", extract=True\n",
     ")\n",
     "# The file is extracted in the same directory as the downloaded file.\n",
@@ -161,7 +160,7 @@
    "source": [
     "dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
     "\n",
-    "local_file_path = keras.utils.get_file(\n",
+    "local_file_path = tf.keras.utils.get_file(\n",
     "    fname=\"text_data\",\n",
     "    origin=dataset_url,\n",
     "    extract=True,\n",
@@ -227,27 +226,31 @@
    "source": [
     "N_BATCHES = 30\n",
     "BATCH_SIZE = 100\n",
+    "N_FEATURES = 10\n",
     "\n",
     "\n",
-    "def get_data_generator(n_batches, batch_size):\n",
-    "    \"\"\"Get a generator returning n_batches random data.\"\"\"\n",
+    "def get_data_generator(n_batches, batch_size, n_features):\n",
+    "    \"\"\"Get a generator returning n_batches random data.\n",
+    "\n",
+    "    The shape of the data is (batch_size, n_features).\n",
+    "    \"\"\"\n",
     "\n",
     "    def data_generator():\n",
     "        for _ in range(n_batches * batch_size):\n",
-    "            x = np.random.randn(32, 32, 3)\n",
-    "            y = x.sum() / 32 * 32 * 3 > 0.5\n",
+    "            x = np.random.randn(n_features)\n",
+    "            y = x.sum(axis=0) / n_features > 0.5\n",
     "            yield x, y\n",
     "\n",
     "    return data_generator\n",
     "\n",
     "\n",
     "dataset = tf.data.Dataset.from_generator(\n",
-    "    get_data_generator(N_BATCHES, BATCH_SIZE),\n",
+    "    get_data_generator(N_BATCHES, BATCH_SIZE, N_FEATURES),\n",
     "    output_types=(tf.float32, tf.float32),\n",
-    "    output_shapes=((32, 32, 3), tuple()),\n",
+    "    output_shapes=((N_FEATURES,), tuple()),\n",
     ").batch(BATCH_SIZE)\n",
     "\n",
-    "clf = ak.ImageDataClassifier(overwrite=True, max_trials=1, seed=5)\n",
+    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=1, seed=5)\n",
     "clf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)\n",
     "print(clf.evaluate(dataset))"
    ]
diff --git a/docs/ipynb/multi.ipynb b/docs/ipynb/multi.ipynb
index 4409822..92dea6a 100644
--- a/docs/ipynb/multi.ipynb
+++ b/docs/ipynb/multi.ipynb
@@ -39,7 +39,7 @@
     "Multi-modal data means each data instance has multiple forms of information.\n",
     "For example, a photo can be saved as a image. Besides the image, it may also\n",
     "have when and where it was taken as its attributes, which can be represented as\n",
-    "numerical data.\n",
+    "structured data.\n",
     "\n",
     "## What is multi-task?\n",
     "\n",
@@ -54,19 +54,18 @@
     "<div class=\"mermaid\">\n",
     "graph TD\n",
     "    id1(ImageInput) --> id3(Some Neural Network Model)\n",
-    "    id2(Input) --> id3\n",
+    "    id2(StructuredDataInput) --> id3\n",
     "    id3 --> id4(ClassificationHead)\n",
     "    id3 --> id5(RegressionHead)\n",
     "</div>\n",
     "\n",
-    "It has two inputs the images and the numerical input data. Each image is\n",
-    "associated with a set of attributes in the numerical input data. From these\n",
-    "data, we are trying to predict the classification label and the regression value\n",
-    "at the same time.\n",
+    "It has two inputs the images and the structured data. Each image is associated\n",
+    "with a set of attributes in the structured data. From these data, we are trying\n",
+    "to predict the classification label and the regression value at the same time.\n",
     "\n",
     "## Data Preparation\n",
     "\n",
-    "To illustrate our idea, we generate some random image and numerical data as\n",
+    "To illustrate our idea, we generate some random image and structured data as\n",
     "the multi-modal data.\n"
    ]
   },
@@ -81,8 +80,8 @@
     "num_instances = 100\n",
     "# Generate image data.\n",
     "image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n",
-    "# Generate numerical data.\n",
-    "numerical_data = np.random.rand(num_instances, 20).astype(np.float32)"
+    "# Generate structured data.\n",
+    "structured_data = np.random.rand(num_instances, 20).astype(np.float32)"
    ]
   },
   {
@@ -130,7 +129,7 @@
    "source": [
     "# Initialize the multi with multiple inputs and outputs.\n",
     "model = ak.AutoModel(\n",
-    "    inputs=[ak.ImageInput(), ak.Input()],\n",
+    "    inputs=[ak.ImageInput(), ak.StructuredDataInput()],\n",
     "    outputs=[\n",
     "        ak.RegressionHead(metrics=[\"mae\"]),\n",
     "        ak.ClassificationHead(\n",
@@ -142,7 +141,7 @@
     ")\n",
     "# Fit the model with prepared data.\n",
     "model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [regression_target, classification_target],\n",
     "    epochs=3,\n",
     ")"
@@ -169,7 +168,7 @@
    "outputs": [],
    "source": [
     "model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [regression_target, classification_target],\n",
     "    # Split the training data and use the last 15% as validation data.\n",
     "    validation_split=0.15,\n",
@@ -198,21 +197,21 @@
     "split = 20\n",
     "\n",
     "image_val = image_data[split:]\n",
-    "numerical_val = numerical_data[split:]\n",
+    "structured_val = structured_data[split:]\n",
     "regression_val = regression_target[split:]\n",
     "classification_val = classification_target[split:]\n",
     "\n",
     "image_data = image_data[:split]\n",
-    "numerical_data = numerical_data[:split]\n",
+    "structured_data = structured_data[:split]\n",
     "regression_target = regression_target[:split]\n",
     "classification_target = classification_target[:split]\n",
     "\n",
     "model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [regression_target, classification_target],\n",
     "    # Use your own validation set.\n",
     "    validation_data=(\n",
-    "        [image_val, numerical_val],\n",
+    "        [image_val, structured_val],\n",
     "        [regression_val, classification_val],\n",
     "    ),\n",
     "    epochs=2,\n",
@@ -237,7 +236,8 @@
     "    id3 --> id5(ResNet V2)\n",
     "    id4 --> id6(Merge)\n",
     "    id5 --> id6\n",
-    "    id7(Input) --> id9(DenseBlock)\n",
+    "    id7(StructuredDataInput) --> id8(CategoricalToNumerical)\n",
+    "    id8 --> id9(DenseBlock)\n",
     "    id6 --> id10(Merge)\n",
     "    id9 --> id10\n",
     "    id10 --> id11(Classification Head)\n",
@@ -260,7 +260,8 @@
     "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
     "output_node1 = ak.Merge()([output_node1, output_node2])\n",
     "\n",
-    "input_node2 = ak.Input()\n",
+    "input_node2 = ak.StructuredDataInput()\n",
+    "output_node = ak.CategoricalToNumerical()(input_node2)\n",
     "output_node2 = ak.DenseBlock()(output_node)\n",
     "\n",
     "output_node = ak.Merge()([output_node1, output_node2])\n",
@@ -275,12 +276,12 @@
     ")\n",
     "\n",
     "image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n",
-    "numerical_data = np.random.rand(num_instances, 20).astype(np.float32)\n",
+    "structured_data = np.random.rand(num_instances, 20).astype(np.float32)\n",
     "regression_target = np.random.rand(num_instances, 1).astype(np.float32)\n",
     "classification_target = np.random.randint(5, size=num_instances)\n",
     "\n",
     "auto_model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [classification_target, regression_target],\n",
     "    batch_size=32,\n",
     "    epochs=3,\n",
@@ -296,7 +297,7 @@
     "## Data Format\n",
     "You can refer to the documentation of\n",
     "[ImageInput](/node/#imageinput-class),\n",
-    "[Input](/node/#input-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[RegressionHead](/block/#regressionhead-class),\n",
     "[ClassificationHead](/block/#classificationhead-class),\n",
@@ -304,11 +305,13 @@
     "You can also refer to the Data Format section of the tutorials of\n",
     "[Image Classification](/tutorial/image_classification/#data-format),\n",
     "[Text Classification](/tutorial/text_classification/#data-format),\n",
+    "[Structured Data Classification](\n",
+    "/tutorial/structured_data_classification/#data-format).\n",
     "\n",
     "## Reference\n",
     "[AutoModel](/auto_model/#automodel-class),\n",
     "[ImageInput](/node/#imageinput-class),\n",
-    "[Input](/node/#input-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[DenseBlock](/block/#denseblock-class),\n",
     "[RegressionHead](/block/#regressionhead-class),\n",
     "[ClassificationHead](/block/#classificationhead-class),\n",
diff --git a/docs/ipynb/structured_data_classification.ipynb b/docs/ipynb/structured_data_classification.ipynb
new file mode 100644
index 0000000..1205b30
--- /dev/null
+++ b/docs/ipynb/structured_data_classification.ipynb
@@ -0,0 +1,424 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install autokeras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "import tensorflow as tf\n",
+    "\n",
+    "import autokeras as ak"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## A Simple Example\n",
+    "The first step is to prepare your data. Here we use the [Titanic\n",
+    "dataset](https://www.kaggle.com/c/titanic) as an example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
+    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
+    "\n",
+    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
+    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The second step is to run the\n",
+    "[StructuredDataClassifier](/structured_data_classifier).\n",
+    "As a quick demo, we set epochs to 10.\n",
+    "You can also leave the epochs unspecified for an adaptive number of epochs.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data classifier.\n",
+    "clf = ak.StructuredDataClassifier(\n",
+    "    overwrite=True, max_trials=3\n",
+    ")  # It tries 3 different models.\n",
+    "# Feed the structured data classifier with training data.\n",
+    "clf.fit(\n",
+    "    # The path to the train.csv file.\n",
+    "    train_file_path,\n",
+    "    # The name of the label column.\n",
+    "    \"survived\",\n",
+    "    epochs=10,\n",
+    ")\n",
+    "# Predict with the best model.\n",
+    "predicted_y = clf.predict(test_file_path)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(test_file_path, \"survived\"))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Data Format\n",
+    "The AutoKeras StructuredDataClassifier is quite flexible for the data format.\n",
+    "\n",
+    "The example above shows how to use the CSV files directly. Besides CSV files,\n",
+    "it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](\n",
+    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The\n",
+    "data should be two-dimensional with numerical or categorical values.\n",
+    "\n",
+    "For the classification labels,\n",
+    "AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded\n",
+    "encoded labels, i.e. vectors of 0s and 1s.\n",
+    "The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series.\n",
+    "\n",
+    "The following examples show how the data can be prepared with numpy.ndarray,\n",
+    "pandas.DataFrame, and tensorflow.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# x_train as pandas.DataFrame, y_train as pandas.Series\n",
+    "x_train = pd.read_csv(train_file_path)\n",
+    "print(type(x_train))  # pandas.DataFrame\n",
+    "y_train = x_train.pop(\"survived\")\n",
+    "print(type(y_train))  # pandas.Series\n",
+    "\n",
+    "# You can also use pandas.DataFrame for y_train.\n",
+    "y_train = pd.DataFrame(y_train)\n",
+    "print(type(y_train))  # pandas.DataFrame\n",
+    "\n",
+    "# You can also use numpy.ndarray for x_train and y_train.\n",
+    "x_train = x_train.to_numpy()\n",
+    "y_train = y_train.to_numpy()\n",
+    "print(type(x_train))  # numpy.ndarray\n",
+    "print(type(y_train))  # numpy.ndarray\n",
+    "\n",
+    "# Preparing testing data.\n",
+    "x_test = pd.read_csv(test_file_path)\n",
+    "y_test = x_test.pop(\"survived\")\n",
+    "\n",
+    "# It tries 10 different models.\n",
+    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)\n",
+    "# Feed the structured data classifier with training data.\n",
+    "clf.fit(x_train, y_train, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = clf.predict(x_test)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(x_test, y_test))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The following code shows how to convert numpy.ndarray to tf.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "train_set = tf.data.Dataset.from_tensor_slices((x_train.astype(str), y_train))\n",
+    "test_set = tf.data.Dataset.from_tensor_slices(\n",
+    "    (x_test.to_numpy().astype(str), y_test)\n",
+    ")\n",
+    "\n",
+    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)\n",
+    "# Feed the tensorflow Dataset to the classifier.\n",
+    "clf.fit(train_set, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = clf.predict(test_set)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(test_set))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also specify the column names and types for the data as follows.  The\n",
+    "`column_names` is optional if the training data already have the column names,\n",
+    "e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will\n",
+    "be inferred from the training data.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data classifier.\n",
+    "clf = ak.StructuredDataClassifier(\n",
+    "    column_names=[\n",
+    "        \"sex\",\n",
+    "        \"age\",\n",
+    "        \"n_siblings_spouses\",\n",
+    "        \"parch\",\n",
+    "        \"fare\",\n",
+    "        \"class\",\n",
+    "        \"deck\",\n",
+    "        \"embark_town\",\n",
+    "        \"alone\",\n",
+    "    ],\n",
+    "    column_types={\"sex\": \"categorical\", \"fare\": \"numerical\"},\n",
+    "    max_trials=10,  # It tries 10 different models.\n",
+    "    overwrite=True,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Validation Data\n",
+    "By default, AutoKeras use the last 20% of training data as validation data.  As\n",
+    "shown in the example below, you can use `validation_split` to specify the\n",
+    "percentage.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "clf.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Split the training data and use the last 15% as validation data.\n",
+    "    validation_split=0.15,\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also use your own validation set\n",
+    "instead of splitting it from the training data with `validation_data`.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "split = 500\n",
+    "x_val = x_train[split:]\n",
+    "y_val = y_train[split:]\n",
+    "x_train = x_train[:split]\n",
+    "y_train = y_train[:split]\n",
+    "clf.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Use your own validation set.\n",
+    "    validation_data=(x_val, y_val),\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Customized Search Space\n",
+    "For advanced users, you may customize your search space by using\n",
+    "[AutoModel](/auto_model/#automodel-class) instead of\n",
+    "[StructuredDataClassifier](/structured_data_classifier). You can configure the\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class) for some high-level\n",
+    "configurations, e.g., `categorical_encoding` for whether to use the\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do\n",
+    "not specify these arguments, which would leave the different choices to be\n",
+    "tuned automatically. See the following example for detail.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n",
+    "output_node = ak.ClassificationHead()(output_node)\n",
+    "clf = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3\n",
+    ")\n",
+    "clf.fit(x_train, y_train, epochs=10)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.\n",
+    "To add an edge from `input_node` to `output_node` with\n",
+    "`output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space even\n",
+    "further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.CategoricalToNumerical()(input_node)\n",
+    "output_node = ak.DenseBlock()(output_node)\n",
+    "output_node = ak.ClassificationHead()(output_node)\n",
+    "clf = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
+    ")\n",
+    "clf.fit(x_train, y_train, epochs=1)\n",
+    "clf.predict(x_train)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also export the best model found by AutoKeras as a Keras Model.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "model = clf.export_model()\n",
+    "model.summary()\n",
+    "print(x_train.dtype)\n",
+    "# numpy array in object (mixed type) is not supported.\n",
+    "# convert it to unicode.\n",
+    "model.predict(x_train.astype(str))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Reference\n",
+    "[StructuredDataClassifier](/structured_data_classifier),\n",
+    "[AutoModel](/auto_model/#automodel-class),\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class),\n",
+    "[DenseBlock](/block/#denseblock-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
+    "[ClassificationHead](/block/#classificationhead-class),\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class).\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "collapsed_sections": [],
+   "name": "structured_data_classification",
+   "private_outputs": false,
+   "provenance": [],
+   "toc_visible": true
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
diff --git a/docs/ipynb/structured_data_regression.ipynb b/docs/ipynb/structured_data_regression.ipynb
new file mode 100644
index 0000000..ff20cf1
--- /dev/null
+++ b/docs/ipynb/structured_data_regression.ipynb
@@ -0,0 +1,427 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install autokeras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "import numpy as np\n",
+    "import pandas as pd\n",
+    "import tensorflow as tf\n",
+    "from sklearn.datasets import fetch_california_housing\n",
+    "\n",
+    "import autokeras as ak"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## A Simple Example\n",
+    "The first step is to prepare your data. Here we use the [California housing\n",
+    "dataset](\n",
+    "https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)\n",
+    "as an example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "house_dataset = fetch_california_housing()\n",
+    "df = pd.DataFrame(\n",
+    "    np.concatenate(\n",
+    "        (house_dataset.data, house_dataset.target.reshape(-1, 1)), axis=1\n",
+    "    ),\n",
+    "    columns=house_dataset.feature_names + [\"Price\"],\n",
+    ")\n",
+    "train_size = int(df.shape[0] * 0.9)\n",
+    "df[:train_size].to_csv(\"train.csv\", index=False)\n",
+    "df[train_size:].to_csv(\"eval.csv\", index=False)\n",
+    "train_file_path = \"train.csv\"\n",
+    "test_file_path = \"eval.csv\""
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The second step is to run the\n",
+    "[StructuredDataRegressor](/structured_data_regressor).\n",
+    "As a quick demo, we set epochs to 10.\n",
+    "You can also leave the epochs unspecified for an adaptive number of epochs.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data regressor.\n",
+    "reg = ak.StructuredDataRegressor(\n",
+    "    overwrite=True, max_trials=3\n",
+    ")  # It tries 3 different models.\n",
+    "# Feed the structured data regressor with training data.\n",
+    "reg.fit(\n",
+    "    # The path to the train.csv file.\n",
+    "    train_file_path,\n",
+    "    # The name of the label column.\n",
+    "    \"Price\",\n",
+    "    epochs=10,\n",
+    ")\n",
+    "# Predict with the best model.\n",
+    "predicted_y = reg.predict(test_file_path)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(reg.evaluate(test_file_path, \"Price\"))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Data Format\n",
+    "The AutoKeras StructuredDataRegressor is quite flexible for the data format.\n",
+    "\n",
+    "The example above shows how to use the CSV files directly. Besides CSV files,\n",
+    "it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](\n",
+    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The\n",
+    "data should be two-dimensional with numerical or categorical values.\n",
+    "\n",
+    "For the regression targets, it should be a vector of numerical values.\n",
+    "AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series.\n",
+    "\n",
+    "The following examples show how the data can be prepared with numpy.ndarray,\n",
+    "pandas.DataFrame, and tensorflow.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# x_train as pandas.DataFrame, y_train as pandas.Series\n",
+    "x_train = pd.read_csv(train_file_path)\n",
+    "print(type(x_train))  # pandas.DataFrame\n",
+    "y_train = x_train.pop(\"Price\")\n",
+    "print(type(y_train))  # pandas.Series\n",
+    "\n",
+    "# You can also use pandas.DataFrame for y_train.\n",
+    "y_train = pd.DataFrame(y_train)\n",
+    "print(type(y_train))  # pandas.DataFrame\n",
+    "\n",
+    "# You can also use numpy.ndarray for x_train and y_train.\n",
+    "x_train = x_train.to_numpy()\n",
+    "y_train = y_train.to_numpy()\n",
+    "print(type(x_train))  # numpy.ndarray\n",
+    "print(type(y_train))  # numpy.ndarray\n",
+    "\n",
+    "# Preparing testing data.\n",
+    "x_test = pd.read_csv(test_file_path)\n",
+    "y_test = x_test.pop(\"Price\")\n",
+    "\n",
+    "# It tries 10 different models.\n",
+    "reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
+    "# Feed the structured data regressor with training data.\n",
+    "reg.fit(x_train, y_train, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = reg.predict(x_test)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(reg.evaluate(x_test, y_test))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The following code shows how to convert numpy.ndarray to tf.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
+    "test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
+    "\n",
+    "reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
+    "# Feed the tensorflow Dataset to the regressor.\n",
+    "reg.fit(train_set, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = reg.predict(test_set)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(reg.evaluate(test_set))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also specify the column names and types for the data as follows.  The\n",
+    "`column_names` is optional if the training data already have the column names,\n",
+    "e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will\n",
+    "be inferred from the training data.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data regressor.\n",
+    "reg = ak.StructuredDataRegressor(\n",
+    "    column_names=[\n",
+    "        \"MedInc\",\n",
+    "        \"HouseAge\",\n",
+    "        \"AveRooms\",\n",
+    "        \"AveBedrms\",\n",
+    "        \"Population\",\n",
+    "        \"AveOccup\",\n",
+    "        \"Latitude\",\n",
+    "        \"Longitude\",\n",
+    "    ],\n",
+    "    column_types={\"MedInc\": \"numerical\", \"Latitude\": \"numerical\"},\n",
+    "    max_trials=10,  # It tries 10 different models.\n",
+    "    overwrite=True,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Validation Data\n",
+    "By default, AutoKeras use the last 20% of training data as validation data.  As\n",
+    "shown in the example below, you can use `validation_split` to specify the\n",
+    "percentage.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "reg.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Split the training data and use the last 15% as validation data.\n",
+    "    validation_split=0.15,\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also use your own validation set\n",
+    "instead of splitting it from the training data with `validation_data`.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "split = 500\n",
+    "x_val = x_train[split:]\n",
+    "y_val = y_train[split:]\n",
+    "x_train = x_train[:split]\n",
+    "y_train = y_train[:split]\n",
+    "reg.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Use your own validation set.\n",
+    "    validation_data=(x_val, y_val),\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Customized Search Space\n",
+    "For advanced users, you may customize your search space by using\n",
+    "[AutoModel](/auto_model/#automodel-class) instead of\n",
+    "[StructuredDataRegressor](/structured_data_regressor). You can configure the\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class) for some high-level\n",
+    "configurations, e.g., `categorical_encoding` for whether to use the\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do\n",
+    "not specify these arguments, which would leave the different choices to be\n",
+    "tuned automatically. See the following example for detail.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n",
+    "output_node = ak.RegressionHead()(output_node)\n",
+    "reg = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3\n",
+    ")\n",
+    "reg.fit(x_train, y_train, epochs=10)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.  To add an edge from `input_node` to\n",
+    "`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space\n",
+    "even further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.CategoricalToNumerical()(input_node)\n",
+    "output_node = ak.DenseBlock()(output_node)\n",
+    "output_node = ak.RegressionHead()(output_node)\n",
+    "reg = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, max_trials=3, overwrite=True\n",
+    ")\n",
+    "reg.fit(x_train, y_train, epochs=10)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also export the best model found by AutoKeras as a Keras Model.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "model = reg.export_model()\n",
+    "model.summary()\n",
+    "# numpy array in object (mixed type) is not supported.\n",
+    "# you need convert it to unicode or float first.\n",
+    "model.predict(x_train)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Reference\n",
+    "[StructuredDataRegressor](/structured_data_regressor),\n",
+    "[AutoModel](/auto_model/#automodel-class),\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class),\n",
+    "[DenseBlock](/block/#denseblock-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
+    "[RegressionHead](/block/#regressionhead-class),\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class).\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "collapsed_sections": [],
+   "name": "structured_data_regression",
+   "private_outputs": false,
+   "provenance": [],
+   "toc_visible": true
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
diff --git a/docs/ipynb/text_classification.ipynb b/docs/ipynb/text_classification.ipynb
index 1859fdd..3a9c5ea 100644
--- a/docs/ipynb/text_classification.ipynb
+++ b/docs/ipynb/text_classification.ipynb
@@ -21,7 +21,6 @@
    "source": [
     "import os\n",
     "\n",
-    "import keras\n",
     "import numpy as np\n",
     "import tensorflow as tf\n",
     "from sklearn.datasets import load_files\n",
@@ -49,7 +48,7 @@
    },
    "outputs": [],
    "source": [
-    "dataset = keras.utils.get_file(\n",
+    "dataset = tf.keras.utils.get_file(\n",
     "    fname=\"aclImdb.tar.gz\",\n",
     "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
     "    extract=True,\n",
@@ -177,9 +176,15 @@
     "For advanced users, you may customize your search space by using\n",
     "[AutoModel](/auto_model/#automodel-class) instead of\n",
     "[TextClassifier](/text_classifier). You can configure the\n",
-    "[TextBlock](/block/#textblock-class) for some high-level configurations. You can\n",
-    "also do not specify these arguments, which would leave the different choices to\n",
-    "be tuned automatically.  See the following example for detail.\n"
+    "[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,\n",
+    "`vectorizer` for the type of text vectorization method to use.  You can use\n",
+    "'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to\n",
+    "convert the words to integers and use [Embedding](/block/#embedding-class) for\n",
+    "embedding the integer sequences, or you can use 'ngram', which uses\n",
+    "[TextToNgramVector](/block/#texttongramvector-class) to vectorize the\n",
+    "sentences.  You can also do not specify these arguments, which would leave the\n",
+    "different choices to be tuned automatically.  See the following example for\n",
+    "detail.\n"
    ]
   },
   {
@@ -191,7 +196,43 @@
    "outputs": [],
    "source": [
     "input_node = ak.TextInput()\n",
-    "output_node = ak.TextBlock()(input_node)\n",
+    "output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n",
+    "output_node = ak.ClassificationHead()(output_node)\n",
+    "clf = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
+    ")\n",
+    "clf.fit(x_train, y_train, epochs=2)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.  To add an edge from `input_node` to\n",
+    "`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space\n",
+    "even further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.TextInput()\n",
+    "output_node = ak.TextToIntSequence()(input_node)\n",
+    "output_node = ak.Embedding()(output_node)\n",
+    "# Use separable Conv layers in Keras.\n",
+    "output_node = ak.ConvBlock(separable=True)(output_node)\n",
     "output_node = ak.ClassificationHead()(output_node)\n",
     "clf = ak.AutoModel(\n",
     "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
@@ -248,6 +289,10 @@
     "## Reference\n",
     "[TextClassifier](/text_classifier),\n",
     "[AutoModel](/auto_model/#automodel-class),\n",
+    "[TextBlock](/block/#textblock-class),\n",
+    "[TextToInteSequence](/block/#texttointsequence-class),\n",
+    "[Embedding](/block/#embedding-class),\n",
+    "[TextToNgramVector](/block/#texttongramvector-class),\n",
     "[ConvBlock](/block/#convblock-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[ClassificationHead](/block/#classificationhead-class).\n"
diff --git a/docs/ipynb/text_regression.ipynb b/docs/ipynb/text_regression.ipynb
index ba7f2be..4af6d5a 100644
--- a/docs/ipynb/text_regression.ipynb
+++ b/docs/ipynb/text_regression.ipynb
@@ -181,9 +181,15 @@
     "For advanced users, you may customize your search space by using\n",
     "[AutoModel](/auto_model/#automodel-class) instead of\n",
     "[TextRegressor](/text_regressor). You can configure the\n",
-    "[TextBlock](/block/#textblock-class) for some high-level configurations. You can\n",
-    "also do not specify these arguments, which would leave the different choices to\n",
-    "be tuned automatically.  See the following example for detail.\n"
+    "[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,\n",
+    "`vectorizer` for the type of text vectorization method to use.  You can use\n",
+    "'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to\n",
+    "convert the words to integers and use [Embedding](/block/#embedding-class) for\n",
+    "embedding the integer sequences, or you can use 'ngram', which uses\n",
+    "[TextToNgramVector](/block/#texttongramvector-class) to vectorize the\n",
+    "sentences.  You can also do not specify these arguments, which would leave the\n",
+    "different choices to be tuned automatically.  See the following example for\n",
+    "detail.\n"
    ]
   },
   {
@@ -195,7 +201,43 @@
    "outputs": [],
    "source": [
     "input_node = ak.TextInput()\n",
-    "output_node = ak.TextBlock()(input_node)\n",
+    "output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n",
+    "output_node = ak.RegressionHead()(output_node)\n",
+    "reg = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
+    ")\n",
+    "reg.fit(x_train, y_train, epochs=2)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.  To add an edge from `input_node` to\n",
+    "`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space\n",
+    "even further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.TextInput()\n",
+    "output_node = ak.TextToIntSequence()(input_node)\n",
+    "output_node = ak.Embedding()(output_node)\n",
+    "# Use separable Conv layers in Keras.\n",
+    "output_node = ak.ConvBlock(separable=True)(output_node)\n",
     "output_node = ak.RegressionHead()(output_node)\n",
     "reg = ak.AutoModel(\n",
     "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
@@ -253,6 +295,9 @@
     "[TextRegressor](/text_regressor),\n",
     "[AutoModel](/auto_model/#automodel-class),\n",
     "[TextBlock](/block/#textblock-class),\n",
+    "[TextToInteSequence](/block/#texttointsequence-class),\n",
+    "[Embedding](/block/#embedding-class),\n",
+    "[TextToNgramVector](/block/#texttongramvector-class),\n",
     "[ConvBlock](/block/#convblock-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[RegressionHead](/block/#regressionhead-class).\n"
diff --git a/docs/ipynb/timeseries_forecaster.ipynb b/docs/ipynb/timeseries_forecaster.ipynb
new file mode 100644
index 0000000..e5ae676
--- /dev/null
+++ b/docs/ipynb/timeseries_forecaster.ipynb
@@ -0,0 +1,200 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install autokeras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "import tensorflow as tf\n",
+    "\n",
+    "import autokeras as ak"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "To make this tutorial easy to follow, we use the UCI Airquality dataset, and try to\n",
+    "forecast the AH value at the different timesteps. Some basic preprocessing has also\n",
+    "been performed on the dataset as it required cleanup.\n",
+    "\n",
+    "## A Simple Example\n",
+    "The first step is to prepare your data. Here we use the [UCI Airquality\n",
+    "dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality) as an example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "dataset = tf.keras.utils.get_file(\n",
+    "    fname=\"AirQualityUCI.csv\",\n",
+    "    origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/\"\n",
+    "    \"AirQualityUCI.zip\",\n",
+    "    extract=True,\n",
+    ")\n",
+    "\n",
+    "dataset = pd.read_csv(dataset, sep=\";\")\n",
+    "dataset = dataset[dataset.columns[:-2]]\n",
+    "dataset = dataset.dropna()\n",
+    "dataset = dataset.replace(\",\", \".\", regex=True)\n",
+    "\n",
+    "val_split = int(len(dataset) * 0.7)\n",
+    "data_train = dataset[:val_split]\n",
+    "validation_data = dataset[val_split:]\n",
+    "\n",
+    "data_x = data_train[\n",
+    "    [\n",
+    "        \"CO(GT)\",\n",
+    "        \"PT08.S1(CO)\",\n",
+    "        \"NMHC(GT)\",\n",
+    "        \"C6H6(GT)\",\n",
+    "        \"PT08.S2(NMHC)\",\n",
+    "        \"NOx(GT)\",\n",
+    "        \"PT08.S3(NOx)\",\n",
+    "        \"NO2(GT)\",\n",
+    "        \"PT08.S4(NO2)\",\n",
+    "        \"PT08.S5(O3)\",\n",
+    "        \"T\",\n",
+    "        \"RH\",\n",
+    "    ]\n",
+    "].astype(\"float64\")\n",
+    "\n",
+    "data_x_val = validation_data[\n",
+    "    [\n",
+    "        \"CO(GT)\",\n",
+    "        \"PT08.S1(CO)\",\n",
+    "        \"NMHC(GT)\",\n",
+    "        \"C6H6(GT)\",\n",
+    "        \"PT08.S2(NMHC)\",\n",
+    "        \"NOx(GT)\",\n",
+    "        \"PT08.S3(NOx)\",\n",
+    "        \"NO2(GT)\",\n",
+    "        \"PT08.S4(NO2)\",\n",
+    "        \"PT08.S5(O3)\",\n",
+    "        \"T\",\n",
+    "        \"RH\",\n",
+    "    ]\n",
+    "].astype(\"float64\")\n",
+    "\n",
+    "# Data with train data and the unseen data from subsequent time steps.\n",
+    "data_x_test = dataset[\n",
+    "    [\n",
+    "        \"CO(GT)\",\n",
+    "        \"PT08.S1(CO)\",\n",
+    "        \"NMHC(GT)\",\n",
+    "        \"C6H6(GT)\",\n",
+    "        \"PT08.S2(NMHC)\",\n",
+    "        \"NOx(GT)\",\n",
+    "        \"PT08.S3(NOx)\",\n",
+    "        \"NO2(GT)\",\n",
+    "        \"PT08.S4(NO2)\",\n",
+    "        \"PT08.S5(O3)\",\n",
+    "        \"T\",\n",
+    "        \"RH\",\n",
+    "    ]\n",
+    "].astype(\"float64\")\n",
+    "\n",
+    "data_y = data_train[\"AH\"].astype(\"float64\")\n",
+    "\n",
+    "data_y_val = validation_data[\"AH\"].astype(\"float64\")\n",
+    "\n",
+    "print(data_x.shape)  # (6549, 12)\n",
+    "print(data_y.shape)  # (6549,)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The second step is to run the [TimeSeriesForecaster](/time_series_forecaster).\n",
+    "As a quick demo, we set epochs to 10.\n",
+    "You can also leave the epochs unspecified for an adaptive number of epochs.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "predict_from = 1\n",
+    "predict_until = 10\n",
+    "lookback = 3\n",
+    "clf = ak.TimeseriesForecaster(\n",
+    "    lookback=lookback,\n",
+    "    predict_from=predict_from,\n",
+    "    predict_until=predict_until,\n",
+    "    max_trials=1,\n",
+    "    objective=\"val_loss\",\n",
+    ")\n",
+    "# Train the TimeSeriesForecaster with train data\n",
+    "clf.fit(\n",
+    "    x=data_x,\n",
+    "    y=data_y,\n",
+    "    validation_data=(data_x_val, data_y_val),\n",
+    "    batch_size=32,\n",
+    "    epochs=10,\n",
+    ")\n",
+    "# Predict with the best model(includes original training data).\n",
+    "predictions = clf.predict(data_x_test)\n",
+    "print(predictions.shape)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(data_x_val, data_y_val))"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "collapsed_sections": [],
+   "name": "timeseries_forecaster",
+   "private_outputs": false,
+   "provenance": [],
+   "toc_visible": true
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
diff --git a/docs/py/customized.py b/docs/py/customized.py
index 86a1fc8..acf76ed 100644
--- a/docs/py/customized.py
+++ b/docs/py/customized.py
@@ -1,11 +1,9 @@
 """shell
 pip install autokeras
 """
-
-import keras
 import numpy as np
-import tree
-from keras.datasets import mnist
+import tensorflow as tf
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -86,19 +84,24 @@ If you would like to provide your own validation data or change the ratio of
 the validation data, please refer to the Validation Data section of the
 tutorials of [Image
 Classification](/tutorial/image_classification/#validation-data), [Text
-Classification](/tutorial/text_classification/#validation-data),
+Classification](/tutorial/text_classification/#validation-data), [Structured
+Data
+Classification](/tutorial/structured_data_classification/#validation-data),
 [Multi-task and Multiple Validation](/tutorial/multi/#validation-data).
 
 ## Data Format
 You can refer to the documentation of
 [ImageInput](/node/#imageinput-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
 for the format of different types of data.
 You can also refer to the Data Format section of the tutorials of
 [Image Classification](/tutorial/image_classification/#data-format),
-[Text Classification](/tutorial/text_classification/#data-format).
+[Text Classification](/tutorial/text_classification/#data-format),
+[Structured Data
+Classification](/tutorial/structured_data_classification/#data-format).
 
 ## Implement New Block
 
@@ -117,8 +120,8 @@ number of neurons is tunable.
 class SingleDenseLayerBlock(ak.Block):
     def build(self, hp, inputs=None):
         # Get the input_node from inputs.
-        input_node = tree.flatten(inputs)[0]
-        layer = keras.layers.Dense(
+        input_node = tf.nest.flatten(inputs)[0]
+        layer = tf.keras.layers.Dense(
             hp.Int("num_units", min_value=32, max_value=512, step=32)
         )
         output_node = layer(input_node)
@@ -153,6 +156,7 @@ print(auto_model.evaluate(x_test, y_test))
 **Nodes**:
 [ImageInput](/node/#imageinput-class),
 [Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class).
 
 **Preprocessors**:
@@ -160,10 +164,13 @@ print(auto_model.evaluate(x_test, y_test))
 [ImageAugmentation](/block/#imageaugmentation-class),
 [LightGBM](/block/#lightgbm-class),
 [Normalization](/block/#normalization-class),
+[TextToIntSequence](/block/#texttointsequence-class),
+[TextToNgramVector](/block/#texttongramvector-class).
 
 **Blocks**:
 [ConvBlock](/block/#convblock-class),
 [DenseBlock](/block/#denseblock-class),
+[Embedding](/block/#embedding-class),
 [Merge](/block/#merge-class),
 [ResNetBlock](/block/#resnetblock-class),
 [RNNBlock](/block/#rnnblock-class),
@@ -171,5 +178,6 @@ print(auto_model.evaluate(x_test, y_test))
 [TemporalReduction](/block/#temporalreduction-class),
 [XceptionBlock](/block/#xceptionblock-class),
 [ImageBlock](/block/#imageblock-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
 [TextBlock](/block/#textblock-class).
 """
diff --git a/docs/py/export.py b/docs/py/export.py
index c648fa2..7c600c2 100644
--- a/docs/py/export.py
+++ b/docs/py/export.py
@@ -1,10 +1,9 @@
 """shell
 pip install autokeras
 """
-
-import numpy as np
-from keras.datasets import mnist
-from keras.models import load_model
+import tensorflow as tf
+from tensorflow.keras.datasets import mnist
+from tensorflow.keras.models import load_model
 
 import autokeras as ak
 
@@ -19,6 +18,7 @@ All the tasks and the [AutoModel](/auto_model/#automodel-class) has this
 """
 
 
+print(tf.__version__)
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 
 # Initialize the image classifier.
@@ -32,12 +32,13 @@ model = clf.export_model()
 
 print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>
 
-model.save("model_autokeras.keras")
+try:
+    model.save("model_autokeras", save_format="tf")
+except Exception:
+    model.save("model_autokeras.h5")
 
 
-loaded_model = load_model(
-    "model_autokeras.keras", custom_objects=ak.CUSTOM_OBJECTS
-)
+loaded_model = load_model("model_autokeras", custom_objects=ak.CUSTOM_OBJECTS)
 
-predicted_y = loaded_model.predict(np.expand_dims(x_test, -1))
+predicted_y = loaded_model.predict(tf.expand_dims(x_test, -1))
 print(predicted_y)
diff --git a/docs/py/image_classification.py b/docs/py/image_classification.py
index 79c929e..6886227 100644
--- a/docs/py/image_classification.py
+++ b/docs/py/image_classification.py
@@ -1,10 +1,9 @@
 """shell
 pip install autokeras
 """
-
 import numpy as np
 import tensorflow as tf
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -15,10 +14,6 @@ example
 """
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
-x_train = x_train[:100]
-y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 print(x_train.shape)  # (60000, 28, 28)
 print(y_train.shape)  # (60000,)
 print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
@@ -34,7 +29,7 @@ You can also leave the epochs unspecified for an adaptive number of epochs.
 # Initialize the image classifier.
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
 # Feed the image classifier with training data.
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 
 # Predict with the best model.
@@ -57,7 +52,7 @@ clf.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
+    epochs=10,
 )
 
 """
@@ -75,7 +70,7 @@ clf.fit(
     y_train,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    epochs=1,
+    epochs=10,
 )
 
 """
@@ -102,7 +97,7 @@ output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 """
 The usage of AutoModel is similar to the functional API of Keras. Basically, you
@@ -122,7 +117,7 @@ output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 """
 ## Data Format
@@ -169,7 +164,7 @@ test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))
 
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
 # Feed the tensorflow Dataset to the classifier.
-clf.fit(train_set, epochs=1)
+clf.fit(train_set, epochs=10)
 # Predict with the best model.
 predicted_y = clf.predict(test_set)
 # Evaluate the best model with testing data.
diff --git a/docs/py/image_regression.py b/docs/py/image_regression.py
index dfe4781..aaf847f 100644
--- a/docs/py/image_regression.py
+++ b/docs/py/image_regression.py
@@ -3,7 +3,7 @@ pip install autokeras
 """
 
 import tensorflow as tf
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -22,8 +22,6 @@ example
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 x_train = x_train[:100]
 y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 print(x_train.shape)  # (60000, 28, 28)
 print(y_train.shape)  # (60000,)
 print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
@@ -31,14 +29,14 @@ print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
 """
 The second step is to run the ImageRegressor.  It is recommended have more
 trials for more complicated datasets.  This is just a quick demo of MNIST, so
-we set max_trials to 1.  For the same reason, we set epochs to 1.  You can also
+we set max_trials to 1.  For the same reason, we set epochs to 2.  You can also
 leave the epochs unspecified for an adaptive number of epochs.
 """
 
 # Initialize the image regressor.
 reg = ak.ImageRegressor(overwrite=True, max_trials=1)
 # Feed the image regressor with training data.
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 
 # Predict with the best model.
@@ -61,7 +59,7 @@ reg.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
+    epochs=2,
 )
 
 """
@@ -106,7 +104,7 @@ output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 The usage of AutoModel is similar to the functional API of Keras. Basically,
@@ -126,7 +124,7 @@ output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -146,10 +144,6 @@ case, the images would have to be 3-dimentional.
 """
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
-x_train = x_train[:100]
-y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 
 # Reshape the images to have the channel dimension.
 x_train = x_train.reshape(x_train.shape + (1,))
@@ -165,7 +159,7 @@ test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))
 
 reg = ak.ImageRegressor(overwrite=True, max_trials=1)
 # Feed the tensorflow Dataset to the regressor.
-reg.fit(train_set, epochs=1)
+reg.fit(train_set, epochs=2)
 # Predict with the best model.
 predicted_y = reg.predict(test_set)
 # Evaluate the best model with testing data.
diff --git a/docs/py/load.py b/docs/py/load.py
index bc43a3f..1f8df90 100644
--- a/docs/py/load.py
+++ b/docs/py/load.py
@@ -5,7 +5,6 @@ pip install autokeras
 import os
 import shutil
 
-import keras
 import numpy as np
 import tensorflow as tf
 
@@ -24,7 +23,7 @@ First, we download the data and extract the files.
 """
 
 dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"  # noqa: E501
-local_file_path = keras.utils.get_file(
+local_file_path = tf.keras.utils.get_file(
     origin=dataset_url, fname="image_data", extract=True
 )
 # The file is extracted in the same directory as the downloaded file.
@@ -49,7 +48,7 @@ flowers_photos/
 We can split the data into training and testing as we load them.
 """
 
-batch_size = 2
+batch_size = 32
 img_height = 180
 img_width = 180
 
@@ -78,8 +77,8 @@ Then we just do one quick demo of AutoKeras to make sure the dataset works.
 """
 
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
-clf.fit(train_data.take(100), epochs=1)
-print(clf.evaluate(test_data.take(2)))
+clf.fit(train_data, epochs=1)
+print(clf.evaluate(test_data))
 
 """
 ## Load Texts from Disk
@@ -88,7 +87,7 @@ You can also load text datasets in the same way.
 
 dataset_url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
 
-local_file_path = keras.utils.get_file(
+local_file_path = tf.keras.utils.get_file(
     fname="text_data",
     origin=dataset_url,
     extract=True,
@@ -117,8 +116,8 @@ test_data = ak.text_dataset_from_directory(
 )
 
 clf = ak.TextClassifier(overwrite=True, max_trials=1)
-clf.fit(train_data.take(2), epochs=1)
-print(clf.evaluate(test_data.take(2)))
+clf.fit(train_data, epochs=2)
+print(clf.evaluate(test_data))
 
 
 """
@@ -127,29 +126,33 @@ If you want to use generators, you can refer to the following code.
 """
 
 
-N_BATCHES = 2
-BATCH_SIZE = 10
+N_BATCHES = 30
+BATCH_SIZE = 100
+N_FEATURES = 10
 
 
-def get_data_generator(n_batches, batch_size):
-    """Get a generator returning n_batches random data."""
+def get_data_generator(n_batches, batch_size, n_features):
+    """Get a generator returning n_batches random data.
+
+    The shape of the data is (batch_size, n_features).
+    """
 
     def data_generator():
         for _ in range(n_batches * batch_size):
-            x = np.random.randn(32, 32, 3)
-            y = x.sum() / 32 * 32 * 3 > 0.5
+            x = np.random.randn(n_features)
+            y = x.sum(axis=0) / n_features > 0.5
             yield x, y
 
     return data_generator
 
 
 dataset = tf.data.Dataset.from_generator(
-    get_data_generator(N_BATCHES, BATCH_SIZE),
+    get_data_generator(N_BATCHES, BATCH_SIZE, N_FEATURES),
     output_types=(tf.float32, tf.float32),
-    output_shapes=((32, 32, 3), tuple()),
+    output_shapes=((N_FEATURES,), tuple()),
 ).batch(BATCH_SIZE)
 
-clf = ak.ImageClassifier(overwrite=True, max_trials=1, seed=5)
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=1, seed=5)
 clf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)
 print(clf.evaluate(dataset))
 
diff --git a/docs/py/multi.py b/docs/py/multi.py
index c88e5e4..31e7284 100644
--- a/docs/py/multi.py
+++ b/docs/py/multi.py
@@ -16,7 +16,7 @@ In this tutorial we are making use of the
 Multi-modal data means each data instance has multiple forms of information.
 For example, a photo can be saved as a image. Besides the image, it may also
 have when and where it was taken as its attributes, which can be represented as
-numerical data.
+structured data.
 
 ## What is multi-task?
 
@@ -31,28 +31,27 @@ network model.
 <div class="mermaid">
 graph TD
     id1(ImageInput) --> id3(Some Neural Network Model)
-    id2(Input) --> id3
+    id2(StructuredDataInput) --> id3
     id3 --> id4(ClassificationHead)
     id3 --> id5(RegressionHead)
 </div>
 
-It has two inputs the images and the numerical input data. Each image is
-associated with a set of attributes in the numerical input data. From these
-data, we are trying to predict the classification label and the regression value
-at the same time.
+It has two inputs the images and the structured data. Each image is associated
+with a set of attributes in the structured data. From these data, we are trying
+to predict the classification label and the regression value at the same time.
 
 ## Data Preparation
 
-To illustrate our idea, we generate some random image and numerical data as
+To illustrate our idea, we generate some random image and structured data as
 the multi-modal data.
 """
 
 
-num_instances = 10
+num_instances = 100
 # Generate image data.
 image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)
-# Generate numerical data.
-numerical_data = np.random.rand(num_instances, 20).astype(np.float32)
+# Generate structured data.
+structured_data = np.random.rand(num_instances, 20).astype(np.float32)
 
 """
 We also generate some multi-task targets for classification and regression.
@@ -73,7 +72,7 @@ Since this is just a demo, we use small amount of `max_trials` and `epochs`.
 
 # Initialize the multi with multiple inputs and outputs.
 model = ak.AutoModel(
-    inputs=[ak.ImageInput(), ak.Input()],
+    inputs=[ak.ImageInput(), ak.StructuredDataInput()],
     outputs=[
         ak.RegressionHead(metrics=["mae"]),
         ak.ClassificationHead(
@@ -85,10 +84,9 @@ model = ak.AutoModel(
 )
 # Fit the model with prepared data.
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
-    epochs=1,
-    batch_size=3,
+    epochs=3,
 )
 
 """
@@ -99,12 +97,11 @@ percentage.
 """
 
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
-    batch_size=3,
+    epochs=2,
 )
 
 """
@@ -112,28 +109,27 @@ You can also use your own validation set
 instead of splitting it from the training data with `validation_data`.
 """
 
-split = 5
+split = 20
 
 image_val = image_data[split:]
-numerical_val = numerical_data[split:]
+structured_val = structured_data[split:]
 regression_val = regression_target[split:]
 classification_val = classification_target[split:]
 
 image_data = image_data[:split]
-numerical_data = numerical_data[:split]
+structured_data = structured_data[:split]
 regression_target = regression_target[:split]
 classification_target = classification_target[:split]
 
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
     # Use your own validation set.
     validation_data=(
-        [image_val, numerical_val],
+        [image_val, structured_val],
         [regression_val, classification_val],
     ),
-    epochs=1,
-    batch_size=3,
+    epochs=2,
 )
 
 """
@@ -149,7 +145,8 @@ graph LR
     id3 --> id5(ResNet V2)
     id4 --> id6(Merge)
     id5 --> id6
-    id7(Input) --> id9(DenseBlock)
+    id7(StructuredDataInput) --> id8(CategoricalToNumerical)
+    id8 --> id9(DenseBlock)
     id6 --> id10(Merge)
     id9 --> id10
     id10 --> id11(Classification Head)
@@ -164,8 +161,9 @@ output_node1 = ak.ConvBlock()(output_node)
 output_node2 = ak.ResNetBlock(version="v2")(output_node)
 output_node1 = ak.Merge()([output_node1, output_node2])
 
-input_node2 = ak.Input()
-output_node2 = ak.DenseBlock()(input_node2)
+input_node2 = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node2)
+output_node2 = ak.DenseBlock()(output_node)
 
 output_node = ak.Merge()([output_node1, output_node2])
 output_node1 = ak.ClassificationHead()(output_node)
@@ -179,22 +177,22 @@ auto_model = ak.AutoModel(
 )
 
 image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)
-numerical_data = np.random.rand(num_instances, 20).astype(np.float32)
+structured_data = np.random.rand(num_instances, 20).astype(np.float32)
 regression_target = np.random.rand(num_instances, 1).astype(np.float32)
 classification_target = np.random.randint(5, size=num_instances)
 
 auto_model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [classification_target, regression_target],
-    batch_size=3,
-    epochs=1,
+    batch_size=32,
+    epochs=3,
 )
 
 """
 ## Data Format
 You can refer to the documentation of
 [ImageInput](/node/#imageinput-class),
-[Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
@@ -202,11 +200,13 @@ for the format of different types of data.
 You can also refer to the Data Format section of the tutorials of
 [Image Classification](/tutorial/image_classification/#data-format),
 [Text Classification](/tutorial/text_classification/#data-format),
+[Structured Data Classification](
+/tutorial/structured_data_classification/#data-format).
 
 ## Reference
 [AutoModel](/auto_model/#automodel-class),
 [ImageInput](/node/#imageinput-class),
-[Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [DenseBlock](/block/#denseblock-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
diff --git a/docs/py/structured_data_classification.py b/docs/py/structured_data_classification.py
new file mode 100644
index 0000000..30be1f3
--- /dev/null
+++ b/docs/py/structured_data_classification.py
@@ -0,0 +1,234 @@
+"""shell
+pip install autokeras
+"""
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+## A Simple Example
+The first step is to prepare your data. Here we use the [Titanic
+dataset](https://www.kaggle.com/c/titanic) as an example.
+"""
+
+
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+
+"""
+The second step is to run the
+[StructuredDataClassifier](/structured_data_classifier).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+# Initialize the structured data classifier.
+clf = ak.StructuredDataClassifier(
+    overwrite=True, max_trials=3
+)  # It tries 3 different models.
+# Feed the structured data classifier with training data.
+clf.fit(
+    # The path to the train.csv file.
+    train_file_path,
+    # The name of the label column.
+    "survived",
+    epochs=10,
+)
+# Predict with the best model.
+predicted_y = clf.predict(test_file_path)
+# Evaluate the best model with testing data.
+print(clf.evaluate(test_file_path, "survived"))
+
+"""
+## Data Format
+The AutoKeras StructuredDataClassifier is quite flexible for the data format.
+
+The example above shows how to use the CSV files directly. Besides CSV files,
+it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](
+https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The
+data should be two-dimensional with numerical or categorical values.
+
+For the classification labels, AutoKeras accepts both plain labels, i.e. strings
+or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The
+labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series.
+
+The following examples show how the data can be prepared with numpy.ndarray,
+pandas.DataFrame, and tensorflow.data.Dataset.
+"""
+
+
+# x_train as pandas.DataFrame, y_train as pandas.Series
+x_train = pd.read_csv(train_file_path)
+print(type(x_train))  # pandas.DataFrame
+y_train = x_train.pop("survived")
+print(type(y_train))  # pandas.Series
+
+# You can also use pandas.DataFrame for y_train.
+y_train = pd.DataFrame(y_train)
+print(type(y_train))  # pandas.DataFrame
+
+# You can also use numpy.ndarray for x_train and y_train.
+x_train = x_train.to_numpy()
+y_train = y_train.to_numpy()
+print(type(x_train))  # numpy.ndarray
+print(type(y_train))  # numpy.ndarray
+
+# Preparing testing data.
+x_test = pd.read_csv(test_file_path)
+y_test = x_test.pop("survived")
+
+# It tries 10 different models.
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)
+# Feed the structured data classifier with training data.
+clf.fit(x_train, y_train, epochs=10)
+# Predict with the best model.
+predicted_y = clf.predict(x_test)
+# Evaluate the best model with testing data.
+print(clf.evaluate(x_test, y_test))
+
+"""
+The following code shows how to convert numpy.ndarray to tf.data.Dataset.
+"""
+
+train_set = tf.data.Dataset.from_tensor_slices((x_train.astype(str), y_train))
+test_set = tf.data.Dataset.from_tensor_slices(
+    (x_test.to_numpy().astype(str), y_test)
+)
+
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)
+# Feed the tensorflow Dataset to the classifier.
+clf.fit(train_set, epochs=10)
+# Predict with the best model.
+predicted_y = clf.predict(test_set)
+# Evaluate the best model with testing data.
+print(clf.evaluate(test_set))
+
+"""
+You can also specify the column names and types for the data as follows.  The
+`column_names` is optional if the training data already have the column names,
+e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will
+be inferred from the training data.
+"""
+
+# Initialize the structured data classifier.
+clf = ak.StructuredDataClassifier(
+    column_names=[
+        "sex",
+        "age",
+        "n_siblings_spouses",
+        "parch",
+        "fare",
+        "class",
+        "deck",
+        "embark_town",
+        "alone",
+    ],
+    column_types={"sex": "categorical", "fare": "numerical"},
+    max_trials=10,  # It tries 10 different models.
+    overwrite=True,
+)
+
+
+"""
+## Validation Data
+By default, AutoKeras use the last 20% of training data as validation data.  As
+shown in the example below, you can use `validation_split` to specify the
+percentage.
+"""
+
+clf.fit(
+    x_train,
+    y_train,
+    # Split the training data and use the last 15% as validation data.
+    validation_split=0.15,
+    epochs=10,
+)
+
+"""
+You can also use your own validation set
+instead of splitting it from the training data with `validation_data`.
+"""
+
+split = 500
+x_val = x_train[split:]
+y_val = y_train[split:]
+x_train = x_train[:split]
+y_train = y_train[:split]
+clf.fit(
+    x_train,
+    y_train,
+    # Use your own validation set.
+    validation_data=(x_val, y_val),
+    epochs=10,
+)
+
+"""
+## Customized Search Space
+For advanced users, you may customize your search space by using
+[AutoModel](/auto_model/#automodel-class) instead of
+[StructuredDataClassifier](/structured_data_classifier). You can configure the
+[StructuredDataBlock](/block/#structureddatablock-class) for some high-level
+configurations, e.g., `categorical_encoding` for whether to use the
+[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do
+not specify these arguments, which would leave the different choices to be
+tuned automatically. See the following example for detail.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3
+)
+clf.fit(x_train, y_train, epochs=10)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.
+To add an edge from `input_node` to `output_node` with
+`output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node)
+output_node = ak.DenseBlock()(output_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+clf.fit(x_train, y_train, epochs=1)
+clf.predict(x_train)
+
+"""
+You can also export the best model found by AutoKeras as a Keras Model.
+"""
+
+model = clf.export_model()
+model.summary()
+print(x_train.dtype)
+# numpy array in object (mixed type) is not supported.
+# convert it to unicode.
+model.predict(x_train.astype(str))
+
+"""
+## Reference
+[StructuredDataClassifier](/structured_data_classifier),
+[AutoModel](/auto_model/#automodel-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
+[DenseBlock](/block/#denseblock-class),
+[StructuredDataInput](/node/#structureddatainput-class),
+[ClassificationHead](/block/#classificationhead-class),
+[CategoricalToNumerical](/block/#categoricaltonumerical-class).
+"""
diff --git a/docs/py/structured_data_regression.py b/docs/py/structured_data_regression.py
new file mode 100644
index 0000000..236fd0f
--- /dev/null
+++ b/docs/py/structured_data_regression.py
@@ -0,0 +1,239 @@
+"""shell
+pip install autokeras
+"""
+
+import numpy as np
+import pandas as pd
+import tensorflow as tf
+from sklearn.datasets import fetch_california_housing
+
+import autokeras as ak
+
+"""
+## A Simple Example
+The first step is to prepare your data. Here we use the [California housing
+dataset](
+https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)
+as an example.
+"""
+
+
+house_dataset = fetch_california_housing()
+df = pd.DataFrame(
+    np.concatenate(
+        (house_dataset.data, house_dataset.target.reshape(-1, 1)), axis=1
+    ),
+    columns=house_dataset.feature_names + ["Price"],
+)
+train_size = int(df.shape[0] * 0.9)
+df[:train_size].to_csv("train.csv", index=False)
+df[train_size:].to_csv("eval.csv", index=False)
+train_file_path = "train.csv"
+test_file_path = "eval.csv"
+
+"""
+The second step is to run the
+[StructuredDataRegressor](/structured_data_regressor).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+# Initialize the structured data regressor.
+reg = ak.StructuredDataRegressor(
+    overwrite=True, max_trials=3
+)  # It tries 3 different models.
+# Feed the structured data regressor with training data.
+reg.fit(
+    # The path to the train.csv file.
+    train_file_path,
+    # The name of the label column.
+    "Price",
+    epochs=10,
+)
+# Predict with the best model.
+predicted_y = reg.predict(test_file_path)
+# Evaluate the best model with testing data.
+print(reg.evaluate(test_file_path, "Price"))
+
+"""
+## Data Format
+The AutoKeras StructuredDataRegressor is quite flexible for the data format.
+
+The example above shows how to use the CSV files directly. Besides CSV files,
+it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](
+https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The
+data should be two-dimensional with numerical or categorical values.
+
+For the regression targets, it should be a vector of numerical values.
+AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series.
+
+The following examples show how the data can be prepared with numpy.ndarray,
+pandas.DataFrame, and tensorflow.data.Dataset.
+"""
+
+
+# x_train as pandas.DataFrame, y_train as pandas.Series
+x_train = pd.read_csv(train_file_path)
+print(type(x_train))  # pandas.DataFrame
+y_train = x_train.pop("Price")
+print(type(y_train))  # pandas.Series
+
+# You can also use pandas.DataFrame for y_train.
+y_train = pd.DataFrame(y_train)
+print(type(y_train))  # pandas.DataFrame
+
+# You can also use numpy.ndarray for x_train and y_train.
+x_train = x_train.to_numpy()
+y_train = y_train.to_numpy()
+print(type(x_train))  # numpy.ndarray
+print(type(y_train))  # numpy.ndarray
+
+# Preparing testing data.
+x_test = pd.read_csv(test_file_path)
+y_test = x_test.pop("Price")
+
+# It tries 10 different models.
+reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)
+# Feed the structured data regressor with training data.
+reg.fit(x_train, y_train, epochs=10)
+# Predict with the best model.
+predicted_y = reg.predict(x_test)
+# Evaluate the best model with testing data.
+print(reg.evaluate(x_test, y_test))
+
+"""
+The following code shows how to convert numpy.ndarray to tf.data.Dataset.
+"""
+
+train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))
+test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test))
+
+reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)
+# Feed the tensorflow Dataset to the regressor.
+reg.fit(train_set, epochs=10)
+# Predict with the best model.
+predicted_y = reg.predict(test_set)
+# Evaluate the best model with testing data.
+print(reg.evaluate(test_set))
+
+"""
+You can also specify the column names and types for the data as follows.  The
+`column_names` is optional if the training data already have the column names,
+e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will
+be inferred from the training data.
+"""
+
+# Initialize the structured data regressor.
+reg = ak.StructuredDataRegressor(
+    column_names=[
+        "MedInc",
+        "HouseAge",
+        "AveRooms",
+        "AveBedrms",
+        "Population",
+        "AveOccup",
+        "Latitude",
+        "Longitude",
+    ],
+    column_types={"MedInc": "numerical", "Latitude": "numerical"},
+    max_trials=10,  # It tries 10 different models.
+    overwrite=True,
+)
+
+
+"""
+## Validation Data
+By default, AutoKeras use the last 20% of training data as validation data.  As
+shown in the example below, you can use `validation_split` to specify the
+percentage.
+"""
+
+reg.fit(
+    x_train,
+    y_train,
+    # Split the training data and use the last 15% as validation data.
+    validation_split=0.15,
+    epochs=10,
+)
+
+"""
+You can also use your own validation set
+instead of splitting it from the training data with `validation_data`.
+"""
+
+split = 500
+x_val = x_train[split:]
+y_val = y_train[split:]
+x_train = x_train[:split]
+y_train = y_train[:split]
+reg.fit(
+    x_train,
+    y_train,
+    # Use your own validation set.
+    validation_data=(x_val, y_val),
+    epochs=10,
+)
+
+"""
+## Customized Search Space
+For advanced users, you may customize your search space by using
+[AutoModel](/auto_model/#automodel-class) instead of
+[StructuredDataRegressor](/structured_data_regressor). You can configure the
+[StructuredDataBlock](/block/#structureddatablock-class) for some high-level
+configurations, e.g., `categorical_encoding` for whether to use the
+[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do
+not specify these arguments, which would leave the different choices to be
+tuned automatically. See the following example for detail.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3
+)
+reg.fit(x_train, y_train, epochs=10)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node)
+output_node = ak.DenseBlock()(output_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, max_trials=3, overwrite=True
+)
+reg.fit(x_train, y_train, epochs=10)
+
+"""
+You can also export the best model found by AutoKeras as a Keras Model.
+"""
+
+model = reg.export_model()
+model.summary()
+# numpy array in object (mixed type) is not supported.
+# you need convert it to unicode or float first.
+model.predict(x_train)
+
+
+"""
+## Reference
+[StructuredDataRegressor](/structured_data_regressor),
+[AutoModel](/auto_model/#automodel-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
+[DenseBlock](/block/#denseblock-class),
+[StructuredDataInput](/node/#structureddatainput-class),
+[RegressionHead](/block/#regressionhead-class),
+[CategoricalToNumerical](/block/#categoricaltonumerical-class).
+"""
diff --git a/docs/py/text_classification.py b/docs/py/text_classification.py
index 3365df6..fae3def 100644
--- a/docs/py/text_classification.py
+++ b/docs/py/text_classification.py
@@ -4,7 +4,6 @@ pip install autokeras
 
 import os
 
-import keras
 import numpy as np
 import tensorflow as tf
 from sklearn.datasets import load_files
@@ -19,7 +18,7 @@ as an example.
 """
 
 
-dataset = keras.utils.get_file(
+dataset = tf.keras.utils.get_file(
     fname="aclImdb.tar.gz",
     origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
     extract=True,
@@ -36,10 +35,10 @@ test_data = load_files(
     os.path.join(IMDB_DATADIR, "test"), shuffle=False, categories=classes
 )
 
-x_train = np.array(train_data.data)[:100]
-y_train = np.array(train_data.target)[:100]
-x_test = np.array(test_data.data)[:100]
-y_test = np.array(test_data.target)[:100]
+x_train = np.array(train_data.data)
+y_train = np.array(train_data.target)
+x_test = np.array(test_data.data)
+y_test = np.array(test_data.target)
 
 print(x_train.shape)  # (25000,)
 print(y_train.shape)  # (25000, 1)
@@ -57,7 +56,7 @@ clf = ak.TextClassifier(
     overwrite=True, max_trials=1
 )  # It only tries 1 model as a quick demo.
 # Feed the text classifier with training data.
-clf.fit(x_train, y_train, epochs=1, batch_size=2)
+clf.fit(x_train, y_train, epochs=2)
 # Predict with the best model.
 predicted_y = clf.predict(x_test)
 # Evaluate the best model with testing data.
@@ -76,8 +75,6 @@ clf.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
-    batch_size=2,
 )
 
 """
@@ -85,7 +82,7 @@ You can also use your own validation set instead of splitting it from the
 training data with `validation_data`.
 """
 
-split = 5
+split = 5000
 x_val = x_train[split:]
 y_val = y_train[split:]
 x_train = x_train[:split]
@@ -93,10 +90,9 @@ y_train = y_train[:split]
 clf.fit(
     x_train,
     y_train,
-    epochs=1,
+    epochs=2,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    batch_size=2,
 )
 
 """
@@ -104,19 +100,48 @@ clf.fit(
 For advanced users, you may customize your search space by using
 [AutoModel](/auto_model/#automodel-class) instead of
 [TextClassifier](/text_classifier). You can configure the
-[TextBlock](/block/#textblock-class) for some high-level configurations. You can
-also do not specify these arguments, which would leave the different choices to
-be tuned automatically.  See the following example for detail.
+[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,
+`vectorizer` for the type of text vectorization method to use.  You can use
+'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to
+convert the words to integers and use [Embedding](/block/#embedding-class) for
+embedding the integer sequences, or you can use 'ngram', which uses
+[TextToNgramVector](/block/#texttongramvector-class) to vectorize the
+sentences.  You can also do not specify these arguments, which would leave the
+different choices to be tuned automatically.  See the following example for
+detail.
 """
 
 
 input_node = ak.TextInput()
-output_node = ak.TextBlock()(input_node)
+output_node = ak.TextBlock(block_type="ngram")(input_node)
 output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1, batch_size=2)
+clf.fit(x_train, y_train, epochs=2)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.TextInput()
+output_node = ak.TextToIntSequence()(input_node)
+output_node = ak.Embedding()(output_node)
+# Use separable Conv layers in Keras.
+output_node = ak.ConvBlock(separable=True)(output_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+clf.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -132,22 +157,26 @@ format for the training data.
 """
 
 train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(
-    2
+    32
 )
-test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)
+test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)
 
-clf = ak.TextClassifier(overwrite=True, max_trials=1)
+clf = ak.TextClassifier(overwrite=True, max_trials=2)
 # Feed the tensorflow Dataset to the classifier.
-clf.fit(train_set.take(2), epochs=1)
+clf.fit(train_set, epochs=2)
 # Predict with the best model.
-predicted_y = clf.predict(test_set.take(2))
+predicted_y = clf.predict(test_set)
 # Evaluate the best model with testing data.
-print(clf.evaluate(test_set.take(2)))
+print(clf.evaluate(test_set))
 
 """
 ## Reference
 [TextClassifier](/text_classifier),
 [AutoModel](/auto_model/#automodel-class),
+[TextBlock](/block/#textblock-class),
+[TextToInteSequence](/block/#texttointsequence-class),
+[Embedding](/block/#embedding-class),
+[TextToNgramVector](/block/#texttongramvector-class),
 [ConvBlock](/block/#convblock-class),
 [TextInput](/node/#textinput-class),
 [ClassificationHead](/block/#classificationhead-class).
diff --git a/docs/py/text_regression.py b/docs/py/text_regression.py
index 59d1058..d934809 100644
--- a/docs/py/text_regression.py
+++ b/docs/py/text_regression.py
@@ -1,7 +1,6 @@
 """shell
 pip install autokeras
 """
-
 import os
 
 import numpy as np
@@ -40,10 +39,10 @@ test_data = load_files(
     os.path.join(IMDB_DATADIR, "test"), shuffle=False, categories=classes
 )
 
-x_train = np.array(train_data.data)[:100]
-y_train = np.array(train_data.target)[:100]
-x_test = np.array(test_data.data)[:100]
-y_test = np.array(test_data.target)[:100]
+x_train = np.array(train_data.data)
+y_train = np.array(train_data.target)
+x_test = np.array(test_data.data)
+y_test = np.array(test_data.target)
 
 print(x_train.shape)  # (25000,)
 print(y_train.shape)  # (25000, 1)
@@ -58,10 +57,10 @@ adaptive number of epochs.
 
 # Initialize the text regressor.
 reg = ak.TextRegressor(
-    overwrite=True, max_trials=1  # It tries 10 different models.
+    overwrite=True, max_trials=10  # It tries 10 different models.
 )
 # Feed the text regressor with training data.
-reg.fit(x_train, y_train, epochs=1, batch_size=2)
+reg.fit(x_train, y_train, epochs=2)
 # Predict with the best model.
 predicted_y = reg.predict(x_test)
 # Evaluate the best model with testing data.
@@ -87,7 +86,7 @@ You can also use your own validation set instead of splitting it from the
 training data with `validation_data`.
 """
 
-split = 5
+split = 5000
 x_val = x_train[split:]
 y_val = y_train[split:]
 x_train = x_train[:split]
@@ -95,10 +94,9 @@ y_train = y_train[:split]
 reg.fit(
     x_train,
     y_train,
-    epochs=1,
+    epochs=2,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    batch_size=2,
 )
 
 """
@@ -106,19 +104,48 @@ reg.fit(
 For advanced users, you may customize your search space by using
 [AutoModel](/auto_model/#automodel-class) instead of
 [TextRegressor](/text_regressor). You can configure the
-[TextBlock](/block/#textblock-class) for some high-level configurations. You can
-also do not specify these arguments, which would leave the different choices to
-be tuned automatically.  See the following example for detail.
+[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,
+`vectorizer` for the type of text vectorization method to use.  You can use
+'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to
+convert the words to integers and use [Embedding](/block/#embedding-class) for
+embedding the integer sequences, or you can use 'ngram', which uses
+[TextToNgramVector](/block/#texttongramvector-class) to vectorize the
+sentences.  You can also do not specify these arguments, which would leave the
+different choices to be tuned automatically.  See the following example for
+detail.
+"""
+
+
+input_node = ak.TextInput()
+output_node = ak.TextBlock(block_type="ngram")(input_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+reg.fit(x_train, y_train, epochs=2)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
 """
 
 
 input_node = ak.TextInput()
-output_node = ak.TextBlock()(input_node)
+output_node = ak.TextToIntSequence()(input_node)
+output_node = ak.Embedding()(output_node)
+# Use separable Conv layers in Keras.
+output_node = ak.ConvBlock(separable=True)(output_node)
 output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1, batch_size=2)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -135,23 +162,26 @@ format for the training data.
 
 
 train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(
-    2
+    32
 )
-test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)
+test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)
 
 reg = ak.TextRegressor(overwrite=True, max_trials=2)
 # Feed the tensorflow Dataset to the regressor.
-reg.fit(train_set.take(2), epochs=1)
+reg.fit(train_set, epochs=2)
 # Predict with the best model.
-predicted_y = reg.predict(test_set.take(2))
+predicted_y = reg.predict(test_set)
 # Evaluate the best model with testing data.
-print(reg.evaluate(test_set.take(2)))
+print(reg.evaluate(test_set))
 
 """
 ## Reference
 [TextRegressor](/text_regressor),
 [AutoModel](/auto_model/#automodel-class),
 [TextBlock](/block/#textblock-class),
+[TextToInteSequence](/block/#texttointsequence-class),
+[Embedding](/block/#embedding-class),
+[TextToNgramVector](/block/#texttongramvector-class),
 [ConvBlock](/block/#convblock-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class).
diff --git a/docs/py/timeseries_forecaster.py b/docs/py/timeseries_forecaster.py
new file mode 100644
index 0000000..5eeda09
--- /dev/null
+++ b/docs/py/timeseries_forecaster.py
@@ -0,0 +1,123 @@
+"""shell
+pip install autokeras
+"""
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+To make this tutorial easy to follow, we use the UCI Airquality dataset, and try
+to forecast the AH value at the different timesteps. Some basic preprocessing
+has also been performed on the dataset as it required cleanup.
+
+## A Simple Example
+The first step is to prepare your data. Here we use the [UCI Airquality
+dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality) as an example.
+"""
+
+dataset = tf.keras.utils.get_file(
+    fname="AirQualityUCI.csv",
+    origin="https://archive.ics.uci.edu/ml/machine-learning-databases/00360/"
+    "AirQualityUCI.zip",
+    extract=True,
+)
+
+dataset = pd.read_csv(dataset, sep=";")
+dataset = dataset[dataset.columns[:-2]]
+dataset = dataset.dropna()
+dataset = dataset.replace(",", ".", regex=True)
+
+val_split = int(len(dataset) * 0.7)
+data_train = dataset[:val_split]
+validation_data = dataset[val_split:]
+
+data_x = data_train[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+data_x_val = validation_data[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+# Data with train data and the unseen data from subsequent time steps.
+data_x_test = dataset[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+data_y = data_train["AH"].astype("float64")
+
+data_y_val = validation_data["AH"].astype("float64")
+
+print(data_x.shape)  # (6549, 12)
+print(data_y.shape)  # (6549,)
+
+"""
+The second step is to run the [TimeSeriesForecaster](/time_series_forecaster).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+predict_from = 1
+predict_until = 10
+lookback = 3
+clf = ak.TimeseriesForecaster(
+    lookback=lookback,
+    predict_from=predict_from,
+    predict_until=predict_until,
+    max_trials=1,
+    objective="val_loss",
+)
+# Train the TimeSeriesForecaster with train data
+clf.fit(
+    x=data_x,
+    y=data_y,
+    validation_data=(data_x_val, data_y_val),
+    batch_size=32,
+    epochs=10,
+)
+# Predict with the best model(includes original training data).
+predictions = clf.predict(data_x_test)
+print(predictions.shape)
+# Evaluate the best model with testing data.
+print(clf.evaluate(data_x_val, data_y_val))
diff --git a/docs/templates/benchmarks.md b/docs/templates/benchmarks.md
index d77c382..4f70a6d 100644
--- a/docs/templates/benchmarks.md
+++ b/docs/templates/benchmarks.md
@@ -7,4 +7,6 @@ Tested on a single NVIDIA Tesla V100 GPU.
 | - | - | - | - | - |
 | [MNIST](http://yann.lecun.com/exdb/mnist/)  | ImageClassifier| Accuracy | 99.04% | 0.51 |
 | [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)   | ImageClassifier| Accuracy | 97.10% | 1.8 |
-| [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)  | TextClassifier | Accuracy | 93.93% | 1.2 |
\ No newline at end of file
+| [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)  | TextClassifier | Accuracy | 93.93% | 1.2 |
+| [Titanic](https://www.tensorflow.org/datasets/catalog/titanic)  | StructuredDataClassifier | Accuracy | 82.20% | 0.007 |
+| [California Housing](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset)  | StructuredDataRegression | MSE | 0.23 | 0.06 |
\ No newline at end of file
diff --git a/docs/templates/tutorial/faq.md b/docs/templates/tutorial/faq.md
index 57c1b2a..af4d615 100644
--- a/docs/templates/tutorial/faq.md
+++ b/docs/templates/tutorial/faq.md
@@ -55,7 +55,7 @@ clf = ak.ImageClassifier(
 
 ## How to use multiple GPUs?
 You can use the `distribution_strategy` argument when initializing any model you created with AutoKeras,
-like AutoModel, ImageClassifier and so on. This argument is supported by Keras Tuner.
+like AutoModel, ImageClassifier, StructuredDataRegressor and so on. This argument is supported by Keras Tuner.
 AutoKeras supports the arguments supported by Keras Tuner.
 Please see the discription of the argument [here](https://keras-team.github.io/keras-tuner/documentation/tuners/#tuner-class).
 
diff --git a/docs/templates/tutorial/overview.md b/docs/templates/tutorial/overview.md
index a472b8b..4776554 100644
--- a/docs/templates/tutorial/overview.md
+++ b/docs/templates/tutorial/overview.md
@@ -15,6 +15,12 @@ You can click the links below to see the detailed tutorial for each task.
 
 [Text Regression](/tutorial/text_regression)
 
+[Structured Data Classification](/tutorial/structured_data_classification)
+
+[Structured Data Regression](/tutorial/structured_data_regression)
+
+**Coming Soon**: Time Series Forecasting, Object Detection, Image Segmentation.
+
 
 ## Multi-Task and Multi-Modal Data
 
@@ -36,6 +42,8 @@ The following are the links to the documentation of the predefined input nodes a
 
 [Input](/node/#input-class)
 
+[StructuredDataInput](/node/#structureddatainput-class)
+
 [TextInput](/node/#textinput-class)
 
 **Blocks**:
@@ -44,10 +52,18 @@ The following are the links to the documentation of the predefined input nodes a
 
 [Normalization](/block/#normalization-class)
 
+[TextToIntSequence](/block/#texttointsequence-class)
+
+[TextToNgramVector](/block/#texttongramvector-class)
+
+[CategoricalToNumerical](/block/#categoricaltonumerical-class)
+
 [ConvBlock](/block/#convblock-class)
 
 [DenseBlock](/block/#denseblock-class)
 
+[Embedding](/block/#embedding-class)
+
 [Merge](/block/#merge-class)
 
 [ResNetBlock](/block/#resnetblock-class)
@@ -62,6 +78,8 @@ The following are the links to the documentation of the predefined input nodes a
 
 [ImageBlock](/block/#imageblock-class)
 
+[StructuredDataBlock](/block/#structureddatablock-class)
+
 [TextBlock](/block/#textblock-class)
 
 [ClassificationHead](/block/#classificationhead-class)
diff --git a/docs/tutobooks.py b/docs/tutobooks.py
index 76226a1..502a739 100644
--- a/docs/tutobooks.py
+++ b/docs/tutobooks.py
@@ -60,7 +60,6 @@ you expect. If not, keep editing `your_example.py` until it does.
 
 Finally, submit a PR adding `examples/your_example.py`.
 """
-
 import json
 import os
 import random
diff --git a/examples/automodel_with_cnn.py b/examples/automodel_with_cnn.py
index 9c26056..d7ec20b 100644
--- a/examples/automodel_with_cnn.py
+++ b/examples/automodel_with_cnn.py
@@ -1,6 +1,6 @@
 # Library import
-import keras
 import numpy as np
+import tensorflow as tf
 
 import autokeras as ak
 
@@ -45,6 +45,6 @@ model = auto_model.export_model()
 print(type(model.summary()))
 
 # print model as image
-keras.utils.plot_model(
-    model, show_shapes=True, expand_treeed=True, to_file="name.png"
+tf.keras.utils.plot_model(
+    model, show_shapes=True, expand_nested=True, to_file="name.png"
 )
diff --git a/examples/celeb_age.py b/examples/celeb_age.py
index 5a1bb9b..b4446ba 100644
--- a/examples/celeb_age.py
+++ b/examples/celeb_age.py
@@ -13,7 +13,6 @@ First, prepare your image data in a numpy.ndarray or tensorflow.Dataset format.
 Each image must have the same shape, meaning each has the same width, height,
 and color channels as other images in the set.
 """
-
 from datetime import datetime
 from datetime import timedelta
 
diff --git a/examples/cifar10.py b/examples/cifar10.py
index 4555a0a..75a28df 100644
--- a/examples/cifar10.py
+++ b/examples/cifar10.py
@@ -1,4 +1,4 @@
-from keras.datasets import cifar10
+from tensorflow.keras.datasets import cifar10
 
 import autokeras as ak
 
diff --git a/examples/imdb.py b/examples/imdb.py
index cb024e4..22f48fe 100644
--- a/examples/imdb.py
+++ b/examples/imdb.py
@@ -3,9 +3,8 @@ Search for a good model for the
 [IMDB](
 https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) dataset.
 """
-
-import keras
 import numpy as np
+import tensorflow as tf
 
 import autokeras as ak
 
@@ -14,7 +13,7 @@ def imdb_raw():
     max_features = 20000
     index_offset = 3  # word index offset
 
-    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(
+    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(
         num_words=max_features, index_from=index_offset
     )
     x_train = x_train
@@ -22,7 +21,7 @@ def imdb_raw():
     x_test = x_test
     y_test = y_test.reshape(-1, 1)
 
-    word_to_id = keras.datasets.imdb.get_word_index()
+    word_to_id = tf.keras.datasets.imdb.get_word_index()
     word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}
     word_to_id["<PAD>"] = 0
     word_to_id["<START>"] = 1
diff --git a/examples/iris.py b/examples/iris.py
new file mode 100644
index 0000000..70a1dda
--- /dev/null
+++ b/examples/iris.py
@@ -0,0 +1,71 @@
+"""shell
+pip install -q -U autokeras==1.0.5
+pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
+"""
+
+import os
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+Search for a good model for the
+[iris](https://www.tensorflow.org/datasets/catalog/iris) dataset.
+"""
+
+
+# Prepare the dataset.
+train_dataset_url = "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv"  # noqa: E501
+train_dataset_fp = tf.keras.utils.get_file(
+    fname=os.path.basename(train_dataset_url), origin=train_dataset_url
+)
+
+test_dataset_url = (
+    "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv"
+)
+test_dataset_fp = tf.keras.utils.get_file(
+    fname=os.path.basename(test_dataset_url), origin=test_dataset_url
+)
+
+column_names = [
+    "sepal_length",
+    "sepal_width",
+    "petal_length",
+    "petal_width",
+    "species",
+]
+feature_names = column_names[:-1]
+label_name = column_names[-1]
+class_names = ["Iris setosa", "Iris versicolor", "Iris virginica"]
+
+train = pd.read_csv(train_dataset_fp, names=column_names, header=0)
+
+test = pd.read_csv(test_dataset_fp, names=column_names, header=0)
+
+print(train.shape)  # (120, 5)
+print(test.shape)  # (30, 5)
+
+# Initialize the StructuredDataClassifier.
+clf = ak.StructuredDataClassifier(
+    max_trials=5,
+    overwrite=True,
+)
+# Search for the best model with EarlyStopping.
+cbs = [
+    tf.keras.callbacks.EarlyStopping(patience=3),
+]
+
+clf.fit(
+    x=train[feature_names],
+    y=train[label_name],
+    epochs=200,
+    callbacks=cbs,
+)
+# Evaluate on the testing data.
+print(
+    "Accuracy: {accuracy}".format(
+        accuracy=clf.evaluate(x=test[feature_names], y=test[label_name])
+    )
+)
diff --git a/examples/mnist.py b/examples/mnist.py
index 26c2472..01baaef 100644
--- a/examples/mnist.py
+++ b/examples/mnist.py
@@ -4,7 +4,7 @@ Search for a good model for the
 dataset.
 """
 
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
diff --git a/examples/new_pop.py b/examples/new_pop.py
index b385cfd..7bf9551 100644
--- a/examples/new_pop.py
+++ b/examples/new_pop.py
@@ -1,7 +1,6 @@
 """shell
 pip install autokeras
 """
-
 import pandas as pd
 
 import autokeras as ak
@@ -43,3 +42,28 @@ reg.fit(text_inputs, media_success_outputs)
 
 # Predict with the chosen model:
 predict_y = reg.predict(text_inputs)
+
+"""
+If your text source has a larger vocabulary (number of distinct words), you may
+need to create a custom pipeline in AutoKeras to increase the `max_tokens`
+parameter.
+"""
+
+text_input = (df.Title + " " + df.Headline).to_numpy(dtype="str")
+
+# text input and tokenization
+input_node = ak.TextInput()
+output_node = ak.TextToIntSequence(max_tokens=20000)(input_node)
+
+# regression output
+output_node = ak.RegressionHead()(output_node)
+
+# initialize AutoKeras and find the best model
+reg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=15)
+reg.fit(text_input, media_success_outputs)
+
+"""
+Measure the accuracy of the regressor on an independent test set:
+"""
+
+print(reg.evaluate(text_input, media_success_outputs))
diff --git a/examples/reuters.py b/examples/reuters.py
index 794ba32..27c5abe 100644
--- a/examples/reuters.py
+++ b/examples/reuters.py
@@ -3,10 +3,9 @@
 !pip install -q -U autokeras==1.0.8
 !pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
 """
-
-import keras
 import numpy as np
-from keras.datasets import reuters
+import tensorflow as tf
+from tensorflow.keras.datasets import reuters
 
 import autokeras as ak
 
@@ -60,7 +59,7 @@ clf = ak.TextClassifier(
 
 # Callback to avoid overfitting with the EarlyStopping.
 cbs = [
-    keras.callbacks.EarlyStopping(patience=3),
+    tf.keras.callbacks.EarlyStopping(patience=3),
 ]
 
 # Search for the best model.
diff --git a/examples/titanic.py b/examples/titanic.py
new file mode 100644
index 0000000..979a2b9
--- /dev/null
+++ b/examples/titanic.py
@@ -0,0 +1,41 @@
+"""
+Search for a good model for the [Titanic](https://www.kaggle.com/c/titanic)
+dataset.
+"""
+
+import timeit
+
+import tensorflow as tf
+
+import autokeras as ak
+
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+
+def main():
+    # Initialize the classifier.
+    train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+    test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+    clf = ak.StructuredDataClassifier(
+        max_trials=10, directory="tmp_dir", overwrite=True
+    )
+
+    start_time = timeit.default_timer()
+    # x is the path to the csv file. y is the column name of the column to
+    # predict.
+    clf.fit(train_file_path, "survived")
+    stop_time = timeit.default_timer()
+
+    # Evaluate the accuracy of the found model.
+    accuracy = clf.evaluate(test_file_path, "survived")[1]
+    print("Accuracy: {accuracy}%".format(accuracy=round(accuracy * 100, 2)))
+    print(
+        "Total time: {time} seconds.".format(
+            time=round(stop_time - start_time, 2)
+        )
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/wine.py b/examples/wine.py
new file mode 100644
index 0000000..34f0cee
--- /dev/null
+++ b/examples/wine.py
@@ -0,0 +1,67 @@
+"""
+Run the following commands first
+pip3 install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
+pip3 install autokeras==1.0.5
+
+This Script searches for a model for the wine dataset
+Source and Description of data:
+"""
+import os
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+dataset_url = (
+    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
+)
+
+# save data
+data_file_path = tf.keras.utils.get_file(
+    fname=os.path.basename(dataset_url), origin=dataset_url
+)
+
+column_names = [
+    "Wine",
+    "Alcohol",
+    "Malic.acid",
+    "Ash",
+    "Acl",
+    "Mg",
+    "Phenols",
+    "Flavanoids",
+    "Nonflavanoid.phenols",
+    "Proanth",
+    "Color.int",
+    "Hue",
+    "OD",
+    "Proline",
+]
+
+feature_names = column_names[1:]
+label_name = column_names[0]  # Wine
+
+data = pd.read_csv(data_file_path, header=0, names=column_names)
+# Shuffling
+data = data.sample(frac=1)
+
+split_length = int(data.shape[0] * 0.8)  # 141
+
+# train and test
+train_data = data.iloc[:split_length]
+test_data = data.iloc[split_length:]
+
+
+# Initialize the classifier.
+clf = ak.StructuredDataClassifier(max_trials=5)
+
+# Evaluate
+clf.fit(x=train_data[feature_names], y=train_data[label_name])
+print(
+    "Accuracy: {accuracy}".format(
+        accuracy=clf.evaluate(
+            x=test_data[feature_names], y=test_data[label_name]
+        )
+    )
+)