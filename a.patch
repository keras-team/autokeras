diff --git a/docs/py/customized.py b/docs/py/customized.py
index 86a1fc8..acf76ed 100644
--- a/docs/py/customized.py
+++ b/docs/py/customized.py
@@ -1,11 +1,9 @@
 """shell
 pip install autokeras
 """
-
-import keras
 import numpy as np
-import tree
-from keras.datasets import mnist
+import tensorflow as tf
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -86,19 +84,24 @@ If you would like to provide your own validation data or change the ratio of
 the validation data, please refer to the Validation Data section of the
 tutorials of [Image
 Classification](/tutorial/image_classification/#validation-data), [Text
-Classification](/tutorial/text_classification/#validation-data),
+Classification](/tutorial/text_classification/#validation-data), [Structured
+Data
+Classification](/tutorial/structured_data_classification/#validation-data),
 [Multi-task and Multiple Validation](/tutorial/multi/#validation-data).
 
 ## Data Format
 You can refer to the documentation of
 [ImageInput](/node/#imageinput-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
 for the format of different types of data.
 You can also refer to the Data Format section of the tutorials of
 [Image Classification](/tutorial/image_classification/#data-format),
-[Text Classification](/tutorial/text_classification/#data-format).
+[Text Classification](/tutorial/text_classification/#data-format),
+[Structured Data
+Classification](/tutorial/structured_data_classification/#data-format).
 
 ## Implement New Block
 
@@ -117,8 +120,8 @@ number of neurons is tunable.
 class SingleDenseLayerBlock(ak.Block):
     def build(self, hp, inputs=None):
         # Get the input_node from inputs.
-        input_node = tree.flatten(inputs)[0]
-        layer = keras.layers.Dense(
+        input_node = tf.nest.flatten(inputs)[0]
+        layer = tf.keras.layers.Dense(
             hp.Int("num_units", min_value=32, max_value=512, step=32)
         )
         output_node = layer(input_node)
@@ -153,6 +156,7 @@ print(auto_model.evaluate(x_test, y_test))
 **Nodes**:
 [ImageInput](/node/#imageinput-class),
 [Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class).
 
 **Preprocessors**:
@@ -160,10 +164,13 @@ print(auto_model.evaluate(x_test, y_test))
 [ImageAugmentation](/block/#imageaugmentation-class),
 [LightGBM](/block/#lightgbm-class),
 [Normalization](/block/#normalization-class),
+[TextToIntSequence](/block/#texttointsequence-class),
+[TextToNgramVector](/block/#texttongramvector-class).
 
 **Blocks**:
 [ConvBlock](/block/#convblock-class),
 [DenseBlock](/block/#denseblock-class),
+[Embedding](/block/#embedding-class),
 [Merge](/block/#merge-class),
 [ResNetBlock](/block/#resnetblock-class),
 [RNNBlock](/block/#rnnblock-class),
@@ -171,5 +178,6 @@ print(auto_model.evaluate(x_test, y_test))
 [TemporalReduction](/block/#temporalreduction-class),
 [XceptionBlock](/block/#xceptionblock-class),
 [ImageBlock](/block/#imageblock-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
 [TextBlock](/block/#textblock-class).
 """
diff --git a/docs/py/export.py b/docs/py/export.py
index c648fa2..7c600c2 100644
--- a/docs/py/export.py
+++ b/docs/py/export.py
@@ -1,10 +1,9 @@
 """shell
 pip install autokeras
 """
-
-import numpy as np
-from keras.datasets import mnist
-from keras.models import load_model
+import tensorflow as tf
+from tensorflow.keras.datasets import mnist
+from tensorflow.keras.models import load_model
 
 import autokeras as ak
 
@@ -19,6 +18,7 @@ All the tasks and the [AutoModel](/auto_model/#automodel-class) has this
 """
 
 
+print(tf.__version__)
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 
 # Initialize the image classifier.
@@ -32,12 +32,13 @@ model = clf.export_model()
 
 print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>
 
-model.save("model_autokeras.keras")
+try:
+    model.save("model_autokeras", save_format="tf")
+except Exception:
+    model.save("model_autokeras.h5")
 
 
-loaded_model = load_model(
-    "model_autokeras.keras", custom_objects=ak.CUSTOM_OBJECTS
-)
+loaded_model = load_model("model_autokeras", custom_objects=ak.CUSTOM_OBJECTS)
 
-predicted_y = loaded_model.predict(np.expand_dims(x_test, -1))
+predicted_y = loaded_model.predict(tf.expand_dims(x_test, -1))
 print(predicted_y)
diff --git a/docs/py/image_classification.py b/docs/py/image_classification.py
index 79c929e..6886227 100644
--- a/docs/py/image_classification.py
+++ b/docs/py/image_classification.py
@@ -1,10 +1,9 @@
 """shell
 pip install autokeras
 """
-
 import numpy as np
 import tensorflow as tf
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -15,10 +14,6 @@ example
 """
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
-x_train = x_train[:100]
-y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 print(x_train.shape)  # (60000, 28, 28)
 print(y_train.shape)  # (60000,)
 print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
@@ -34,7 +29,7 @@ You can also leave the epochs unspecified for an adaptive number of epochs.
 # Initialize the image classifier.
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
 # Feed the image classifier with training data.
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 
 # Predict with the best model.
@@ -57,7 +52,7 @@ clf.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
+    epochs=10,
 )
 
 """
@@ -75,7 +70,7 @@ clf.fit(
     y_train,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    epochs=1,
+    epochs=10,
 )
 
 """
@@ -102,7 +97,7 @@ output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 """
 The usage of AutoModel is similar to the functional API of Keras. Basically, you
@@ -122,7 +117,7 @@ output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 """
 ## Data Format
@@ -169,7 +164,7 @@ test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))
 
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
 # Feed the tensorflow Dataset to the classifier.
-clf.fit(train_set, epochs=1)
+clf.fit(train_set, epochs=10)
 # Predict with the best model.
 predicted_y = clf.predict(test_set)
 # Evaluate the best model with testing data.
diff --git a/docs/py/image_regression.py b/docs/py/image_regression.py
index dfe4781..aaf847f 100644
--- a/docs/py/image_regression.py
+++ b/docs/py/image_regression.py
@@ -3,7 +3,7 @@ pip install autokeras
 """
 
 import tensorflow as tf
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -22,8 +22,6 @@ example
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 x_train = x_train[:100]
 y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 print(x_train.shape)  # (60000, 28, 28)
 print(y_train.shape)  # (60000,)
 print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
@@ -31,14 +29,14 @@ print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
 """
 The second step is to run the ImageRegressor.  It is recommended have more
 trials for more complicated datasets.  This is just a quick demo of MNIST, so
-we set max_trials to 1.  For the same reason, we set epochs to 1.  You can also
+we set max_trials to 1.  For the same reason, we set epochs to 2.  You can also
 leave the epochs unspecified for an adaptive number of epochs.
 """
 
 # Initialize the image regressor.
 reg = ak.ImageRegressor(overwrite=True, max_trials=1)
 # Feed the image regressor with training data.
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 
 # Predict with the best model.
@@ -61,7 +59,7 @@ reg.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
+    epochs=2,
 )
 
 """
@@ -106,7 +104,7 @@ output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 The usage of AutoModel is similar to the functional API of Keras. Basically,
@@ -126,7 +124,7 @@ output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -146,10 +144,6 @@ case, the images would have to be 3-dimentional.
 """
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
-x_train = x_train[:100]
-y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 
 # Reshape the images to have the channel dimension.
 x_train = x_train.reshape(x_train.shape + (1,))
@@ -165,7 +159,7 @@ test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))
 
 reg = ak.ImageRegressor(overwrite=True, max_trials=1)
 # Feed the tensorflow Dataset to the regressor.
-reg.fit(train_set, epochs=1)
+reg.fit(train_set, epochs=2)
 # Predict with the best model.
 predicted_y = reg.predict(test_set)
 # Evaluate the best model with testing data.
diff --git a/docs/py/load.py b/docs/py/load.py
index bc43a3f..1f8df90 100644
--- a/docs/py/load.py
+++ b/docs/py/load.py
@@ -5,7 +5,6 @@ pip install autokeras
 import os
 import shutil
 
-import keras
 import numpy as np
 import tensorflow as tf
 
@@ -24,7 +23,7 @@ First, we download the data and extract the files.
 """
 
 dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"  # noqa: E501
-local_file_path = keras.utils.get_file(
+local_file_path = tf.keras.utils.get_file(
     origin=dataset_url, fname="image_data", extract=True
 )
 # The file is extracted in the same directory as the downloaded file.
@@ -49,7 +48,7 @@ flowers_photos/
 We can split the data into training and testing as we load them.
 """
 
-batch_size = 2
+batch_size = 32
 img_height = 180
 img_width = 180
 
@@ -78,8 +77,8 @@ Then we just do one quick demo of AutoKeras to make sure the dataset works.
 """
 
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
-clf.fit(train_data.take(100), epochs=1)
-print(clf.evaluate(test_data.take(2)))
+clf.fit(train_data, epochs=1)
+print(clf.evaluate(test_data))
 
 """
 ## Load Texts from Disk
@@ -88,7 +87,7 @@ You can also load text datasets in the same way.
 
 dataset_url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
 
-local_file_path = keras.utils.get_file(
+local_file_path = tf.keras.utils.get_file(
     fname="text_data",
     origin=dataset_url,
     extract=True,
@@ -117,8 +116,8 @@ test_data = ak.text_dataset_from_directory(
 )
 
 clf = ak.TextClassifier(overwrite=True, max_trials=1)
-clf.fit(train_data.take(2), epochs=1)
-print(clf.evaluate(test_data.take(2)))
+clf.fit(train_data, epochs=2)
+print(clf.evaluate(test_data))
 
 
 """
@@ -127,29 +126,33 @@ If you want to use generators, you can refer to the following code.
 """
 
 
-N_BATCHES = 2
-BATCH_SIZE = 10
+N_BATCHES = 30
+BATCH_SIZE = 100
+N_FEATURES = 10
 
 
-def get_data_generator(n_batches, batch_size):
-    """Get a generator returning n_batches random data."""
+def get_data_generator(n_batches, batch_size, n_features):
+    """Get a generator returning n_batches random data.
+
+    The shape of the data is (batch_size, n_features).
+    """
 
     def data_generator():
         for _ in range(n_batches * batch_size):
-            x = np.random.randn(32, 32, 3)
-            y = x.sum() / 32 * 32 * 3 > 0.5
+            x = np.random.randn(n_features)
+            y = x.sum(axis=0) / n_features > 0.5
             yield x, y
 
     return data_generator
 
 
 dataset = tf.data.Dataset.from_generator(
-    get_data_generator(N_BATCHES, BATCH_SIZE),
+    get_data_generator(N_BATCHES, BATCH_SIZE, N_FEATURES),
     output_types=(tf.float32, tf.float32),
-    output_shapes=((32, 32, 3), tuple()),
+    output_shapes=((N_FEATURES,), tuple()),
 ).batch(BATCH_SIZE)
 
-clf = ak.ImageClassifier(overwrite=True, max_trials=1, seed=5)
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=1, seed=5)
 clf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)
 print(clf.evaluate(dataset))
 
diff --git a/docs/py/multi.py b/docs/py/multi.py
index c88e5e4..31e7284 100644
--- a/docs/py/multi.py
+++ b/docs/py/multi.py
@@ -16,7 +16,7 @@ In this tutorial we are making use of the
 Multi-modal data means each data instance has multiple forms of information.
 For example, a photo can be saved as a image. Besides the image, it may also
 have when and where it was taken as its attributes, which can be represented as
-numerical data.
+structured data.
 
 ## What is multi-task?
 
@@ -31,28 +31,27 @@ network model.
 <div class="mermaid">
 graph TD
     id1(ImageInput) --> id3(Some Neural Network Model)
-    id2(Input) --> id3
+    id2(StructuredDataInput) --> id3
     id3 --> id4(ClassificationHead)
     id3 --> id5(RegressionHead)
 </div>
 
-It has two inputs the images and the numerical input data. Each image is
-associated with a set of attributes in the numerical input data. From these
-data, we are trying to predict the classification label and the regression value
-at the same time.
+It has two inputs the images and the structured data. Each image is associated
+with a set of attributes in the structured data. From these data, we are trying
+to predict the classification label and the regression value at the same time.
 
 ## Data Preparation
 
-To illustrate our idea, we generate some random image and numerical data as
+To illustrate our idea, we generate some random image and structured data as
 the multi-modal data.
 """
 
 
-num_instances = 10
+num_instances = 100
 # Generate image data.
 image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)
-# Generate numerical data.
-numerical_data = np.random.rand(num_instances, 20).astype(np.float32)
+# Generate structured data.
+structured_data = np.random.rand(num_instances, 20).astype(np.float32)
 
 """
 We also generate some multi-task targets for classification and regression.
@@ -73,7 +72,7 @@ Since this is just a demo, we use small amount of `max_trials` and `epochs`.
 
 # Initialize the multi with multiple inputs and outputs.
 model = ak.AutoModel(
-    inputs=[ak.ImageInput(), ak.Input()],
+    inputs=[ak.ImageInput(), ak.StructuredDataInput()],
     outputs=[
         ak.RegressionHead(metrics=["mae"]),
         ak.ClassificationHead(
@@ -85,10 +84,9 @@ model = ak.AutoModel(
 )
 # Fit the model with prepared data.
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
-    epochs=1,
-    batch_size=3,
+    epochs=3,
 )
 
 """
@@ -99,12 +97,11 @@ percentage.
 """
 
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
-    batch_size=3,
+    epochs=2,
 )
 
 """
@@ -112,28 +109,27 @@ You can also use your own validation set
 instead of splitting it from the training data with `validation_data`.
 """
 
-split = 5
+split = 20
 
 image_val = image_data[split:]
-numerical_val = numerical_data[split:]
+structured_val = structured_data[split:]
 regression_val = regression_target[split:]
 classification_val = classification_target[split:]
 
 image_data = image_data[:split]
-numerical_data = numerical_data[:split]
+structured_data = structured_data[:split]
 regression_target = regression_target[:split]
 classification_target = classification_target[:split]
 
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
     # Use your own validation set.
     validation_data=(
-        [image_val, numerical_val],
+        [image_val, structured_val],
         [regression_val, classification_val],
     ),
-    epochs=1,
-    batch_size=3,
+    epochs=2,
 )
 
 """
@@ -149,7 +145,8 @@ graph LR
     id3 --> id5(ResNet V2)
     id4 --> id6(Merge)
     id5 --> id6
-    id7(Input) --> id9(DenseBlock)
+    id7(StructuredDataInput) --> id8(CategoricalToNumerical)
+    id8 --> id9(DenseBlock)
     id6 --> id10(Merge)
     id9 --> id10
     id10 --> id11(Classification Head)
@@ -164,8 +161,9 @@ output_node1 = ak.ConvBlock()(output_node)
 output_node2 = ak.ResNetBlock(version="v2")(output_node)
 output_node1 = ak.Merge()([output_node1, output_node2])
 
-input_node2 = ak.Input()
-output_node2 = ak.DenseBlock()(input_node2)
+input_node2 = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node2)
+output_node2 = ak.DenseBlock()(output_node)
 
 output_node = ak.Merge()([output_node1, output_node2])
 output_node1 = ak.ClassificationHead()(output_node)
@@ -179,22 +177,22 @@ auto_model = ak.AutoModel(
 )
 
 image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)
-numerical_data = np.random.rand(num_instances, 20).astype(np.float32)
+structured_data = np.random.rand(num_instances, 20).astype(np.float32)
 regression_target = np.random.rand(num_instances, 1).astype(np.float32)
 classification_target = np.random.randint(5, size=num_instances)
 
 auto_model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [classification_target, regression_target],
-    batch_size=3,
-    epochs=1,
+    batch_size=32,
+    epochs=3,
 )
 
 """
 ## Data Format
 You can refer to the documentation of
 [ImageInput](/node/#imageinput-class),
-[Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
@@ -202,11 +200,13 @@ for the format of different types of data.
 You can also refer to the Data Format section of the tutorials of
 [Image Classification](/tutorial/image_classification/#data-format),
 [Text Classification](/tutorial/text_classification/#data-format),
+[Structured Data Classification](
+/tutorial/structured_data_classification/#data-format).
 
 ## Reference
 [AutoModel](/auto_model/#automodel-class),
 [ImageInput](/node/#imageinput-class),
-[Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [DenseBlock](/block/#denseblock-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
diff --git a/docs/py/structured_data_classification.py b/docs/py/structured_data_classification.py
new file mode 100644
index 0000000..30be1f3
--- /dev/null
+++ b/docs/py/structured_data_classification.py
@@ -0,0 +1,234 @@
+"""shell
+pip install autokeras
+"""
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+## A Simple Example
+The first step is to prepare your data. Here we use the [Titanic
+dataset](https://www.kaggle.com/c/titanic) as an example.
+"""
+
+
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+
+"""
+The second step is to run the
+[StructuredDataClassifier](/structured_data_classifier).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+# Initialize the structured data classifier.
+clf = ak.StructuredDataClassifier(
+    overwrite=True, max_trials=3
+)  # It tries 3 different models.
+# Feed the structured data classifier with training data.
+clf.fit(
+    # The path to the train.csv file.
+    train_file_path,
+    # The name of the label column.
+    "survived",
+    epochs=10,
+)
+# Predict with the best model.
+predicted_y = clf.predict(test_file_path)
+# Evaluate the best model with testing data.
+print(clf.evaluate(test_file_path, "survived"))
+
+"""
+## Data Format
+The AutoKeras StructuredDataClassifier is quite flexible for the data format.
+
+The example above shows how to use the CSV files directly. Besides CSV files,
+it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](
+https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The
+data should be two-dimensional with numerical or categorical values.
+
+For the classification labels, AutoKeras accepts both plain labels, i.e. strings
+or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The
+labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series.
+
+The following examples show how the data can be prepared with numpy.ndarray,
+pandas.DataFrame, and tensorflow.data.Dataset.
+"""
+
+
+# x_train as pandas.DataFrame, y_train as pandas.Series
+x_train = pd.read_csv(train_file_path)
+print(type(x_train))  # pandas.DataFrame
+y_train = x_train.pop("survived")
+print(type(y_train))  # pandas.Series
+
+# You can also use pandas.DataFrame for y_train.
+y_train = pd.DataFrame(y_train)
+print(type(y_train))  # pandas.DataFrame
+
+# You can also use numpy.ndarray for x_train and y_train.
+x_train = x_train.to_numpy()
+y_train = y_train.to_numpy()
+print(type(x_train))  # numpy.ndarray
+print(type(y_train))  # numpy.ndarray
+
+# Preparing testing data.
+x_test = pd.read_csv(test_file_path)
+y_test = x_test.pop("survived")
+
+# It tries 10 different models.
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)
+# Feed the structured data classifier with training data.
+clf.fit(x_train, y_train, epochs=10)
+# Predict with the best model.
+predicted_y = clf.predict(x_test)
+# Evaluate the best model with testing data.
+print(clf.evaluate(x_test, y_test))
+
+"""
+The following code shows how to convert numpy.ndarray to tf.data.Dataset.
+"""
+
+train_set = tf.data.Dataset.from_tensor_slices((x_train.astype(str), y_train))
+test_set = tf.data.Dataset.from_tensor_slices(
+    (x_test.to_numpy().astype(str), y_test)
+)
+
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)
+# Feed the tensorflow Dataset to the classifier.
+clf.fit(train_set, epochs=10)
+# Predict with the best model.
+predicted_y = clf.predict(test_set)
+# Evaluate the best model with testing data.
+print(clf.evaluate(test_set))
+
+"""
+You can also specify the column names and types for the data as follows.  The
+`column_names` is optional if the training data already have the column names,
+e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will
+be inferred from the training data.
+"""
+
+# Initialize the structured data classifier.
+clf = ak.StructuredDataClassifier(
+    column_names=[
+        "sex",
+        "age",
+        "n_siblings_spouses",
+        "parch",
+        "fare",
+        "class",
+        "deck",
+        "embark_town",
+        "alone",
+    ],
+    column_types={"sex": "categorical", "fare": "numerical"},
+    max_trials=10,  # It tries 10 different models.
+    overwrite=True,
+)
+
+
+"""
+## Validation Data
+By default, AutoKeras use the last 20% of training data as validation data.  As
+shown in the example below, you can use `validation_split` to specify the
+percentage.
+"""
+
+clf.fit(
+    x_train,
+    y_train,
+    # Split the training data and use the last 15% as validation data.
+    validation_split=0.15,
+    epochs=10,
+)
+
+"""
+You can also use your own validation set
+instead of splitting it from the training data with `validation_data`.
+"""
+
+split = 500
+x_val = x_train[split:]
+y_val = y_train[split:]
+x_train = x_train[:split]
+y_train = y_train[:split]
+clf.fit(
+    x_train,
+    y_train,
+    # Use your own validation set.
+    validation_data=(x_val, y_val),
+    epochs=10,
+)
+
+"""
+## Customized Search Space
+For advanced users, you may customize your search space by using
+[AutoModel](/auto_model/#automodel-class) instead of
+[StructuredDataClassifier](/structured_data_classifier). You can configure the
+[StructuredDataBlock](/block/#structureddatablock-class) for some high-level
+configurations, e.g., `categorical_encoding` for whether to use the
+[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do
+not specify these arguments, which would leave the different choices to be
+tuned automatically. See the following example for detail.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3
+)
+clf.fit(x_train, y_train, epochs=10)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.
+To add an edge from `input_node` to `output_node` with
+`output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node)
+output_node = ak.DenseBlock()(output_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+clf.fit(x_train, y_train, epochs=1)
+clf.predict(x_train)
+
+"""
+You can also export the best model found by AutoKeras as a Keras Model.
+"""
+
+model = clf.export_model()
+model.summary()
+print(x_train.dtype)
+# numpy array in object (mixed type) is not supported.
+# convert it to unicode.
+model.predict(x_train.astype(str))
+
+"""
+## Reference
+[StructuredDataClassifier](/structured_data_classifier),
+[AutoModel](/auto_model/#automodel-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
+[DenseBlock](/block/#denseblock-class),
+[StructuredDataInput](/node/#structureddatainput-class),
+[ClassificationHead](/block/#classificationhead-class),
+[CategoricalToNumerical](/block/#categoricaltonumerical-class).
+"""
diff --git a/docs/py/structured_data_regression.py b/docs/py/structured_data_regression.py
new file mode 100644
index 0000000..236fd0f
--- /dev/null
+++ b/docs/py/structured_data_regression.py
@@ -0,0 +1,239 @@
+"""shell
+pip install autokeras
+"""
+
+import numpy as np
+import pandas as pd
+import tensorflow as tf
+from sklearn.datasets import fetch_california_housing
+
+import autokeras as ak
+
+"""
+## A Simple Example
+The first step is to prepare your data. Here we use the [California housing
+dataset](
+https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)
+as an example.
+"""
+
+
+house_dataset = fetch_california_housing()
+df = pd.DataFrame(
+    np.concatenate(
+        (house_dataset.data, house_dataset.target.reshape(-1, 1)), axis=1
+    ),
+    columns=house_dataset.feature_names + ["Price"],
+)
+train_size = int(df.shape[0] * 0.9)
+df[:train_size].to_csv("train.csv", index=False)
+df[train_size:].to_csv("eval.csv", index=False)
+train_file_path = "train.csv"
+test_file_path = "eval.csv"
+
+"""
+The second step is to run the
+[StructuredDataRegressor](/structured_data_regressor).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+# Initialize the structured data regressor.
+reg = ak.StructuredDataRegressor(
+    overwrite=True, max_trials=3
+)  # It tries 3 different models.
+# Feed the structured data regressor with training data.
+reg.fit(
+    # The path to the train.csv file.
+    train_file_path,
+    # The name of the label column.
+    "Price",
+    epochs=10,
+)
+# Predict with the best model.
+predicted_y = reg.predict(test_file_path)
+# Evaluate the best model with testing data.
+print(reg.evaluate(test_file_path, "Price"))
+
+"""
+## Data Format
+The AutoKeras StructuredDataRegressor is quite flexible for the data format.
+
+The example above shows how to use the CSV files directly. Besides CSV files,
+it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](
+https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The
+data should be two-dimensional with numerical or categorical values.
+
+For the regression targets, it should be a vector of numerical values.
+AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series.
+
+The following examples show how the data can be prepared with numpy.ndarray,
+pandas.DataFrame, and tensorflow.data.Dataset.
+"""
+
+
+# x_train as pandas.DataFrame, y_train as pandas.Series
+x_train = pd.read_csv(train_file_path)
+print(type(x_train))  # pandas.DataFrame
+y_train = x_train.pop("Price")
+print(type(y_train))  # pandas.Series
+
+# You can also use pandas.DataFrame for y_train.
+y_train = pd.DataFrame(y_train)
+print(type(y_train))  # pandas.DataFrame
+
+# You can also use numpy.ndarray for x_train and y_train.
+x_train = x_train.to_numpy()
+y_train = y_train.to_numpy()
+print(type(x_train))  # numpy.ndarray
+print(type(y_train))  # numpy.ndarray
+
+# Preparing testing data.
+x_test = pd.read_csv(test_file_path)
+y_test = x_test.pop("Price")
+
+# It tries 10 different models.
+reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)
+# Feed the structured data regressor with training data.
+reg.fit(x_train, y_train, epochs=10)
+# Predict with the best model.
+predicted_y = reg.predict(x_test)
+# Evaluate the best model with testing data.
+print(reg.evaluate(x_test, y_test))
+
+"""
+The following code shows how to convert numpy.ndarray to tf.data.Dataset.
+"""
+
+train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))
+test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test))
+
+reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)
+# Feed the tensorflow Dataset to the regressor.
+reg.fit(train_set, epochs=10)
+# Predict with the best model.
+predicted_y = reg.predict(test_set)
+# Evaluate the best model with testing data.
+print(reg.evaluate(test_set))
+
+"""
+You can also specify the column names and types for the data as follows.  The
+`column_names` is optional if the training data already have the column names,
+e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will
+be inferred from the training data.
+"""
+
+# Initialize the structured data regressor.
+reg = ak.StructuredDataRegressor(
+    column_names=[
+        "MedInc",
+        "HouseAge",
+        "AveRooms",
+        "AveBedrms",
+        "Population",
+        "AveOccup",
+        "Latitude",
+        "Longitude",
+    ],
+    column_types={"MedInc": "numerical", "Latitude": "numerical"},
+    max_trials=10,  # It tries 10 different models.
+    overwrite=True,
+)
+
+
+"""
+## Validation Data
+By default, AutoKeras use the last 20% of training data as validation data.  As
+shown in the example below, you can use `validation_split` to specify the
+percentage.
+"""
+
+reg.fit(
+    x_train,
+    y_train,
+    # Split the training data and use the last 15% as validation data.
+    validation_split=0.15,
+    epochs=10,
+)
+
+"""
+You can also use your own validation set
+instead of splitting it from the training data with `validation_data`.
+"""
+
+split = 500
+x_val = x_train[split:]
+y_val = y_train[split:]
+x_train = x_train[:split]
+y_train = y_train[:split]
+reg.fit(
+    x_train,
+    y_train,
+    # Use your own validation set.
+    validation_data=(x_val, y_val),
+    epochs=10,
+)
+
+"""
+## Customized Search Space
+For advanced users, you may customize your search space by using
+[AutoModel](/auto_model/#automodel-class) instead of
+[StructuredDataRegressor](/structured_data_regressor). You can configure the
+[StructuredDataBlock](/block/#structureddatablock-class) for some high-level
+configurations, e.g., `categorical_encoding` for whether to use the
+[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do
+not specify these arguments, which would leave the different choices to be
+tuned automatically. See the following example for detail.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3
+)
+reg.fit(x_train, y_train, epochs=10)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node)
+output_node = ak.DenseBlock()(output_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, max_trials=3, overwrite=True
+)
+reg.fit(x_train, y_train, epochs=10)
+
+"""
+You can also export the best model found by AutoKeras as a Keras Model.
+"""
+
+model = reg.export_model()
+model.summary()
+# numpy array in object (mixed type) is not supported.
+# you need convert it to unicode or float first.
+model.predict(x_train)
+
+
+"""
+## Reference
+[StructuredDataRegressor](/structured_data_regressor),
+[AutoModel](/auto_model/#automodel-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
+[DenseBlock](/block/#denseblock-class),
+[StructuredDataInput](/node/#structureddatainput-class),
+[RegressionHead](/block/#regressionhead-class),
+[CategoricalToNumerical](/block/#categoricaltonumerical-class).
+"""
diff --git a/docs/py/text_classification.py b/docs/py/text_classification.py
index 3365df6..fae3def 100644
--- a/docs/py/text_classification.py
+++ b/docs/py/text_classification.py
@@ -4,7 +4,6 @@ pip install autokeras
 
 import os
 
-import keras
 import numpy as np
 import tensorflow as tf
 from sklearn.datasets import load_files
@@ -19,7 +18,7 @@ as an example.
 """
 
 
-dataset = keras.utils.get_file(
+dataset = tf.keras.utils.get_file(
     fname="aclImdb.tar.gz",
     origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
     extract=True,
@@ -36,10 +35,10 @@ test_data = load_files(
     os.path.join(IMDB_DATADIR, "test"), shuffle=False, categories=classes
 )
 
-x_train = np.array(train_data.data)[:100]
-y_train = np.array(train_data.target)[:100]
-x_test = np.array(test_data.data)[:100]
-y_test = np.array(test_data.target)[:100]
+x_train = np.array(train_data.data)
+y_train = np.array(train_data.target)
+x_test = np.array(test_data.data)
+y_test = np.array(test_data.target)
 
 print(x_train.shape)  # (25000,)
 print(y_train.shape)  # (25000, 1)
@@ -57,7 +56,7 @@ clf = ak.TextClassifier(
     overwrite=True, max_trials=1
 )  # It only tries 1 model as a quick demo.
 # Feed the text classifier with training data.
-clf.fit(x_train, y_train, epochs=1, batch_size=2)
+clf.fit(x_train, y_train, epochs=2)
 # Predict with the best model.
 predicted_y = clf.predict(x_test)
 # Evaluate the best model with testing data.
@@ -76,8 +75,6 @@ clf.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
-    batch_size=2,
 )
 
 """
@@ -85,7 +82,7 @@ You can also use your own validation set instead of splitting it from the
 training data with `validation_data`.
 """
 
-split = 5
+split = 5000
 x_val = x_train[split:]
 y_val = y_train[split:]
 x_train = x_train[:split]
@@ -93,10 +90,9 @@ y_train = y_train[:split]
 clf.fit(
     x_train,
     y_train,
-    epochs=1,
+    epochs=2,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    batch_size=2,
 )
 
 """
@@ -104,19 +100,48 @@ clf.fit(
 For advanced users, you may customize your search space by using
 [AutoModel](/auto_model/#automodel-class) instead of
 [TextClassifier](/text_classifier). You can configure the
-[TextBlock](/block/#textblock-class) for some high-level configurations. You can
-also do not specify these arguments, which would leave the different choices to
-be tuned automatically.  See the following example for detail.
+[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,
+`vectorizer` for the type of text vectorization method to use.  You can use
+'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to
+convert the words to integers and use [Embedding](/block/#embedding-class) for
+embedding the integer sequences, or you can use 'ngram', which uses
+[TextToNgramVector](/block/#texttongramvector-class) to vectorize the
+sentences.  You can also do not specify these arguments, which would leave the
+different choices to be tuned automatically.  See the following example for
+detail.
 """
 
 
 input_node = ak.TextInput()
-output_node = ak.TextBlock()(input_node)
+output_node = ak.TextBlock(block_type="ngram")(input_node)
 output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1, batch_size=2)
+clf.fit(x_train, y_train, epochs=2)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.TextInput()
+output_node = ak.TextToIntSequence()(input_node)
+output_node = ak.Embedding()(output_node)
+# Use separable Conv layers in Keras.
+output_node = ak.ConvBlock(separable=True)(output_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+clf.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -132,22 +157,26 @@ format for the training data.
 """
 
 train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(
-    2
+    32
 )
-test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)
+test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)
 
-clf = ak.TextClassifier(overwrite=True, max_trials=1)
+clf = ak.TextClassifier(overwrite=True, max_trials=2)
 # Feed the tensorflow Dataset to the classifier.
-clf.fit(train_set.take(2), epochs=1)
+clf.fit(train_set, epochs=2)
 # Predict with the best model.
-predicted_y = clf.predict(test_set.take(2))
+predicted_y = clf.predict(test_set)
 # Evaluate the best model with testing data.
-print(clf.evaluate(test_set.take(2)))
+print(clf.evaluate(test_set))
 
 """
 ## Reference
 [TextClassifier](/text_classifier),
 [AutoModel](/auto_model/#automodel-class),
+[TextBlock](/block/#textblock-class),
+[TextToInteSequence](/block/#texttointsequence-class),
+[Embedding](/block/#embedding-class),
+[TextToNgramVector](/block/#texttongramvector-class),
 [ConvBlock](/block/#convblock-class),
 [TextInput](/node/#textinput-class),
 [ClassificationHead](/block/#classificationhead-class).
diff --git a/docs/py/text_regression.py b/docs/py/text_regression.py
index 59d1058..d934809 100644
--- a/docs/py/text_regression.py
+++ b/docs/py/text_regression.py
@@ -1,7 +1,6 @@
 """shell
 pip install autokeras
 """
-
 import os
 
 import numpy as np
@@ -40,10 +39,10 @@ test_data = load_files(
     os.path.join(IMDB_DATADIR, "test"), shuffle=False, categories=classes
 )
 
-x_train = np.array(train_data.data)[:100]
-y_train = np.array(train_data.target)[:100]
-x_test = np.array(test_data.data)[:100]
-y_test = np.array(test_data.target)[:100]
+x_train = np.array(train_data.data)
+y_train = np.array(train_data.target)
+x_test = np.array(test_data.data)
+y_test = np.array(test_data.target)
 
 print(x_train.shape)  # (25000,)
 print(y_train.shape)  # (25000, 1)
@@ -58,10 +57,10 @@ adaptive number of epochs.
 
 # Initialize the text regressor.
 reg = ak.TextRegressor(
-    overwrite=True, max_trials=1  # It tries 10 different models.
+    overwrite=True, max_trials=10  # It tries 10 different models.
 )
 # Feed the text regressor with training data.
-reg.fit(x_train, y_train, epochs=1, batch_size=2)
+reg.fit(x_train, y_train, epochs=2)
 # Predict with the best model.
 predicted_y = reg.predict(x_test)
 # Evaluate the best model with testing data.
@@ -87,7 +86,7 @@ You can also use your own validation set instead of splitting it from the
 training data with `validation_data`.
 """
 
-split = 5
+split = 5000
 x_val = x_train[split:]
 y_val = y_train[split:]
 x_train = x_train[:split]
@@ -95,10 +94,9 @@ y_train = y_train[:split]
 reg.fit(
     x_train,
     y_train,
-    epochs=1,
+    epochs=2,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    batch_size=2,
 )
 
 """
@@ -106,19 +104,48 @@ reg.fit(
 For advanced users, you may customize your search space by using
 [AutoModel](/auto_model/#automodel-class) instead of
 [TextRegressor](/text_regressor). You can configure the
-[TextBlock](/block/#textblock-class) for some high-level configurations. You can
-also do not specify these arguments, which would leave the different choices to
-be tuned automatically.  See the following example for detail.
+[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,
+`vectorizer` for the type of text vectorization method to use.  You can use
+'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to
+convert the words to integers and use [Embedding](/block/#embedding-class) for
+embedding the integer sequences, or you can use 'ngram', which uses
+[TextToNgramVector](/block/#texttongramvector-class) to vectorize the
+sentences.  You can also do not specify these arguments, which would leave the
+different choices to be tuned automatically.  See the following example for
+detail.
+"""
+
+
+input_node = ak.TextInput()
+output_node = ak.TextBlock(block_type="ngram")(input_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+reg.fit(x_train, y_train, epochs=2)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
 """
 
 
 input_node = ak.TextInput()
-output_node = ak.TextBlock()(input_node)
+output_node = ak.TextToIntSequence()(input_node)
+output_node = ak.Embedding()(output_node)
+# Use separable Conv layers in Keras.
+output_node = ak.ConvBlock(separable=True)(output_node)
 output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1, batch_size=2)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -135,23 +162,26 @@ format for the training data.
 
 
 train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(
-    2
+    32
 )
-test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)
+test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)
 
 reg = ak.TextRegressor(overwrite=True, max_trials=2)
 # Feed the tensorflow Dataset to the regressor.
-reg.fit(train_set.take(2), epochs=1)
+reg.fit(train_set, epochs=2)
 # Predict with the best model.
-predicted_y = reg.predict(test_set.take(2))
+predicted_y = reg.predict(test_set)
 # Evaluate the best model with testing data.
-print(reg.evaluate(test_set.take(2)))
+print(reg.evaluate(test_set))
 
 """
 ## Reference
 [TextRegressor](/text_regressor),
 [AutoModel](/auto_model/#automodel-class),
 [TextBlock](/block/#textblock-class),
+[TextToInteSequence](/block/#texttointsequence-class),
+[Embedding](/block/#embedding-class),
+[TextToNgramVector](/block/#texttongramvector-class),
 [ConvBlock](/block/#convblock-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class).
diff --git a/docs/py/timeseries_forecaster.py b/docs/py/timeseries_forecaster.py
new file mode 100644
index 0000000..5eeda09
--- /dev/null
+++ b/docs/py/timeseries_forecaster.py
@@ -0,0 +1,123 @@
+"""shell
+pip install autokeras
+"""
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+To make this tutorial easy to follow, we use the UCI Airquality dataset, and try
+to forecast the AH value at the different timesteps. Some basic preprocessing
+has also been performed on the dataset as it required cleanup.
+
+## A Simple Example
+The first step is to prepare your data. Here we use the [UCI Airquality
+dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality) as an example.
+"""
+
+dataset = tf.keras.utils.get_file(
+    fname="AirQualityUCI.csv",
+    origin="https://archive.ics.uci.edu/ml/machine-learning-databases/00360/"
+    "AirQualityUCI.zip",
+    extract=True,
+)
+
+dataset = pd.read_csv(dataset, sep=";")
+dataset = dataset[dataset.columns[:-2]]
+dataset = dataset.dropna()
+dataset = dataset.replace(",", ".", regex=True)
+
+val_split = int(len(dataset) * 0.7)
+data_train = dataset[:val_split]
+validation_data = dataset[val_split:]
+
+data_x = data_train[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+data_x_val = validation_data[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+# Data with train data and the unseen data from subsequent time steps.
+data_x_test = dataset[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+data_y = data_train["AH"].astype("float64")
+
+data_y_val = validation_data["AH"].astype("float64")
+
+print(data_x.shape)  # (6549, 12)
+print(data_y.shape)  # (6549,)
+
+"""
+The second step is to run the [TimeSeriesForecaster](/time_series_forecaster).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+predict_from = 1
+predict_until = 10
+lookback = 3
+clf = ak.TimeseriesForecaster(
+    lookback=lookback,
+    predict_from=predict_from,
+    predict_until=predict_until,
+    max_trials=1,
+    objective="val_loss",
+)
+# Train the TimeSeriesForecaster with train data
+clf.fit(
+    x=data_x,
+    y=data_y,
+    validation_data=(data_x_val, data_y_val),
+    batch_size=32,
+    epochs=10,
+)
+# Predict with the best model(includes original training data).
+predictions = clf.predict(data_x_test)
+print(predictions.shape)
+# Evaluate the best model with testing data.
+print(clf.evaluate(data_x_val, data_y_val))
diff --git a/docs/templates/benchmarks.md b/docs/templates/benchmarks.md
index d77c382..4f70a6d 100644
--- a/docs/templates/benchmarks.md
+++ b/docs/templates/benchmarks.md
@@ -7,4 +7,6 @@ Tested on a single NVIDIA Tesla V100 GPU.
 | - | - | - | - | - |
 | [MNIST](http://yann.lecun.com/exdb/mnist/)  | ImageClassifier| Accuracy | 99.04% | 0.51 |
 | [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)   | ImageClassifier| Accuracy | 97.10% | 1.8 |
-| [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)  | TextClassifier | Accuracy | 93.93% | 1.2 |
\ No newline at end of file
+| [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)  | TextClassifier | Accuracy | 93.93% | 1.2 |
+| [Titanic](https://www.tensorflow.org/datasets/catalog/titanic)  | StructuredDataClassifier | Accuracy | 82.20% | 0.007 |
+| [California Housing](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset)  | StructuredDataRegression | MSE | 0.23 | 0.06 |
\ No newline at end of file
diff --git a/docs/templates/tutorial/faq.md b/docs/templates/tutorial/faq.md
index 57c1b2a..af4d615 100644
--- a/docs/templates/tutorial/faq.md
+++ b/docs/templates/tutorial/faq.md
@@ -55,7 +55,7 @@ clf = ak.ImageClassifier(
 
 ## How to use multiple GPUs?
 You can use the `distribution_strategy` argument when initializing any model you created with AutoKeras,
-like AutoModel, ImageClassifier and so on. This argument is supported by Keras Tuner.
+like AutoModel, ImageClassifier, StructuredDataRegressor and so on. This argument is supported by Keras Tuner.
 AutoKeras supports the arguments supported by Keras Tuner.
 Please see the discription of the argument [here](https://keras-team.github.io/keras-tuner/documentation/tuners/#tuner-class).
 
diff --git a/docs/templates/tutorial/overview.md b/docs/templates/tutorial/overview.md
index a472b8b..4776554 100644
--- a/docs/templates/tutorial/overview.md
+++ b/docs/templates/tutorial/overview.md
@@ -15,6 +15,12 @@ You can click the links below to see the detailed tutorial for each task.
 
 [Text Regression](/tutorial/text_regression)
 
+[Structured Data Classification](/tutorial/structured_data_classification)
+
+[Structured Data Regression](/tutorial/structured_data_regression)
+
+**Coming Soon**: Time Series Forecasting, Object Detection, Image Segmentation.
+
 
 ## Multi-Task and Multi-Modal Data
 
@@ -36,6 +42,8 @@ The following are the links to the documentation of the predefined input nodes a
 
 [Input](/node/#input-class)
 
+[StructuredDataInput](/node/#structureddatainput-class)
+
 [TextInput](/node/#textinput-class)
 
 **Blocks**:
@@ -44,10 +52,18 @@ The following are the links to the documentation of the predefined input nodes a
 
 [Normalization](/block/#normalization-class)
 
+[TextToIntSequence](/block/#texttointsequence-class)
+
+[TextToNgramVector](/block/#texttongramvector-class)
+
+[CategoricalToNumerical](/block/#categoricaltonumerical-class)
+
 [ConvBlock](/block/#convblock-class)
 
 [DenseBlock](/block/#denseblock-class)
 
+[Embedding](/block/#embedding-class)
+
 [Merge](/block/#merge-class)
 
 [ResNetBlock](/block/#resnetblock-class)
@@ -62,6 +78,8 @@ The following are the links to the documentation of the predefined input nodes a
 
 [ImageBlock](/block/#imageblock-class)
 
+[StructuredDataBlock](/block/#structureddatablock-class)
+
 [TextBlock](/block/#textblock-class)
 
 [ClassificationHead](/block/#classificationhead-class)
diff --git a/docs/tutobooks.py b/docs/tutobooks.py
index 76226a1..502a739 100644
--- a/docs/tutobooks.py
+++ b/docs/tutobooks.py
@@ -60,7 +60,6 @@ you expect. If not, keep editing `your_example.py` until it does.
 
 Finally, submit a PR adding `examples/your_example.py`.
 """
-
 import json
 import os
 import random
diff --git a/examples/automodel_with_cnn.py b/examples/automodel_with_cnn.py
index 9c26056..d7ec20b 100644
--- a/examples/automodel_with_cnn.py
+++ b/examples/automodel_with_cnn.py
@@ -1,6 +1,6 @@
 # Library import
-import keras
 import numpy as np
+import tensorflow as tf
 
 import autokeras as ak
 
@@ -45,6 +45,6 @@ model = auto_model.export_model()
 print(type(model.summary()))
 
 # print model as image
-keras.utils.plot_model(
-    model, show_shapes=True, expand_treeed=True, to_file="name.png"
+tf.keras.utils.plot_model(
+    model, show_shapes=True, expand_nested=True, to_file="name.png"
 )
diff --git a/examples/celeb_age.py b/examples/celeb_age.py
index 5a1bb9b..b4446ba 100644
--- a/examples/celeb_age.py
+++ b/examples/celeb_age.py
@@ -13,7 +13,6 @@ First, prepare your image data in a numpy.ndarray or tensorflow.Dataset format.
 Each image must have the same shape, meaning each has the same width, height,
 and color channels as other images in the set.
 """
-
 from datetime import datetime
 from datetime import timedelta
 
diff --git a/examples/cifar10.py b/examples/cifar10.py
index 4555a0a..75a28df 100644
--- a/examples/cifar10.py
+++ b/examples/cifar10.py
@@ -1,4 +1,4 @@
-from keras.datasets import cifar10
+from tensorflow.keras.datasets import cifar10
 
 import autokeras as ak
 
diff --git a/examples/imdb.py b/examples/imdb.py
index cb024e4..22f48fe 100644
--- a/examples/imdb.py
+++ b/examples/imdb.py
@@ -3,9 +3,8 @@ Search for a good model for the
 [IMDB](
 https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) dataset.
 """
-
-import keras
 import numpy as np
+import tensorflow as tf
 
 import autokeras as ak
 
@@ -14,7 +13,7 @@ def imdb_raw():
     max_features = 20000
     index_offset = 3  # word index offset
 
-    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(
+    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(
         num_words=max_features, index_from=index_offset
     )
     x_train = x_train
@@ -22,7 +21,7 @@ def imdb_raw():
     x_test = x_test
     y_test = y_test.reshape(-1, 1)
 
-    word_to_id = keras.datasets.imdb.get_word_index()
+    word_to_id = tf.keras.datasets.imdb.get_word_index()
     word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}
     word_to_id["<PAD>"] = 0
     word_to_id["<START>"] = 1
diff --git a/examples/iris.py b/examples/iris.py
new file mode 100644
index 0000000..70a1dda
--- /dev/null
+++ b/examples/iris.py
@@ -0,0 +1,71 @@
+"""shell
+pip install -q -U autokeras==1.0.5
+pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
+"""
+
+import os
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+Search for a good model for the
+[iris](https://www.tensorflow.org/datasets/catalog/iris) dataset.
+"""
+
+
+# Prepare the dataset.
+train_dataset_url = "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv"  # noqa: E501
+train_dataset_fp = tf.keras.utils.get_file(
+    fname=os.path.basename(train_dataset_url), origin=train_dataset_url
+)
+
+test_dataset_url = (
+    "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv"
+)
+test_dataset_fp = tf.keras.utils.get_file(
+    fname=os.path.basename(test_dataset_url), origin=test_dataset_url
+)
+
+column_names = [
+    "sepal_length",
+    "sepal_width",
+    "petal_length",
+    "petal_width",
+    "species",
+]
+feature_names = column_names[:-1]
+label_name = column_names[-1]
+class_names = ["Iris setosa", "Iris versicolor", "Iris virginica"]
+
+train = pd.read_csv(train_dataset_fp, names=column_names, header=0)
+
+test = pd.read_csv(test_dataset_fp, names=column_names, header=0)
+
+print(train.shape)  # (120, 5)
+print(test.shape)  # (30, 5)
+
+# Initialize the StructuredDataClassifier.
+clf = ak.StructuredDataClassifier(
+    max_trials=5,
+    overwrite=True,
+)
+# Search for the best model with EarlyStopping.
+cbs = [
+    tf.keras.callbacks.EarlyStopping(patience=3),
+]
+
+clf.fit(
+    x=train[feature_names],
+    y=train[label_name],
+    epochs=200,
+    callbacks=cbs,
+)
+# Evaluate on the testing data.
+print(
+    "Accuracy: {accuracy}".format(
+        accuracy=clf.evaluate(x=test[feature_names], y=test[label_name])
+    )
+)
diff --git a/examples/mnist.py b/examples/mnist.py
index 26c2472..01baaef 100644
--- a/examples/mnist.py
+++ b/examples/mnist.py
@@ -4,7 +4,7 @@ Search for a good model for the
 dataset.
 """
 
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
diff --git a/examples/new_pop.py b/examples/new_pop.py
index b385cfd..7bf9551 100644
--- a/examples/new_pop.py
+++ b/examples/new_pop.py
@@ -1,7 +1,6 @@
 """shell
 pip install autokeras
 """
-
 import pandas as pd
 
 import autokeras as ak
@@ -43,3 +42,28 @@ reg.fit(text_inputs, media_success_outputs)
 
 # Predict with the chosen model:
 predict_y = reg.predict(text_inputs)
+
+"""
+If your text source has a larger vocabulary (number of distinct words), you may
+need to create a custom pipeline in AutoKeras to increase the `max_tokens`
+parameter.
+"""
+
+text_input = (df.Title + " " + df.Headline).to_numpy(dtype="str")
+
+# text input and tokenization
+input_node = ak.TextInput()
+output_node = ak.TextToIntSequence(max_tokens=20000)(input_node)
+
+# regression output
+output_node = ak.RegressionHead()(output_node)
+
+# initialize AutoKeras and find the best model
+reg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=15)
+reg.fit(text_input, media_success_outputs)
+
+"""
+Measure the accuracy of the regressor on an independent test set:
+"""
+
+print(reg.evaluate(text_input, media_success_outputs))
diff --git a/examples/reuters.py b/examples/reuters.py
index 794ba32..27c5abe 100644
--- a/examples/reuters.py
+++ b/examples/reuters.py
@@ -3,10 +3,9 @@
 !pip install -q -U autokeras==1.0.8
 !pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
 """
-
-import keras
 import numpy as np
-from keras.datasets import reuters
+import tensorflow as tf
+from tensorflow.keras.datasets import reuters
 
 import autokeras as ak
 
@@ -60,7 +59,7 @@ clf = ak.TextClassifier(
 
 # Callback to avoid overfitting with the EarlyStopping.
 cbs = [
-    keras.callbacks.EarlyStopping(patience=3),
+    tf.keras.callbacks.EarlyStopping(patience=3),
 ]
 
 # Search for the best model.
diff --git a/examples/titanic.py b/examples/titanic.py
new file mode 100644
index 0000000..979a2b9
--- /dev/null
+++ b/examples/titanic.py
@@ -0,0 +1,41 @@
+"""
+Search for a good model for the [Titanic](https://www.kaggle.com/c/titanic)
+dataset.
+"""
+
+import timeit
+
+import tensorflow as tf
+
+import autokeras as ak
+
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+
+def main():
+    # Initialize the classifier.
+    train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+    test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+    clf = ak.StructuredDataClassifier(
+        max_trials=10, directory="tmp_dir", overwrite=True
+    )
+
+    start_time = timeit.default_timer()
+    # x is the path to the csv file. y is the column name of the column to
+    # predict.
+    clf.fit(train_file_path, "survived")
+    stop_time = timeit.default_timer()
+
+    # Evaluate the accuracy of the found model.
+    accuracy = clf.evaluate(test_file_path, "survived")[1]
+    print("Accuracy: {accuracy}%".format(accuracy=round(accuracy * 100, 2)))
+    print(
+        "Total time: {time} seconds.".format(
+            time=round(stop_time - start_time, 2)
+        )
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/wine.py b/examples/wine.py
new file mode 100644
index 0000000..34f0cee
--- /dev/null
+++ b/examples/wine.py
@@ -0,0 +1,67 @@
+"""
+Run the following commands first
+pip3 install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
+pip3 install autokeras==1.0.5
+
+This Script searches for a model for the wine dataset
+Source and Description of data:
+"""
+import os
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+dataset_url = (
+    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
+)
+
+# save data
+data_file_path = tf.keras.utils.get_file(
+    fname=os.path.basename(dataset_url), origin=dataset_url
+)
+
+column_names = [
+    "Wine",
+    "Alcohol",
+    "Malic.acid",
+    "Ash",
+    "Acl",
+    "Mg",
+    "Phenols",
+    "Flavanoids",
+    "Nonflavanoid.phenols",
+    "Proanth",
+    "Color.int",
+    "Hue",
+    "OD",
+    "Proline",
+]
+
+feature_names = column_names[1:]
+label_name = column_names[0]  # Wine
+
+data = pd.read_csv(data_file_path, header=0, names=column_names)
+# Shuffling
+data = data.sample(frac=1)
+
+split_length = int(data.shape[0] * 0.8)  # 141
+
+# train and test
+train_data = data.iloc[:split_length]
+test_data = data.iloc[split_length:]
+
+
+# Initialize the classifier.
+clf = ak.StructuredDataClassifier(max_trials=5)
+
+# Evaluate
+clf.fit(x=train_data[feature_names], y=train_data[label_name])
+print(
+    "Accuracy: {accuracy}".format(
+        accuracy=clf.evaluate(
+            x=test_data[feature_names], y=test_data[label_name]
+        )
+    )
+)