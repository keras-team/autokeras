+++ b/autokeras/blocks/heads.py
@@ -14,12 +14,12 @@
 
 from typing import Optional
 
-import keras
 import tensorflow as tf
-import tree
-from keras import activations
-from keras import layers
-from keras import losses
+from tensorflow import keras
+from tensorflow import nest
+from tensorflow.keras import activations
+from tensorflow.keras import layers
+from tensorflow.keras import losses
 
 from autokeras import adapters
 from autokeras import analysers
@@ -100,7 +100,7 @@ class ClassificationHead(head_module.Head):
         return config
 
     def build(self, hp, inputs=None):
-        inputs = tree.flatten(inputs)
+        inputs = nest.flatten(inputs)
         utils.validate_num_inputs(inputs, 1)
         input_node = inputs[0]
         output_node = input_node
@@ -231,7 +231,7 @@ class RegressionHead(head_module.Head):
         return config
 
     def build(self, hp, inputs=None):
-        inputs = tree.flatten(inputs)
+        inputs = nest.flatten(inputs)
         utils.validate_num_inputs(inputs, 1)
         input_node = inputs[0]
         output_node = input_node
@@ -262,9 +262,57 @@ class RegressionHead(head_module.Head):
     def get_hyper_preprocessors(self):
         hyper_preprocessors = []
         if self._add_one_dimension:
-            hyper_preprocessors.append(  # pragma: no cover
+            hyper_preprocessors.append(
                 hpps_module.DefaultHyperPreprocessor(
                     preprocessors.AddOneDimension()
                 )
             )
         return hyper_preprocessors
+
+
+class SegmentationHead(ClassificationHead):
+    """Segmentation layers.
+
+    Use sigmoid and binary crossentropy for binary element segmentation.
+    Use softmax and categorical crossentropy for multi-class
+    (more than 2) segmentation. Use Accuracy as metrics by default.
+
+    The targets passing to the head would have to be tf.data.Dataset,
+    np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded
+    if more than two classes, or binary encoded for binary element segmentation.
+
+    The raw labels will be encoded to 0s and 1s if two classes were found, or
+    one-hot encoded if more than two classes were found.
+    One pixel only corresponds to one label.
+
+    # Arguments
+        num_classes: Int. Defaults to None. If None, it will be inferred from
+            the data.
+        loss: A Keras loss function. Defaults to use `binary_crossentropy` or
+            `categorical_crossentropy` based on the number of classes.
+        metrics: A list of Keras metrics. Defaults to use 'accuracy'.
+        dropout: Float. The dropout rate for the layers.
+            If left unspecified, it will be tuned automatically.
+    """
+
+    def __init__(
+        self,
+        num_classes: Optional[int] = None,
+        loss: Optional[types.LossType] = None,
+        metrics: Optional[types.MetricsType] = None,
+        dropout: Optional[float] = None,
+        **kwargs
+    ):
+        super().__init__(
+            loss=loss,
+            metrics=metrics,
+            num_classes=num_classes,
+            dropout=dropout,
+            **kwargs
+        )
+
+    def build(self, hp, inputs):
+        return inputs
+
+    def get_adapter(self):
+        return adapters.SegmentationHeadAdapter(name=self.name)
diff --git a/autokeras/blocks/heads_test.py b/autokeras/blocks/heads_test.py
index 6f2bf42..cad9969 100644
--- a/autokeras/blocks/heads_test.py
+++ b/autokeras/blocks/heads_test.py
@@ -12,12 +12,13 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import keras_tuner
 import numpy as np
 import tensorflow as tf
-import tree
+from tensorflow import keras
+from tensorflow import nest
 
+import autokeras as ak
 from autokeras import hyper_preprocessors
 from autokeras import nodes as input_module
 from autokeras import preprocessors
@@ -102,7 +103,7 @@ def test_clf_head_build_with_zero_dropout_return_tensor():
         keras.Input(shape=(5,), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_clf_head_hpps_with_uint8_contain_cast_to_int32():
@@ -132,4 +133,20 @@ def test_reg_head_build_with_zero_dropout_return_tensor():
         keras.Input(shape=(5,), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_segmentation():
+    dataset = np.array(["a", "a", "c", "b"])
+    head = head_module.SegmentationHead(name="a", shape=(1,))
+    adapter = head.get_adapter()
+    dataset = adapter.adapt(dataset, batch_size=32)
+    analyser = head.get_analyser()
+    for data in dataset:
+        analyser.update(data)
+    analyser.finalize()
+    head.config_from_analyser(analyser)
+    head.build(
+        keras_tuner.HyperParameters(),
+        ak.Input(shape=(32,)).build_node(keras_tuner.HyperParameters()),
+    )
diff --git a/autokeras/blocks/preprocessing.py b/autokeras/blocks/preprocessing.py
index 2e43b4d..5b02e2f 100644
--- a/autokeras/blocks/preprocessing.py
+++ b/autokeras/blocks/preprocessing.py
@@ -16,10 +16,12 @@ from typing import Optional
 from typing import Tuple
 from typing import Union
 
-import tree
-from keras import layers
 from keras_tuner.engine import hyperparameters
+from tensorflow import nest
+from tensorflow.keras import layers
 
+from autokeras import analysers
+from autokeras import keras_layers
 from autokeras.engine import block as block_module
 from autokeras.utils import io_utils
 from autokeras.utils import utils
@@ -43,7 +45,7 @@ class Normalization(block_module.Block):
         self.axis = axis
 
     def build(self, hp, inputs=None):
-        input_node = tree.flatten(inputs)[0]
+        input_node = nest.flatten(inputs)[0]
         return layers.Normalization(axis=self.axis)(input_node)
 
     def get_config(self):
@@ -52,6 +54,91 @@ class Normalization(block_module.Block):
         return config
 
 
+class TextToIntSequence(block_module.Block):
+    """Convert raw texts to sequences of word indices.
+
+    # Arguments
+        output_sequence_length: Int. The maximum length of a sentence. If
+            unspecified, it would be tuned automatically.
+        max_tokens: Int. The maximum size of the vocabulary. Defaults to 20000.
+    """
+
+    def __init__(
+        self,
+        output_sequence_length: Optional[int] = None,
+        max_tokens: int = 20000,
+        **kwargs
+    ):
+        super().__init__(**kwargs)
+        self.output_sequence_length = output_sequence_length
+        self.max_tokens = max_tokens
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "output_sequence_length": self.output_sequence_length,
+                "max_tokens": self.max_tokens,
+            }
+        )
+        return config
+
+    def build(self, hp, inputs=None):
+        input_node = nest.flatten(inputs)[0]
+        if self.output_sequence_length is not None:
+            output_sequence_length = self.output_sequence_length
+        else:
+            output_sequence_length = hp.Choice(
+                "output_sequence_length", [64, 128, 256, 512], default=64
+            )
+        output_node = layers.TextVectorization(
+            max_tokens=self.max_tokens,
+            output_mode="int",
+            output_sequence_length=output_sequence_length,
+        )(input_node)
+        return output_node
+
+
+class TextToNgramVector(block_module.Block):
+    """Convert raw texts to n-gram vectors.
+
+    # Arguments
+        max_tokens: Int. The maximum size of the vocabulary. Defaults to 20000.
+        ngrams: Int or tuple of ints. Passing an integer will create ngrams up
+            to that integer, and passing a tuple of integers will create ngrams
+            for the specified values in the tuple. If left unspecified, it will
+            be tuned automatically.
+    """
+
+    def __init__(
+        self,
+        max_tokens: int = 20000,
+        ngrams: Union[int, Tuple[int], None] = None,
+        **kwargs
+    ):
+        super().__init__(**kwargs)
+        self.max_tokens = max_tokens
+        self.ngrams = ngrams
+
+    def build(self, hp, inputs=None):
+        input_node = nest.flatten(inputs)[0]
+        if self.ngrams is not None:
+            ngrams = self.ngrams
+        else:
+            ngrams = hp.Int("ngrams", min_value=1, max_value=2, default=2)
+        return layers.TextVectorization(
+            max_tokens=self.max_tokens,
+            ngrams=ngrams,
+            output_mode="tf-idf",
+            pad_to_max_tokens=True,
+        )(input_node)
+
+    def get_config(self):
+        config = super().get_config()
+        config.update({"max_tokens": self.max_tokens, "ngrams": self.ngrams})
+        return config
+
+
 class ImageAugmentation(block_module.Block):
     """Collection of various image augmentation methods.
 
@@ -133,7 +220,7 @@ class ImageAugmentation(block_module.Block):
         return value, value
 
     def build(self, hp, inputs=None):
-        input_node = tree.flatten(inputs)[0]
+        input_node = nest.flatten(inputs)[0]
         output_node = input_node
 
         # Translate
@@ -219,3 +306,43 @@ class ImageAugmentation(block_module.Block):
             config["contrast_factor"]
         )
         return cls(**config)
+
+
+class CategoricalToNumerical(block_module.Block):
+    """Encode the categorical features to numerical features."""
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.column_types = None
+        self.column_names = None
+
+    def build(self, hp, inputs=None):
+        input_node = nest.flatten(inputs)[0]
+        encoding = []
+        for column_name in self.column_names:
+            column_type = self.column_types[column_name]
+            if column_type == analysers.CATEGORICAL:
+                # TODO: Search to use one-hot or int.
+                encoding.append(keras_layers.INT)
+            else:
+                encoding.append(keras_layers.NONE)
+        return keras_layers.MultiCategoryEncoding(encoding)(input_node)
+
+    @classmethod
+    def from_config(cls, config):
+        column_types = config.pop("column_types")
+        column_names = config.pop("column_names")
+        instance = cls(**config)
+        instance.column_types = column_types
+        instance.column_names = column_names
+        return instance
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "column_types": self.column_types,
+                "column_names": self.column_names,
+            }
+        )
+        return config
diff --git a/autokeras/blocks/preprocessing_test.py b/autokeras/blocks/preprocessing_test.py
index be4522d..5c7146e 100644
--- a/autokeras/blocks/preprocessing_test.py
+++ b/autokeras/blocks/preprocessing_test.py
@@ -12,11 +12,11 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import keras_tuner
 import tensorflow as tf
-import tree
 from keras_tuner.engine import hyperparameters
+from tensorflow import keras
+from tensorflow import nest
 
 from autokeras import blocks
 from autokeras import test_utils
@@ -30,7 +30,7 @@ def test_augment_build_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_augment_build_with_translation_factor_range_return_tensor():
@@ -41,7 +41,7 @@ def test_augment_build_with_translation_factor_range_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_augment_build_with_no_flip_return_tensor():
@@ -52,7 +52,7 @@ def test_augment_build_with_no_flip_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_augment_build_with_vflip_only_return_tensor():
@@ -63,7 +63,7 @@ def test_augment_build_with_vflip_only_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_augment_build_with_zoom_factor_return_tensor():
@@ -74,7 +74,7 @@ def test_augment_build_with_zoom_factor_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_augment_build_with_contrast_factor_return_tensor():
@@ -85,7 +85,7 @@ def test_augment_build_with_contrast_factor_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_augment_deserialize_to_augment():
@@ -111,3 +111,109 @@ def test_augment_get_config_has_all_attributes():
     assert test_utils.get_func_args(blocks.ImageAugmentation.__init__).issubset(
         config.keys()
     )
+
+
+def test_ngram_build_return_tensor():
+    block = blocks.TextToNgramVector()
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_ngram_build_with_ngrams_return_tensor():
+    block = blocks.TextToNgramVector(ngrams=2)
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_ngram_deserialize_to_ngram():
+    serialized_block = blocks.serialize(blocks.TextToNgramVector())
+
+    block = blocks.deserialize(serialized_block)
+
+    assert isinstance(block, blocks.TextToNgramVector)
+
+
+def test_ngram_get_config_has_all_attributes():
+    block = blocks.TextToNgramVector()
+
+    config = block.get_config()
+
+    assert test_utils.get_func_args(blocks.TextToNgramVector.__init__).issubset(
+        config.keys()
+    )
+
+
+def test_int_seq_build_return_tensor():
+    block = blocks.TextToIntSequence()
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_int_seq_build_with_seq_len_return_tensor():
+    block = blocks.TextToIntSequence(output_sequence_length=50)
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_int_seq_deserialize_to_int_seq():
+    serialized_block = blocks.serialize(blocks.TextToIntSequence())
+
+    block = blocks.deserialize(serialized_block)
+
+    assert isinstance(block, blocks.TextToIntSequence)
+
+
+def test_int_seq_get_config_has_all_attributes():
+    block = blocks.TextToIntSequence()
+
+    config = block.get_config()
+
+    assert test_utils.get_func_args(blocks.TextToIntSequence.__init__).issubset(
+        config.keys()
+    )
+
+
+def test_cat_to_num_build_return_tensor():
+    block = blocks.CategoricalToNumerical()
+    block.column_names = ["a"]
+    block.column_types = {"a": "num"}
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_cat_to_num_deserialize_to_cat_to_num():
+    serialized_block = blocks.serialize(blocks.CategoricalToNumerical())
+
+    block = blocks.deserialize(serialized_block)
+
+    assert isinstance(block, blocks.CategoricalToNumerical)
+
+
+def test_cat_to_num_get_config_has_all_attributes():
+    block = blocks.CategoricalToNumerical()
+
+    config = block.get_config()
+
+    assert test_utils.get_func_args(
+        blocks.CategoricalToNumerical.__init__
+    ).issubset(config.keys())
diff --git a/autokeras/blocks/reduction.py b/autokeras/blocks/reduction.py
index 284451a..d32171d 100644
--- a/autokeras/blocks/reduction.py
+++ b/autokeras/blocks/reduction.py
@@ -14,9 +14,9 @@
 
 from typing import Optional
 
-import tree
-from keras import layers
-from keras import ops
+import tensorflow as tf
+from tensorflow import nest
+from tensorflow.keras import layers
 
 from autokeras.engine import block as block_module
 from autokeras.utils import layer_utils
@@ -54,7 +54,7 @@ class Merge(block_module.Block):
         return config
 
     def build(self, hp, inputs=None):
-        inputs = tree.flatten(inputs)
+        inputs = nest.flatten(inputs)
         if len(inputs) == 1:
             return inputs
 
@@ -79,14 +79,17 @@ class Merge(block_module.Block):
         return layers.Concatenate()(inputs)
 
     def _inputs_same_shape(self, inputs):
-        return all(input_node.shape == inputs[0].shape for input_node in inputs)
+        return all(
+            input_node.shape.as_list() == inputs[0].shape.as_list()
+            for input_node in inputs
+        )
 
 
 class Flatten(block_module.Block):
     """Flatten the input tensor with Keras Flatten layer."""
 
     def build(self, hp, inputs=None):
-        inputs = tree.flatten(inputs)
+        inputs = nest.flatten(inputs)
         utils.validate_num_inputs(inputs, 1)
         input_node = inputs[0]
         if len(input_node.shape) > 2:
@@ -111,7 +114,7 @@ class Reduction(block_module.Block):
         raise NotImplementedError
 
     def build(self, hp, inputs=None):
-        inputs = tree.flatten(inputs)
+        inputs = nest.flatten(inputs)
         utils.validate_num_inputs(inputs, 1)
         input_node = inputs[0]
         output_node = input_node
@@ -173,7 +176,7 @@ class TemporalReduction(Reduction):
         super().__init__(reduction_type, **kwargs)
 
     def global_max(self, input_node):
-        return ops.max(input_node, axis=-2)
+        return tf.math.reduce_max(input_node, axis=-2)
 
     def global_avg(self, input_node):
-        return ops.mean(input_node, axis=-2)
+        return tf.math.reduce_mean(input_node, axis=-2)
diff --git a/autokeras/blocks/reduction_test.py b/autokeras/blocks/reduction_test.py
index 1618595..516880c 100644
--- a/autokeras/blocks/reduction_test.py
+++ b/autokeras/blocks/reduction_test.py
@@ -12,10 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import keras_tuner
 import tensorflow as tf
-import tree
+from tensorflow import keras
+from tensorflow import nest
 
 from autokeras import blocks
 from autokeras import test_utils
@@ -32,7 +32,7 @@ def test_merge_build_return_tensor():
         ],
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_merge_single_input_return_tensor():
@@ -43,7 +43,7 @@ def test_merge_single_input_return_tensor():
         keras.Input(shape=(32,), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_merge_inputs_with_same_shape_return_tensor():
@@ -57,7 +57,7 @@ def test_merge_inputs_with_same_shape_return_tensor():
         ],
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_merge_deserialize_to_merge():
@@ -86,7 +86,7 @@ def test_temporal_build_return_tensor():
         keras.Input(shape=(32, 10), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_temporal_global_max_return_tensor():
@@ -97,7 +97,7 @@ def test_temporal_global_max_return_tensor():
         keras.Input(shape=(32, 10), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_temporal_global_avg_return_tensor():
@@ -108,7 +108,7 @@ def test_temporal_global_avg_return_tensor():
         keras.Input(shape=(32, 10), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_reduction_2d_tensor_return_input_node():
@@ -120,8 +120,8 @@ def test_reduction_2d_tensor_return_input_node():
         input_node,
     )
 
-    assert len(tree.flatten(outputs)) == 1
-    assert tree.flatten(outputs)[0] is input_node
+    assert len(nest.flatten(outputs)) == 1
+    assert nest.flatten(outputs)[0] is input_node
 
 
 def test_temporal_deserialize_to_temporal():
@@ -150,7 +150,7 @@ def test_spatial_build_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_spatial_deserialize_to_spatial():
diff --git a/autokeras/blocks/wrapper.py b/autokeras/blocks/wrapper.py
index e6d1faf..8c7d1de 100644
--- a/autokeras/blocks/wrapper.py
+++ b/autokeras/blocks/wrapper.py
@@ -14,13 +14,12 @@
 
 from typing import Optional
 
-import tree
+from tensorflow import nest
 
 from autokeras.blocks import basic
 from autokeras.blocks import preprocessing
 from autokeras.blocks import reduction
 from autokeras.engine import block as block_module
-from autokeras.utils import utils
 
 BLOCK_TYPE = "block_type"
 RESNET = "resnet"
@@ -29,6 +28,10 @@ VANILLA = "vanilla"
 EFFICIENT = "efficient"
 NORMALIZE = "normalize"
 AUGMENT = "augment"
+TRANSFORMER = "transformer"
+MAX_TOKENS = "max_tokens"
+NGRAM = "ngram"
+BERT = "bert"
 
 
 class ImageBlock(block_module.Block):
@@ -80,7 +83,7 @@ class ImageBlock(block_module.Block):
             return basic.EfficientNetBlock().build(hp, output_node)
 
     def build(self, hp, inputs=None):
-        input_node = tree.flatten(inputs)[0]
+        input_node = nest.flatten(inputs)[0]
         output_node = input_node
 
         if self.normalize is None and hp.Boolean(NORMALIZE):
@@ -114,19 +117,167 @@ class ImageBlock(block_module.Block):
 
 
 class TextBlock(block_module.Block):
-    """Block for text data."""
+    """Block for text data.
 
-    def __init__(self, **kwargs):
+    # Arguments
+        block_type: String. 'vanilla', 'transformer', and 'ngram'. The type of
+            Block to use. 'vanilla' and 'transformer' use a TextToIntSequence
+            vectorizer, whereas 'ngram' uses TextToNgramVector. If unspecified,
+            it will be tuned automatically.
+        max_tokens: Int. The maximum size of the vocabulary.
+            If left unspecified, it will be tuned automatically.
+        pretraining: String. 'random' (use random weights instead any pretrained
+            model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word
+            embedding.  If left unspecified, it will be tuned automatically.
+    """
+
+    def __init__(
+        self,
+        block_type: Optional[str] = None,
+        max_tokens: Optional[int] = None,
+        pretraining: Optional[str] = None,
+        **kwargs
+    ):
         super().__init__(**kwargs)
+        self.block_type = block_type
+        self.max_tokens = max_tokens
+        self.pretraining = pretraining
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                BLOCK_TYPE: self.block_type,
+                MAX_TOKENS: self.max_tokens,
+                "pretraining": self.pretraining,
+            }
+        )
+        return config
 
     def build(self, hp, inputs=None):
-        input_node = tree.flatten(inputs)[0]
+        input_node = nest.flatten(inputs)[0]
         output_node = input_node
-        output_node = self._build_block(hp, output_node)
+        if self.block_type is None:
+            block_type = hp.Choice(
+                BLOCK_TYPE, [VANILLA, TRANSFORMER, NGRAM, BERT]
+            )
+            with hp.conditional_scope(BLOCK_TYPE, [block_type]):
+                output_node = self._build_block(hp, output_node, block_type)
+        else:
+            output_node = self._build_block(hp, output_node, self.block_type)
         return output_node
 
-    def _build_block(self, hp, output_node):
-        output_node = basic.BertBlock().build(hp, output_node)
+    def _build_block(self, hp, output_node, block_type):
+        max_tokens = self.max_tokens or hp.Choice(
+            MAX_TOKENS, [500, 5000, 20000], default=5000
+        )
+        if block_type == NGRAM:
+            output_node = preprocessing.TextToNgramVector(
+                max_tokens=max_tokens
+            ).build(hp, output_node)
+            return basic.DenseBlock().build(hp, output_node)
+        if block_type == BERT:
+            output_node = basic.BertBlock().build(hp, output_node)
+        else:
+            output_node = preprocessing.TextToIntSequence(
+                max_tokens=max_tokens
+            ).build(hp, output_node)
+            if block_type == TRANSFORMER:
+                output_node = basic.Transformer(
+                    max_features=max_tokens + 1,
+                    pretraining=self.pretraining,
+                ).build(hp, output_node)
+            else:
+                output_node = basic.Embedding(
+                    max_features=max_tokens + 1,
+                    pretraining=self.pretraining,
+                ).build(hp, output_node)
+                output_node = basic.ConvBlock().build(hp, output_node)
+            output_node = reduction.SpatialReduction().build(hp, output_node)
+            output_node = basic.DenseBlock().build(hp, output_node)
+        return output_node
+
+
+class StructuredDataBlock(block_module.Block):
+    """Block for structured data.
+
+    # Arguments
+        categorical_encoding: Boolean. Whether to use the CategoricalToNumerical
+            to encode the categorical features to numerical features. Defaults
+            to True.
+        normalize: Boolean. Whether to normalize the features.
+            If unspecified, it will be tuned automatically.
+        seed: Int. Random seed.
+    """
+
+    def __init__(
+        self,
+        categorical_encoding: bool = True,
+        normalize: Optional[bool] = None,
+        seed: Optional[int] = None,
+        **kwargs
+    ):
+        super().__init__(**kwargs)
+        self.categorical_encoding = categorical_encoding
+        self.normalize = normalize
+        self.seed = seed
+        self.column_types = None
+        self.column_names = None
+
+    @classmethod
+    def from_config(cls, config):
+        column_types = config.pop("column_types")
+        column_names = config.pop("column_names")
+        instance = cls(**config)
+        instance.column_types = column_types
+        instance.column_names = column_names
+        return instance
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "categorical_encoding": self.categorical_encoding,
+                "normalize": self.normalize,
+                "seed": self.seed,
+                "column_types": self.column_types,
+                "column_names": self.column_names,
+            }
+        )
+        return config
+
+    def build(self, hp, inputs=None):
+        input_node = nest.flatten(inputs)[0]
+        output_node = input_node
+        if self.categorical_encoding:
+            block = preprocessing.CategoricalToNumerical()
+            block.column_types = self.column_types
+            block.column_names = self.column_names
+            output_node = block.build(hp, output_node)
+
+        if self.normalize is None and hp.Boolean(NORMALIZE):
+            with hp.conditional_scope(NORMALIZE, [True]):
+                output_node = preprocessing.Normalization().build(
+                    hp, output_node
+                )
+        elif self.normalize:
+            output_node = preprocessing.Normalization().build(hp, output_node)
+
+        output_node = basic.DenseBlock().build(hp, output_node)
+        return output_node
+
+
+class TimeseriesBlock(block_module.Block):
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    def get_config(self):
+        return super().get_config()
+
+    def build(self, hp, inputs=None):
+        input_node = nest.flatten(inputs)[0]
+        output_node = input_node
+        output_node = basic.RNNBlock().build(hp, output_node)
         return output_node
 
 
@@ -141,11 +292,4 @@ class GeneralBlock(block_module.Block):
     """
 
     def build(self, hp, inputs=None):
-        inputs = tree.flatten(inputs)
-        utils.validate_num_inputs(inputs, 1)
-        input_node = inputs[0]
-        output_node = input_node
-
-        output_node = reduction.Flatten().build(hp, output_node)
-        output_node = basic.DenseBlock().build(hp, output_node)
-        return output_node
+        raise NotImplementedError
diff --git a/autokeras/blocks/wrapper_test.py b/autokeras/blocks/wrapper_test.py
index 24119e6..a292154 100644
--- a/autokeras/blocks/wrapper_test.py
+++ b/autokeras/blocks/wrapper_test.py
@@ -12,11 +12,12 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import keras_tuner
 import tensorflow as tf
-import tree
+from tensorflow import keras
+from tensorflow import nest
 
+from autokeras import analysers
 from autokeras import blocks
 from autokeras import test_utils
 
@@ -29,18 +30,7 @@ def test_image_build_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
-
-
-def test_general_build_return_tensor():
-    block = blocks.GeneralBlock()
-
-    outputs = block.build(
-        keras_tuner.HyperParameters(),
-        keras.Input(shape=(32, 32, 3), dtype=tf.float32),
-    )
-
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_image_block_xception_return_tensor():
@@ -51,7 +41,7 @@ def test_image_block_xception_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_image_block_normalize_return_tensor():
@@ -62,7 +52,7 @@ def test_image_block_normalize_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_image_block_augment_return_tensor():
@@ -73,7 +63,7 @@ def test_image_block_augment_return_tensor():
         keras.Input(shape=(32, 32, 3), dtype=tf.float32),
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_image_deserialize_to_image():
@@ -101,7 +91,27 @@ def test_text_build_return_tensor():
         keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
     )
 
-    assert len(tree.flatten(outputs)) == 1
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_text_block_ngram_return_tensor():
+    block = blocks.TextBlock(block_type="ngram")
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_text_block_transformer_return_tensor():
+    block = blocks.TextBlock(block_type="transformer")
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(1,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
 
 
 def test_text_deserialize_to_text():
@@ -120,3 +130,88 @@ def test_text_get_config_has_all_attributes():
     assert test_utils.get_func_args(blocks.TextBlock.__init__).issubset(
         config.keys()
     )
+
+
+def test_structured_build_return_tensor():
+    block = blocks.StructuredDataBlock()
+    block.column_names = ["0", "1"]
+    block.column_types = {"0": analysers.NUMERICAL, "1": analysers.NUMERICAL}
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(2,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_structured_block_normalize_return_tensor():
+    block = blocks.StructuredDataBlock(normalize=True)
+    block.column_names = ["0", "1"]
+    block.column_types = {"0": analysers.NUMERICAL, "1": analysers.NUMERICAL}
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(), keras.Input(shape=(2,), dtype=tf.string)
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_structured_block_search_normalize_return_tensor():
+    block = blocks.StructuredDataBlock(name="a")
+    block.column_names = ["0", "1"]
+    block.column_types = {"0": analysers.NUMERICAL, "1": analysers.NUMERICAL}
+    hp = keras_tuner.HyperParameters()
+    hp.values["a/" + blocks.wrapper.NORMALIZE] = True
+
+    outputs = block.build(hp, keras.Input(shape=(2,), dtype=tf.string))
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_structured_deserialize_to_structured():
+    serialized_block = blocks.serialize(blocks.StructuredDataBlock())
+
+    block = blocks.deserialize(serialized_block)
+
+    assert isinstance(block, blocks.StructuredDataBlock)
+
+
+def test_structured_get_config_has_all_attributes():
+    block = blocks.StructuredDataBlock()
+
+    config = block.get_config()
+
+    assert test_utils.get_func_args(
+        blocks.StructuredDataBlock.__init__
+    ).issubset(config.keys())
+
+
+def test_timeseries_build_return_tensor():
+    block = blocks.TimeseriesBlock()
+    block.column_names = ["0", "1"]
+    block.column_types = {"0": analysers.NUMERICAL, "1": analysers.NUMERICAL}
+
+    outputs = block.build(
+        keras_tuner.HyperParameters(),
+        keras.Input(shape=(32, 2), dtype=tf.float32),
+    )
+
+    assert len(nest.flatten(outputs)) == 1
+
+
+def test_timeseries_deserialize_to_timeseries():
+    serialized_block = blocks.serialize(blocks.TimeseriesBlock())
+
+    block = blocks.deserialize(serialized_block)
+
+    assert isinstance(block, blocks.TimeseriesBlock)
+
+
+def test_timeseries_get_config_has_all_attributes():
+    block = blocks.TimeseriesBlock()
+
+    config = block.get_config()
+
+    assert test_utils.get_func_args(blocks.TimeseriesBlock.__init__).issubset(
+        config.keys()
+    )
diff --git a/autokeras/conftest.py b/autokeras/conftest.py
index f9179fa..425f65a 100644
--- a/autokeras/conftest.py
+++ b/autokeras/conftest.py
@@ -14,8 +14,8 @@
 
 import shutil
 
-import keras
 import pytest
+from tensorflow import keras
 
 
 @pytest.fixture(autouse=True)
@@ -29,9 +29,3 @@ def clear_session():
 def remove_tmp_path(tmp_path):
     yield
     shutil.rmtree(tmp_path)
-
-
-@pytest.fixture(autouse=True)
-def disable_traceback_filtering():
-    keras.config.disable_traceback_filtering()
-    yield
diff --git a/autokeras/engine/block.py b/autokeras/engine/block.py
index 7cc762b..f8d6605 100644
--- a/autokeras/engine/block.py
+++ b/autokeras/engine/block.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import tree
+from tensorflow import nest
 
 from autokeras.engine import named_hypermodel
 from autokeras.engine import node as node_module
@@ -47,7 +47,7 @@ class Block(named_hypermodel.NamedHyperModel):
         # Returns
             list: A list of output node(s) of the Block.
         """
-        self.inputs = tree.flatten(inputs)
+        self.inputs = nest.flatten(inputs)
         for input_node in self.inputs:
             if not isinstance(input_node, node_module.Node):
                 raise TypeError(
diff --git a/autokeras/engine/head.py b/autokeras/engine/head.py
index 01cefd1..b0168cf 100644
--- a/autokeras/engine/head.py
+++ b/autokeras/engine/head.py
@@ -13,7 +13,7 @@
 # limitations under the License.
 from typing import Optional
 
-import keras
+from tensorflow import keras
 
 from autokeras.engine import io_hypermodel
 from autokeras.utils import types
diff --git a/autokeras/engine/named_hypermodel.py b/autokeras/engine/named_hypermodel.py
index 716d0bb..6d85e3e 100644
--- a/autokeras/engine/named_hypermodel.py
+++ b/autokeras/engine/named_hypermodel.py
@@ -12,8 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import keras_tuner
+from tensorflow import keras
 
 from autokeras.engine import serializable
 from autokeras.utils import utils
diff --git a/autokeras/engine/tuner.py b/autokeras/engine/tuner.py
index 37bbb6e..916a5ee 100644
--- a/autokeras/engine/tuner.py
+++ b/autokeras/engine/tuner.py
@@ -16,12 +16,12 @@ import collections
 import copy
 import os
 
-import keras
 import keras_tuner
-import tree
-from keras import callbacks as callbacks_module
+from tensorflow import keras
+from tensorflow import nest
+from tensorflow.keras import callbacks as tf_callbacks
+from tensorflow.keras.layers.experimental import preprocessing
 
-from autokeras import keras_layers
 from autokeras import pipeline as pipeline_module
 from autokeras.utils import data_utils
 from autokeras.utils import utils
@@ -62,7 +62,11 @@ class AutoTuner(keras_tuner.engine.tuner.Tuner):
         return
 
     def get_best_model(self):
-        return self.get_best_models()[0]
+        with keras_tuner.engine.tuner.maybe_distribute(
+            self.distribution_strategy
+        ):
+            model = keras.models.load_model(self.best_model_path)
+        return model
 
     def get_best_pipeline(self):
         return pipeline_module.load_pipeline(self.best_pipeline_path)
@@ -115,23 +119,20 @@ class AutoTuner(keras_tuner.engine.tuner.Tuner):
 
         def get_output_layers(tensor):
             output_layers = []
-            tensor = tree.flatten(tensor)[0]
+            tensor = nest.flatten(tensor)[0]
             for layer in model.layers:
                 if isinstance(layer, keras.layers.InputLayer):
                     continue
-                input_node = tree.flatten(layer.input)[0]
+                input_node = nest.flatten(layer.input)[0]
                 if input_node is tensor:
-                    if isinstance(
-                        layer,
-                        keras_layers.PreprocessingLayer,
-                    ) or hasattr(layer, "adapt"):
+                    if isinstance(layer, preprocessing.PreprocessingLayer):
                         output_layers.append(layer)
             return output_layers
 
         dq = collections.deque()
 
-        for index, input_node in enumerate(tree.flatten(model.input)):
-            in_x = x.map(lambda *args: tree.flatten(args)[index])
+        for index, input_node in enumerate(nest.flatten(model.input)):
+            in_x = x.map(lambda *args: nest.flatten(args)[index])
             for layer in get_output_layers(input_node):
                 dq.append((layer, in_x))
 
@@ -177,21 +178,19 @@ class AutoTuner(keras_tuner.engine.tuner.Tuner):
             epochs_provided = False
             epochs = 1000
             if not utils.contain_instance(
-                callbacks, callbacks_module.EarlyStopping
+                callbacks, tf_callbacks.EarlyStopping
             ):
                 callbacks.append(
-                    callbacks_module.EarlyStopping(patience=10, min_delta=1e-4)
+                    tf_callbacks.EarlyStopping(patience=10, min_delta=1e-4)
                 )
 
         # Insert early-stopping for acceleration.
         early_stopping_inserted = False
         new_callbacks = self._deepcopy_callbacks(callbacks)
-        if not utils.contain_instance(
-            callbacks, callbacks_module.EarlyStopping
-        ):
+        if not utils.contain_instance(callbacks, tf_callbacks.EarlyStopping):
             early_stopping_inserted = True
             new_callbacks.append(
-                callbacks_module.EarlyStopping(patience=10, min_delta=1e-4)
+                tf_callbacks.EarlyStopping(patience=10, min_delta=1e-4)
             )
 
         # Populate initial search space.
@@ -234,7 +233,7 @@ class AutoTuner(keras_tuner.engine.tuner.Tuner):
             pipeline, model, history = self.final_fit(**copied_fit_kwargs)
         else:
             # TODO: Add return history functionality in Keras Tuner
-            model = self.get_best_model()
+            model = self.get_best_models()[0]
             history = None
             pipeline = pipeline_module.load_pipeline(
                 self._pipeline_path(self.oracle.get_best_trials(1)[0].trial_id)
@@ -259,7 +258,7 @@ class AutoTuner(keras_tuner.engine.tuner.Tuner):
         return [
             copy.deepcopy(callbacks)
             for callback in callbacks
-            if not isinstance(callback, callbacks_module.EarlyStopping)
+            if not isinstance(callback, tf_callbacks.EarlyStopping)
         ]
 
     def _get_best_trial_epochs(self):
@@ -290,7 +289,7 @@ class AutoTuner(keras_tuner.engine.tuner.Tuner):
 
     @property
     def best_model_path(self):
-        return os.path.join(self.project_dir, "best_model.keras")
+        return os.path.join(self.project_dir, "best_model")
 
     @property
     def best_pipeline_path(self):
diff --git a/autokeras/engine/tuner_test.py b/autokeras/engine/tuner_test.py
index 22eff9d..83bc2df 100644
--- a/autokeras/engine/tuner_test.py
+++ b/autokeras/engine/tuner_test.py
@@ -14,13 +14,13 @@
 
 from unittest import mock
 
-import keras
 import numpy as np
 import tensorflow as tf
-import tree
-from keras import layers
+from tensorflow import keras
+from tensorflow.keras.layers.experimental import preprocessing
 
 import autokeras as ak
+from autokeras import keras_layers
 from autokeras import test_utils
 from autokeras.engine import tuner as tuner_module
 from autokeras.tuners import greedy
@@ -168,8 +168,20 @@ def test_tuner_does_not_crash_with_distribution_strategy(tmp_path):
     tuner.hypermodel.build(tuner.oracle.hyperparameters)
 
 
+def test_preprocessing_adapt_with_cat_to_int_and_norm():
+    x = np.array([["a", 5], ["b", 6]]).astype(str)
+    y = np.array([[1, 2], [3, 4]]).astype(str)
+    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(32)
+    model = keras.models.Sequential()
+    model.add(keras.Input(shape=(2,), dtype=tf.string))
+    model.add(keras_layers.MultiCategoryEncoding(["int", "none"]))
+    model.add(preprocessing.Normalization(axis=-1))
+
+    tuner_module.AutoTuner.adapt(model, dataset)
+
+
 def test_preprocessing_adapt_with_text_vec():
-    class MockLayer(layers.TextVectorization):
+    class MockLayer(preprocessing.TextVectorization):
         def adapt(self, *args, **kwargs):
             super().adapt(*args, **kwargs)
             self.is_called = True
@@ -177,15 +189,14 @@ def test_preprocessing_adapt_with_text_vec():
     x_train = test_utils.generate_text_data()
     y_train = np.random.randint(0, 2, (100,))
     dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
-
-    inputs = keras.Input(shape=(1,), dtype=tf.string)
     layer1 = MockLayer(
         max_tokens=5000, output_mode="int", output_sequence_length=40
     )
-    outputs = layer1(inputs)
-    outputs = keras.layers.Embedding(50001, 10)(outputs)
-    outputs = keras.layers.Dense(1)(outputs)
-    model = keras.models.Model(inputs, outputs)
+    model = keras.models.Sequential()
+    model.add(keras.Input(shape=(1,), dtype=tf.string))
+    model.add(layer1)
+    model.add(keras.layers.Embedding(50001, 10))
+    model.add(keras.layers.Dense(1))
 
     tuner_module.AutoTuner.adapt(model, dataset)
 
@@ -194,7 +205,9 @@ def test_preprocessing_adapt_with_text_vec():
 
 def test_adapt_with_model_with_preprocessing_layer_only():
     input_node = keras.Input(shape=(10,))
-    output_node = keras.layers.Normalization()(input_node)
+    output_node = keras.layers.experimental.preprocessing.Normalization()(
+        input_node
+    )
     model = keras.Model(input_node, output_node)
     greedy.Greedy.adapt(
         model,
@@ -208,7 +221,7 @@ def test_build_block_in_blocks_with_same_name(tmp_path):
     class Block1(ak.Block):
         def build(self, hp, inputs):
             hp.Boolean("a")
-            return keras.layers.Dense(3)(tree.flatten(inputs)[0])
+            return keras.layers.Dense(3)(tf.nest.flatten(inputs)[0])
 
     class Block2(ak.Block):
         def build(self, hp, inputs):
diff --git a/autokeras/graph.py b/autokeras/graph.py
index 5876d9a..59b1f6f 100644
--- a/autokeras/graph.py
+++ b/autokeras/graph.py
@@ -12,17 +12,38 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import keras_tuner
-import tree
+from tensorflow import keras
+from tensorflow import nest
 
 from autokeras import blocks as blocks_module
+from autokeras import keras_layers
 from autokeras import nodes as nodes_module
 from autokeras.engine import head as head_module
 from autokeras.engine import serializable
 from autokeras.utils import io_utils
 
 
+def feature_encoding_input(block):
+    """Fetch the column_types and column_names.
+
+    The values are fetched for FeatureEncoding from StructuredDataInput.
+    """
+    if not isinstance(block.inputs[0], nodes_module.StructuredDataInput):
+        raise TypeError(
+            "CategoricalToNumerical can only be used with StructuredDataInput."
+        )
+    block.column_types = block.inputs[0].column_types
+    block.column_names = block.inputs[0].column_names
+
+
+# Compile the graph.
+COMPILE_FUNCTIONS = {
+    blocks_module.StructuredDataBlock: [feature_encoding_input],
+    blocks_module.CategoricalToNumerical: [feature_encoding_input],
+}
+
+
 def load_graph(filepath, custom_objects=None):
     if custom_objects is None:
         custom_objects = {}
@@ -40,8 +61,8 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
 
     def __init__(self, inputs=None, outputs=None, **kwargs):
         super().__init__(**kwargs)
-        self.inputs = tree.flatten(inputs)
-        self.outputs = tree.flatten(outputs)
+        self.inputs = nest.flatten(inputs)
+        self.outputs = nest.flatten(outputs)
         self._node_to_id = {}
         self._nodes = []
         self.blocks = []
@@ -53,6 +74,12 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
         self.epochs = None
         self.num_samples = None
 
+    def compile(self):
+        """Share the information between blocks."""
+        for block in self.blocks:
+            for func in COMPILE_FUNCTIONS.get(block.__class__, []):
+                func(block)
+
     def _build_network(self):
         self._node_to_id = {}
 
@@ -202,7 +229,7 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
                 nodes[node_id]
                 for node_id in config["block_inputs"][str(block_id)]
             ]
-            output_nodes = tree.flatten(block(input_nodes))
+            output_nodes = nest.flatten(block(input_nodes))
             for output_node, node_id in zip(
                 output_nodes, config["block_outputs"][str(block_id)]
             ):
@@ -213,6 +240,7 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
 
     def build(self, hp):
         """Build the HyperModel into a Keras Model."""
+        self.compile()
         keras_nodes = {}
         keras_input_nodes = []
         for node in self.inputs:
@@ -227,7 +255,7 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
                 for input_node in block.inputs
             ]
             outputs = block.build(hp, inputs=temp_inputs)
-            outputs = tree.flatten(outputs)
+            outputs = nest.flatten(outputs)
             for output_node, real_output_node in zip(block.outputs, outputs):
                 keras_nodes[self._node_to_id[output_node]] = real_output_node
         model = keras.Model(
@@ -276,14 +304,23 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
         elif optimizer_name == "adam_weight_decay":
             steps_per_epoch = int(self.num_samples / self.batch_size)
             num_train_steps = steps_per_epoch * self.epochs
+            warmup_steps = int(
+                self.epochs * self.num_samples * 0.1 / self.batch_size
+            )
 
             lr_schedule = keras.optimizers.schedules.PolynomialDecay(
                 initial_learning_rate=learning_rate,
                 decay_steps=num_train_steps,
                 end_learning_rate=0.0,
             )
-
-            optimizer = keras.optimizers.AdamW(
+            if warmup_steps:
+                lr_schedule = keras_layers.WarmUp(
+                    initial_learning_rate=learning_rate,
+                    decay_schedule_fn=lr_schedule,
+                    warmup_steps=warmup_steps,
+                )
+
+            optimizer = keras.optimizers.experimental.AdamW(
                 learning_rate=lr_schedule,
                 weight_decay=0.01,
                 beta_1=0.9,
@@ -303,9 +340,9 @@ class Graph(keras_tuner.HyperModel, serializable.Serializable):
         io_utils.save_json(filepath, self.get_config())
 
     def set_io_shapes(self, shapes):
-        for node, shape in zip(self.inputs, tree.flatten(shapes[0])):
+        for node, shape in zip(self.inputs, nest.flatten(shapes[0])):
             node.shape = tuple(shape[1:])
-        for node, shape in zip(self.outputs, tree.flatten(shapes[1])):
+        for node, shape in zip(self.outputs, nest.flatten(shapes[1])):
             node.in_blocks[0].shape = tuple(shape[1:])
 
     def set_fit_args(self, validation_split, epochs=None):
diff --git a/autokeras/graph_test.py b/autokeras/graph_test.py
index 6ea5ba3..430e844 100644
--- a/autokeras/graph_test.py
+++ b/autokeras/graph_test.py
@@ -161,6 +161,16 @@ def test_save_custom_metrics_loss(tmp_path):
     assert new_graph.blocks[0].loss(3, 2) == 1
 
 
+def test_cat_to_num_with_img_input_error():
+    input_node = ak.ImageInput()
+    output_node = ak.CategoricalToNumerical()(input_node)
+
+    with pytest.raises(TypeError) as info:
+        graph_module.Graph(input_node, outputs=output_node).compile()
+
+    assert "CategoricalToNumerical can only be used" in str(info.value)
+
+
 def test_graph_can_init_with_one_missing_output():
     input_node = ak.ImageInput()
     output_node = ak.ConvBlock()(input_node)
diff --git a/autokeras/hyper_preprocessors.py b/autokeras/hyper_preprocessors.py
index 51f5396..a3edecf 100644
--- a/autokeras/hyper_preprocessors.py
+++ b/autokeras/hyper_preprocessors.py
@@ -26,6 +26,7 @@ def deserialize(config, custom_objects=None):
         config,
         module_objects=globals(),
         custom_objects=custom_objects,
+        printable_module_name="preprocessors",
     )
 
 
diff --git a/autokeras/hyper_preprocessors_test.py b/autokeras/hyper_preprocessors_test.py
index 37ee066..dc12dc1 100644
--- a/autokeras/hyper_preprocessors_test.py
+++ b/autokeras/hyper_preprocessors_test.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
+import tensorflow as tf
 
 from autokeras import hyper_preprocessors
 from autokeras import preprocessors
@@ -28,3 +30,40 @@ def test_serialize_and_deserialize_default_hpps():
     assert isinstance(
         hyper_preprocessor.preprocessor, preprocessors.AddOneDimension
     )
+
+
+def test_serialize_and_deserialize_default_hpps_categorical():
+    x_train = np.array([["a", "ab", 2.1], ["b", "bc", 1.0], ["a", "bc", "nan"]])
+    preprocessor = preprocessors.CategoricalToNumericalPreprocessor(
+        column_names=["column_a", "column_b", "column_c"],
+        column_types={
+            "column_a": "categorical",
+            "column_b": "categorical",
+            "column_c": "numerical",
+        },
+    )
+
+    hyper_preprocessor = hyper_preprocessors.DefaultHyperPreprocessor(
+        preprocessor
+    )
+    dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(32)
+    hyper_preprocessor.preprocessor.fit(
+        tf.data.Dataset.from_tensor_slices(x_train).batch(32)
+    )
+    hyper_preprocessor = hyper_preprocessors.deserialize(
+        hyper_preprocessors.serialize(hyper_preprocessor)
+    )
+    assert isinstance(
+        hyper_preprocessor.preprocessor,
+        preprocessors.CategoricalToNumericalPreprocessor,
+    )
+
+    results = hyper_preprocessor.preprocessor.transform(dataset)
+
+    for result in results:
+        assert result[0][0] == result[2][0]
+        assert result[0][0] != result[1][0]
+        assert result[0][1] != result[1][1]
+        assert result[0][1] != result[2][1]
+        assert result[2][2] == 0
+        assert result.dtype == tf.float32
diff --git a/autokeras/integration_tests/functional_api_test.py b/autokeras/integration_tests/functional_api_test.py
index b968319..3a4e892 100644
--- a/autokeras/integration_tests/functional_api_test.py
+++ b/autokeras/integration_tests/functional_api_test.py
@@ -13,16 +13,19 @@
 # limitations under the License.
 
 import numpy as np
+import pandas as pd
 
 import autokeras as ak
 from autokeras import test_utils
 
 
-def test_text_data(tmp_path):
+def test_text_and_structured_data(tmp_path):
     # Prepare the data.
-    num_instances = 3
+    num_instances = 80
     x_text = test_utils.generate_text_data(num_instances)
+    x_structured_data = pd.read_csv(test_utils.TRAIN_CSV_PATH)
 
+    x_structured_data = x_structured_data[:num_instances]
     y_classification = test_utils.generate_one_hot_labels(
         num_instances=num_instances, num_classes=3
     )
@@ -31,12 +34,25 @@ def test_text_data(tmp_path):
     )
 
     # Build model and train.
+    structured_data_input = ak.StructuredDataInput()
+    structured_data_output = ak.CategoricalToNumerical()(structured_data_input)
+    structured_data_output = ak.DenseBlock()(structured_data_output)
+
     text_input = ak.TextInput()
-    outputs = ak.BertBlock()(text_input)
-    regression_outputs = ak.RegressionHead()(outputs)
-    classification_outputs = ak.ClassificationHead()(outputs)
+    outputs1 = ak.TextToIntSequence()(text_input)
+    outputs1 = ak.Embedding()(outputs1)
+    outputs1 = ak.ConvBlock(separable=True)(outputs1)
+    outputs1 = ak.SpatialReduction()(outputs1)
+    outputs2 = ak.TextToNgramVector()(text_input)
+    outputs2 = ak.DenseBlock()(outputs2)
+    text_output = ak.Merge()((outputs1, outputs2))
+
+    merged_outputs = ak.Merge()((structured_data_output, text_output))
+
+    regression_outputs = ak.RegressionHead()(merged_outputs)
+    classification_outputs = ak.ClassificationHead()(merged_outputs)
     automodel = ak.AutoModel(
-        inputs=text_input,
+        inputs=[text_input, structured_data_input],
         directory=tmp_path,
         outputs=[regression_outputs, classification_outputs],
         max_trials=2,
@@ -45,16 +61,15 @@ def test_text_data(tmp_path):
     )
 
     automodel.fit(
-        x_text,
+        (x_text, x_structured_data),
         (y_regression, y_classification),
         validation_split=0.2,
         epochs=1,
-        batch_size=2,
     )
 
 
 def test_image_blocks(tmp_path):
-    num_instances = 3
+    num_instances = 10
     x_train = test_utils.generate_data(
         num_instances=num_instances, shape=(28, 28)
     )
@@ -77,9 +92,5 @@ def test_image_blocks(tmp_path):
     )
 
     automodel.fit(
-        x_train,
-        y_train,
-        validation_data=(x_train, y_train),
-        epochs=1,
-        batch_size=2,
+        x_train, y_train, validation_data=(x_train, y_train), epochs=1
     )
diff --git a/autokeras/integration_tests/io_api_test.py b/autokeras/integration_tests/io_api_test.py
index b31708a..0baf399 100644
--- a/autokeras/integration_tests/io_api_test.py
+++ b/autokeras/integration_tests/io_api_test.py
@@ -12,18 +12,25 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pandas as pd
+
 import autokeras as ak
 from autokeras import test_utils
 
 
 def test_io_api(tmp_path):
-    num_instances = 3
+    num_instances = 20
     image_x = test_utils.generate_data(
         num_instances=num_instances, shape=(28, 28)
     )
     text_x = test_utils.generate_text_data(num_instances=num_instances)
 
     image_x = image_x[:num_instances]
+    structured_data_x = (
+        pd.read_csv(test_utils.TRAIN_CSV_PATH)
+        .to_numpy()
+        .astype(str)[:num_instances]
+    )
     classification_y = test_utils.generate_one_hot_labels(
         num_instances=num_instances, num_classes=3
     )
@@ -33,7 +40,7 @@ def test_io_api(tmp_path):
 
     # Build model and train.
     automodel = ak.AutoModel(
-        inputs=[ak.ImageInput(), ak.TextInput()],
+        inputs=[ak.ImageInput(), ak.TextInput(), ak.StructuredDataInput()],
         outputs=[
             ak.RegressionHead(metrics=["mae"]),
             ak.ClassificationHead(
@@ -46,10 +53,10 @@ def test_io_api(tmp_path):
         seed=test_utils.SEED,
     )
     automodel.fit(
-        [image_x, text_x],
+        [image_x, text_x, structured_data_x],
         [regression_y, classification_y],
         epochs=1,
         validation_split=0.2,
-        batch_size=2,
+        batch_size=4,
     )
-    automodel.predict([image_x, text_x])
+    automodel.predict([image_x, text_x, structured_data_x])
diff --git a/autokeras/integration_tests/task_api_test.py b/autokeras/integration_tests/task_api_test.py
index 53325f8..23f1113 100644
--- a/autokeras/integration_tests/task_api_test.py
+++ b/autokeras/integration_tests/task_api_test.py
@@ -12,9 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import numpy as np
+import pandas as pd
 import tensorflow as tf
+from tensorflow import keras
 
 import autokeras as ak
 from autokeras import test_utils
@@ -75,7 +76,7 @@ def test_text_classifier(tmp_path):
     clf.fit(
         train_x,
         train_y,
-        epochs=1,
+        epochs=2,
         validation_data=(test_x, test_y),
         batch_size=BATCH_SIZE,
     )
@@ -99,6 +100,77 @@ def test_text_regressor(tmp_path):
         validation_data=(test_x, test_y),
         batch_size=BATCH_SIZE,
     )
-    clf.predict(test_x)
     clf.export_model()
     assert clf.predict(test_x).shape == (len(test_x), 1)
+
+
+def test_structured_data_regressor(tmp_path):
+    num_data = NUM_INSTANCES * 2
+    num_train = NUM_INSTANCES
+    data = (
+        pd.read_csv(test_utils.TRAIN_CSV_PATH).to_numpy().astype(str)[:num_data]
+    )
+    x_train, x_test = data[:num_train], data[num_train:]
+    y = test_utils.generate_data(num_instances=num_data, shape=tuple())
+    y_train, y_test = y[:num_train], y[num_train:]
+    clf = ak.StructuredDataRegressor(
+        directory=tmp_path, max_trials=2, seed=test_utils.SEED
+    )
+    clf.fit(
+        x_train,
+        y_train,
+        epochs=11,
+        validation_data=(x_train, y_train),
+        batch_size=BATCH_SIZE,
+    )
+    clf.export_model()
+    assert clf.predict(x_test).shape == (len(y_test), 1)
+
+
+def test_structured_data_classifier(tmp_path):
+    num_data = NUM_INSTANCES * 2
+    num_train = NUM_INSTANCES
+    data = (
+        pd.read_csv(test_utils.TRAIN_CSV_PATH).to_numpy().astype(str)[:num_data]
+    )
+    x_train, x_test = data[:num_train], data[num_train:]
+    y = test_utils.generate_one_hot_labels(
+        num_instances=num_data, num_classes=3
+    )
+    y_train, y_test = y[:num_train], y[num_train:]
+    clf = ak.StructuredDataClassifier(
+        directory=tmp_path, max_trials=1, seed=test_utils.SEED
+    )
+    clf.fit(
+        x_train,
+        y_train,
+        epochs=2,
+        validation_data=(x_train, y_train),
+        batch_size=BATCH_SIZE,
+    )
+    clf.export_model()
+    assert clf.predict(x_test).shape == (len(y_test), 3)
+
+
+def test_timeseries_forecaster(tmp_path):
+    lookback = 2
+    predict_from = 1
+    predict_until = 10
+    train_x = test_utils.generate_data_with_categorical(num_instances=100)
+    train_y = test_utils.generate_data(num_instances=80, shape=(1,))
+    clf = ak.TimeseriesForecaster(
+        lookback=lookback,
+        directory=tmp_path,
+        predict_from=predict_from,
+        predict_until=predict_until,
+        max_trials=2,
+        seed=test_utils.SEED,
+    )
+    clf.fit(train_x, train_y, epochs=1, validation_data=(train_x, train_y))
+    keras_model = clf.export_model()
+    clf.evaluate(train_x, train_y)
+    assert clf.predict(train_x).shape == (predict_until - predict_from + 1, 1)
+    assert clf.fit_and_predict(
+        train_x, train_y, epochs=1, validation_split=0.2
+    ).shape == (predict_until - predict_from + 1, 1)
+    assert isinstance(keras_model, keras.Model)
diff --git a/autokeras/keras_layers.py b/autokeras/keras_layers.py
index 3da5b40..e219dbd 100644
--- a/autokeras/keras_layers.py
+++ b/autokeras/keras_layers.py
@@ -11,10 +11,13 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from typing import List
 
-import keras
-from keras import layers
-from keras import ops
+import tensorflow as tf
+from tensorflow import keras
+from tensorflow import nest
+from tensorflow.keras import layers
+from tensorflow.keras.layers.experimental import preprocessing
 
 from autokeras.utils import data_utils
 
@@ -23,12 +26,8 @@ NONE = "none"
 ONE_HOT = "one-hot"
 
 
-class PreprocessingLayer(layers.Layer):
-    pass
-
-
 @keras.utils.register_keras_serializable()
-class CastToFloat32(PreprocessingLayer):
+class CastToFloat32(preprocessing.PreprocessingLayer):
     def get_config(self):
         return super().get_config()
 
@@ -40,12 +39,132 @@ class CastToFloat32(PreprocessingLayer):
 
 
 @keras.utils.register_keras_serializable()
-class ExpandLastDim(PreprocessingLayer):
+class ExpandLastDim(preprocessing.PreprocessingLayer):
     def get_config(self):
         return super().get_config()
 
     def call(self, inputs):
-        return ops.expand_dims(inputs, axis=-1)
+        return tf.expand_dims(inputs, axis=-1)
 
     def adapt(self, data):
         return
+
+
+@keras.utils.register_keras_serializable()
+class MultiCategoryEncoding(preprocessing.PreprocessingLayer):
+    """Encode the categorical features to numerical features.
+
+    # Arguments
+        encoding: A list of strings, which has the same number of elements as
+            the columns in the structured data. Each of the strings specifies
+            the encoding method used for the corresponding column. Use 'int' for
+            categorical columns and 'none' for numerical columns.
+    """
+
+    # TODO: Support one-hot encoding.
+    # TODO: Support frequency encoding.
+
+    def __init__(self, encoding: List[str], **kwargs):
+        super().__init__(**kwargs)
+        self.encoding = encoding
+        self.encoding_layers = []
+        for encoding in self.encoding:
+            if encoding == NONE:
+                self.encoding_layers.append(None)
+            elif encoding == INT:
+                # Set a temporary vocabulary to prevent the error of no
+                # vocabulary when calling the layer to build the model.  The
+                # vocabulary would be reset by adapting the layer later.
+                self.encoding_layers.append(layers.StringLookup())
+            elif encoding == ONE_HOT:
+                self.encoding_layers.append(None)
+
+    def build(self, input_shape):
+        for encoding_layer in self.encoding_layers:
+            if encoding_layer is not None:
+                encoding_layer.build(tf.TensorShape([1]))
+
+    def call(self, inputs):
+        input_nodes = nest.flatten(inputs)[0]
+        split_inputs = tf.split(input_nodes, [1] * len(self.encoding), axis=-1)
+        output_nodes = []
+        for input_node, encoding_layer in zip(
+            split_inputs, self.encoding_layers
+        ):
+            if encoding_layer is None:
+                number = data_utils.cast_to_float32(input_node)
+                # Replace NaN with 0.
+                imputed = tf.where(
+                    tf.math.is_nan(number), tf.zeros_like(number), number
+                )
+                output_nodes.append(imputed)
+            else:
+                output_nodes.append(
+                    data_utils.cast_to_float32(
+                        encoding_layer(data_utils.cast_to_string(input_node))
+                    )
+                )
+        if len(output_nodes) == 1:
+            return output_nodes[0]
+        return layers.Concatenate()(output_nodes)
+
+    def adapt(self, data):
+        for index, encoding_layer in enumerate(self.encoding_layers):
+            if encoding_layer is None:
+                continue
+            data_column = data.map(lambda x: tf.slice(x, [0, index], [-1, 1]))
+            encoding_layer.adapt(data_column.map(data_utils.cast_to_string))
+
+    def get_config(self):
+        config = {
+            "encoding": self.encoding,
+        }
+        base_config = super().get_config()
+        return dict(list(base_config.items()) + list(config.items()))
+
+
+@keras.utils.register_keras_serializable()
+class WarmUp(keras.optimizers.schedules.LearningRateSchedule):
+    """official.nlp.optimization.WarmUp"""
+
+    def __init__(
+        self,
+        initial_learning_rate,
+        decay_schedule_fn,
+        warmup_steps,
+        power=1.0,
+        name=None,
+    ):
+        super(WarmUp, self).__init__()
+        self.initial_learning_rate = initial_learning_rate
+        self.warmup_steps = warmup_steps
+        self.power = power
+        self.decay_schedule_fn = decay_schedule_fn
+        self.name = name
+
+    def __call__(self, step):
+        with tf.name_scope(self.name or "WarmUp") as name:
+            # Implements polynomial warmup. i.e., if global_step < warmup_steps,
+            # the learning rate will be
+            # `global_step/num_warmup_steps * init_lr`.
+            global_step_float = tf.cast(step, tf.float32)
+            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)
+            warmup_percent_done = global_step_float / warmup_steps_float
+            warmup_learning_rate = self.initial_learning_rate * tf.math.pow(
+                warmup_percent_done, self.power
+            )
+            return tf.cond(
+                global_step_float < warmup_steps_float,
+                lambda: warmup_learning_rate,
+                lambda: self.decay_schedule_fn(step),
+                name=name,
+            )
+
+    def get_config(self):
+        return {  # pragma: no cover
+            "initial_learning_rate": self.initial_learning_rate,
+            "decay_schedule_fn": self.decay_schedule_fn,
+            "warmup_steps": self.warmup_steps,
+            "power": self.power,
+            "name": self.name,
+        }
diff --git a/autokeras/keras_layers_test.py b/autokeras/keras_layers_test.py
index 778fd0d..84cc564 100644
--- a/autokeras/keras_layers_test.py
+++ b/autokeras/keras_layers_test.py
@@ -12,12 +12,64 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
+
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 from autokeras import keras_layers as layer_module
 
 
+def test_multi_cat_encode_strings_correctly(tmp_path):
+    x_train = np.array([["a", "ab", 2.1], ["b", "bc", 1.0], ["a", "bc", "nan"]])
+    layer = layer_module.MultiCategoryEncoding(
+        [layer_module.INT, layer_module.INT, layer_module.NONE]
+    )
+    dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(32)
+
+    layer.adapt(tf.data.Dataset.from_tensor_slices(x_train).batch(32))
+
+    for data in dataset:
+        result = layer(data)
+
+    assert result[0][0] == result[2][0]
+    assert result[0][0] != result[1][0]
+    assert result[0][1] != result[1][1]
+    assert result[0][1] != result[2][1]
+    assert result[2][2] == 0
+    assert result.dtype == tf.float32
+
+
+def test_model_save_load_output_same(tmp_path):
+    x_train = np.array([["a", "ab", 2.1], ["b", "bc", 1.0], ["a", "bc", "nan"]])
+    layer = layer_module.MultiCategoryEncoding(
+        encoding=[layer_module.INT, layer_module.INT, layer_module.NONE]
+    )
+    layer.adapt(tf.data.Dataset.from_tensor_slices(x_train).batch(32))
+
+    model = keras.Sequential([keras.Input(shape=(3,), dtype=tf.string), layer])
+    model.save(os.path.join(tmp_path, "model"))
+    model2 = keras.models.load_model(os.path.join(tmp_path, "model"))
+
+    assert np.array_equal(model.predict(x_train), model2.predict(x_train))
+
+
+def test_init_multi_one_hot_encode():
+    layer_module.MultiCategoryEncoding(
+        encoding=[layer_module.ONE_HOT, layer_module.INT, layer_module.NONE]
+    )
+    # TODO: add more content when it is implemented
+
+
+def test_call_multi_with_single_column_return_right_shape():
+    x_train = np.array([["a"], ["b"], ["a"]])
+    layer = layer_module.MultiCategoryEncoding(encoding=[layer_module.INT])
+    layer.adapt(tf.data.Dataset.from_tensor_slices(x_train).batch(32))
+
+    assert layer(x_train).shape == (3, 1)
+
+
 def get_text_data():
     train = np.array(
         [
diff --git a/autokeras/nodes.py b/autokeras/nodes.py
index 367e22b..857d320 100644
--- a/autokeras/nodes.py
+++ b/autokeras/nodes.py
@@ -12,16 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Dict
+from typing import List
 from typing import Optional
 
-import keras
 import tensorflow as tf
-import tree
+from tensorflow import keras
+from tensorflow import nest
 
 from autokeras import adapters
 from autokeras import analysers
 from autokeras import blocks
+from autokeras import hyper_preprocessors as hpps_module
 from autokeras import keras_layers
+from autokeras import preprocessors
 from autokeras.engine import io_hypermodel
 from autokeras.engine import node as node_module
 from autokeras.utils import utils
@@ -36,6 +40,7 @@ def deserialize(config, custom_objects=None):
         config,
         module_objects=globals(),
         custom_objects=custom_objects,
+        printable_module_name="nodes",
     )
 
 
@@ -56,7 +61,7 @@ class Input(node_module.Node, io_hypermodel.IOHyperModel):
         return keras.Input(shape=self.shape, dtype=self.dtype)
 
     def build(self, hp, inputs=None):
-        input_node = tree.flatten(inputs)[0]
+        input_node = nest.flatten(inputs)[0]
         return keras_layers.CastToFloat32()(input_node)
 
     def get_adapter(self):
@@ -89,7 +94,7 @@ class ImageInput(Input):
 
     def build(self, hp, inputs=None):
         inputs = super().build(hp, inputs)
-        output_node = tree.flatten(inputs)[0]
+        output_node = nest.flatten(inputs)[0]
         if len(output_node.shape) == 3:
             output_node = keras_layers.ExpandLastDim()(output_node)
         return output_node
@@ -123,7 +128,7 @@ class TextInput(Input):
         return keras.Input(shape=self.shape, dtype=tf.string)
 
     def build(self, hp, inputs=None):
-        output_node = tree.flatten(inputs)[0]
+        output_node = nest.flatten(inputs)[0]
         if len(output_node.shape) == 1:
             output_node = keras_layers.ExpandLastDim()(output_node)
         return output_node
@@ -136,3 +141,145 @@ class TextInput(Input):
 
     def get_block(self):
         return blocks.TextBlock()
+
+
+class StructuredDataInput(Input):
+    """Input node for structured data.
+
+    The input data should be numpy.ndarray, pandas.DataFrame or
+    tensorflow.Dataset.  The data should be two-dimensional with numerical or
+    categorical values.
+
+    # Arguments
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data. Defaults to None. If None, it will be obtained from the
+            header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column. Defaults to None. If not None, the column_names need to be
+            specified. If None, it will be inferred from the data. A column will
+            be judged as categorical if the number of different values is less
+            than 5% of the number of instances.
+        name: String. The name of the input node. If unspecified, it will be set
+            automatically with the class name.
+    """
+
+    def __init__(
+        self,
+        column_names: Optional[List[str]] = None,
+        column_types: Optional[Dict[str, str]] = None,
+        name: Optional[str] = None,
+        **kwargs
+    ):
+        super().__init__(name=name, **kwargs)
+        self.column_names = column_names
+        self.column_types = column_types
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "column_names": self.column_names,
+                "column_types": self.column_types,
+            }
+        )
+        return config
+
+    def get_adapter(self):
+        return adapters.StructuredDataAdapter()
+
+    def get_analyser(self):
+        return analysers.StructuredDataAnalyser(
+            self.column_names, self.column_types
+        )
+
+    def get_block(self):
+        return blocks.StructuredDataBlock()
+
+    def config_from_analyser(self, analyser):
+        super().config_from_analyser(analyser)
+        self.column_names = analyser.column_names
+        # Analyser keeps the specified ones and infer the missing ones.
+        self.column_types = analyser.column_types
+
+    def build(self, hp, inputs=None):
+        return inputs
+
+
+class TimeseriesInput(StructuredDataInput):
+    """Input node for timeseries data.
+
+    # Arguments
+        lookback: Int. The range of history steps to consider for each
+            prediction. For example, if lookback=n, the data in the range of [i
+            - n, i - 1] is used to predict the value of step i. If unspecified,
+            it will be tuned automatically.
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data. Defaults to None. If None, it will be obtained from the
+            header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column.  Defaults to None. If not None, the column_names need to be
+            specified.  If None, it will be inferred from the data. A column
+            will be judged as categorical if the number of different values is
+            less than 5% of the number of instances.
+        name: String. The name of the input node. If unspecified, it will be set
+            automatically with the class name.
+    """
+
+    def __init__(
+        self,
+        lookback: Optional[int] = None,
+        column_names: Optional[List[str]] = None,
+        column_types: Optional[Dict[str, str]] = None,
+        name: Optional[str] = None,
+        **kwargs
+    ):
+        super().__init__(
+            column_names=column_names,
+            column_types=column_types,
+            name=name,
+            **kwargs
+        )
+        self.lookback = lookback
+
+    def get_config(self):
+        config = super().get_config()
+        config.update({"lookback": self.lookback})
+        return config
+
+    def get_adapter(self):
+        return adapters.TimeseriesAdapter()
+
+    def get_analyser(self):
+        return analysers.TimeseriesAnalyser(
+            column_names=self.column_names, column_types=self.column_types
+        )
+
+    def get_block(self):
+        return blocks.TimeseriesBlock()
+
+    def config_from_analyser(self, analyser):
+        super().config_from_analyser(analyser)
+
+    def get_hyper_preprocessors(self):
+        hyper_preprocessors = []
+        if self.column_names:
+            hyper_preprocessors.append(
+                hpps_module.DefaultHyperPreprocessor(
+                    preprocessors.CategoricalToNumericalPreprocessor(
+                        column_names=self.column_names,
+                        column_types=self.column_types,
+                    )
+                )
+            )
+        hyper_preprocessors.append(
+            hpps_module.DefaultHyperPreprocessor(
+                preprocessors.SlidingWindow(
+                    lookback=self.lookback, batch_size=self.batch_size
+                )
+            )
+        )
+        return hyper_preprocessors
diff --git a/autokeras/nodes_test.py b/autokeras/nodes_test.py
index dec47c6..9dd3845 100644
--- a/autokeras/nodes_test.py
+++ b/autokeras/nodes_test.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import keras_tuner
+
 from autokeras import blocks
 from autokeras import nodes
 
@@ -19,3 +21,20 @@ from autokeras import nodes
 def test_input_get_block_return_general_block():
     input_node = nodes.Input()
     assert isinstance(input_node.get_block(), blocks.GeneralBlock)
+
+
+def test_time_series_input_node_build_no_error():
+    node = nodes.TimeseriesInput(lookback=2, shape=(32,))
+    hp = keras_tuner.HyperParameters()
+
+    input_node = node.build_node(hp)
+    node.build(hp, input_node)
+
+
+def test_time_series_input_node_deserialize_build_no_error():
+    node = nodes.TimeseriesInput(lookback=2, shape=(32,))
+    node = nodes.deserialize(nodes.serialize(node))
+    hp = keras_tuner.HyperParameters()
+
+    input_node = node.build_node(hp)
+    node.build(hp, input_node)
diff --git a/autokeras/pipeline.py b/autokeras/pipeline.py
index fd89e54..be9010d 100644
--- a/autokeras/pipeline.py
+++ b/autokeras/pipeline.py
@@ -12,9 +12,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import tensorflow as tf
-import tree
+from tensorflow import keras
+from tensorflow import nest
 
 from autokeras import preprocessors as preprocessors_module
 from autokeras.engine import hyper_preprocessor as hpps_module
@@ -98,8 +98,8 @@ class Pipeline(pps_module.Preprocessor):
         sources_x = data_utils.unzip_dataset(x)
         for pps_list, data in zip(self.inputs, sources_x):
             for preprocessor in pps_list:
-                preprocessor.fit(data)  # pragma: no cover
-                data = preprocessor.transform(data)  # pragma: no cover
+                preprocessor.fit(data)
+                data = preprocessor.transform(data)
         y = dataset.map(lambda x, y: y)
         sources_y = data_utils.unzip_dataset(y)
         for pps_list, data in zip(self.outputs, sources_y):
@@ -208,7 +208,7 @@ class Pipeline(pps_module.Preprocessor):
             the heads.
         """
         outputs = []
-        for data, preprocessors in zip(tree.flatten(y), self.outputs):
+        for data, preprocessors in zip(nest.flatten(y), self.outputs):
             for preprocessor in preprocessors[::-1]:
                 if isinstance(preprocessor, pps_module.TargetPreprocessor):
                     data = preprocessor.postprocess(data)
diff --git a/autokeras/preprocessors/__init__.py b/autokeras/preprocessors/__init__.py
index a993dcf..db6d88f 100644
--- a/autokeras/preprocessors/__init__.py
+++ b/autokeras/preprocessors/__init__.py
@@ -11,12 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import keras
+import tensorflow as tf
+from tensorflow import keras
 
 from autokeras.preprocessors.common import AddOneDimension
 from autokeras.preprocessors.common import CastToInt32
 from autokeras.preprocessors.common import CastToString
+from autokeras.preprocessors.common import CategoricalToNumericalPreprocessor
 from autokeras.preprocessors.common import LambdaPreprocessor
+from autokeras.preprocessors.common import SlidingWindow
 from autokeras.preprocessors.encoders import LabelEncoder
 from autokeras.preprocessors.encoders import OneHotEncoder
 from autokeras.preprocessors.postprocessors import SigmoidPostprocessor
@@ -33,4 +36,5 @@ def deserialize(config, custom_objects=None):
         config,
         module_objects=globals(),
         custom_objects=custom_objects,
+        printable_module_name="preprocessors",
     )
diff --git a/autokeras/preprocessors/common.py b/autokeras/preprocessors/common.py
index 274db77..bfda0d8 100644
--- a/autokeras/preprocessors/common.py
+++ b/autokeras/preprocessors/common.py
@@ -14,6 +14,9 @@
 
 import tensorflow as tf
 
+from autokeras import analysers
+from autokeras import keras_layers
+from autokeras import preprocessors
 from autokeras.engine import preprocessor
 from autokeras.utils import data_utils
 
@@ -52,3 +55,99 @@ class CastToString(preprocessor.Preprocessor):
 
     def transform(self, dataset):
         return dataset.map(data_utils.cast_to_string)
+
+
+class SlidingWindow(preprocessor.Preprocessor):
+    """Apply sliding window to the dataset.
+
+    It groups the consecutive data items together. Therefore, it inserts one
+    more dimension of size `lookback` to the dataset shape after the batch_size
+    dimension. It also reduce the number of instances in the dataset by
+    (lookback - 1).
+
+    # Arguments
+        lookback: Int. The window size. The number of data items to group
+            together.
+        batch_size: Int. The batch size of the dataset.
+    """
+
+    def __init__(self, lookback, batch_size, **kwargs):
+        super().__init__(**kwargs)
+        self.lookback = lookback
+        self.batch_size = batch_size
+
+    def transform(self, dataset):
+        dataset = dataset.unbatch()
+        dataset = dataset.window(self.lookback, shift=1, drop_remainder=True)
+        dataset = dataset.flat_map(
+            lambda x: x.batch(self.lookback, drop_remainder=True)
+        )
+        dataset = dataset.batch(self.batch_size)
+        return dataset
+
+    def get_config(self):
+        return {"lookback": self.lookback, "batch_size": self.batch_size}
+
+
+class CategoricalToNumericalPreprocessor(preprocessor.Preprocessor):
+    """Encode the categorical features to numerical features.
+
+    # Arguments
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data.  Defaults to None. If None, it will be obtained from the
+            header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column. Defaults to None. If not None, the column_names need to be
+            specified.  If None, it will be inferred from the data.
+    """
+
+    def __init__(self, column_names, column_types, **kwargs):
+        super().__init__(**kwargs)
+        self.column_names = column_names
+        self.column_types = column_types
+        encoding = []
+        for column_name in self.column_names:
+            column_type = self.column_types[column_name]
+            if column_type == analysers.CATEGORICAL:
+                # TODO: Search to use one-hot or int.
+                encoding.append(keras_layers.INT)
+            else:
+                encoding.append(keras_layers.NONE)
+        self.layer = keras_layers.MultiCategoryEncoding(encoding)
+
+    def fit(self, dataset):
+        self.layer.adapt(dataset)
+
+    def transform(self, dataset):
+        return dataset.map(self.layer)
+
+    def get_config(self):
+        vocab = []
+        for encoding_layer in self.layer.encoding_layers:
+            if encoding_layer is None:
+                vocab.append([])
+            else:
+                vocab.append(encoding_layer.get_vocabulary())
+        return {
+            "column_types": self.column_types,
+            "column_names": self.column_names,
+            "encoding_layer": preprocessors.serialize(self.layer),
+            "encoding_vocab": vocab,
+        }
+
+    @classmethod
+    def from_config(cls, config):
+        init_config = {
+            "column_types": config["column_types"],
+            "column_names": config["column_names"],
+        }
+        obj = cls(**init_config)
+        obj.layer = preprocessors.deserialize(config["encoding_layer"])
+        for encoding_layer, vocab in zip(
+            obj.layer.encoding_layers, config["encoding_vocab"]
+        ):
+            if encoding_layer is not None:
+                encoding_layer.set_vocabulary(vocab)
+        return obj
diff --git a/autokeras/preprocessors/common_test.py b/autokeras/preprocessors/common_test.py
index 96f10df..06bfec4 100644
--- a/autokeras/preprocessors/common_test.py
+++ b/autokeras/preprocessors/common_test.py
@@ -12,10 +12,45 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import numpy as np
 import tensorflow as tf
 
 from autokeras import test_utils
 from autokeras.preprocessors import common
+from autokeras.utils import data_utils
+
+
+def test_time_series_input_transform():
+    dataset = tf.data.Dataset.from_tensor_slices(np.random.rand(100, 32)).batch(
+        32
+    )
+    preprocessor = common.SlidingWindow(lookback=2, batch_size=32)
+    x = preprocessor.transform(dataset)
+    assert data_utils.dataset_shape(x).as_list() == [None, 2, 32]
+
+
+def test_categorical_to_numerical_input_transform():
+    x_train = np.array([["a", "ab", 2.1], ["b", "bc", 1.0], ["a", "bc", "nan"]])
+    preprocessor = common.CategoricalToNumericalPreprocessor(
+        column_names=["column_a", "column_b", "column_c"],
+        column_types={
+            "column_a": "categorical",
+            "column_b": "categorical",
+            "column_c": "numerical",
+        },
+    )
+    dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(32)
+
+    preprocessor.fit(tf.data.Dataset.from_tensor_slices(x_train).batch(32))
+    results = preprocessor.transform(dataset)
+
+    for result in results:
+        assert result[0][0] == result[2][0]
+        assert result[0][0] != result[1][0]
+        assert result[0][1] != result[1][1]
+        assert result[0][1] != result[2][1]
+        assert result[2][2] == 0
+        assert result.dtype == tf.float32
 
 
 def test_cast_to_int32_return_int32():
diff --git a/autokeras/prototype/__init__.py b/autokeras/prototype/__init__.py
new file mode 100644
index 0000000..3d57915
--- /dev/null
+++ b/autokeras/prototype/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
diff --git a/autokeras/prototype/base_block.py b/autokeras/prototype/base_block.py
new file mode 100644
index 0000000..4170e7d
--- /dev/null
+++ b/autokeras/prototype/base_block.py
@@ -0,0 +1,27 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from autokeras.engine import block
+from autokeras.prototype import graph_state
+
+
+class BaseBlock(block.Block):
+    # Renaming the autokeras.engine.block to BaseBlock.
+    # Not open for extension.
+    pass
+
+    def _build_wrapper(self, hp, *args, **kwargs):
+        with graph_state.get_state().build_scope(self):
+            with hp.name_scope(self.name):
+                return super()._build_wrapper(hp, *args, **kwargs)
diff --git a/autokeras/prototype/block.py b/autokeras/prototype/block.py
new file mode 100644
index 0000000..f86e60a
--- /dev/null
+++ b/autokeras/prototype/block.py
@@ -0,0 +1,34 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from autokeras.prototype import base_block
+from autokeras.prototype import graph_state
+
+
+def is_dataset(inputs):
+    raise NotImplementedError
+
+
+def convert_to_keras_inputs(inputs):
+    raise NotImplementedError
+
+
+class Block(base_block.BaseBlock):
+    # Open for extension. The same as the old Block class.
+    def _build_wrapper(self, hp, inputs, *args, **kwargs):
+        # Accept only KerasTensor.
+        if is_dataset(inputs):
+            inputs = convert_to_keras_inputs(inputs)
+        graph_state.get_state().register_inputs(inputs)
+        return super()._build_wrapper(hp, inputs, *args, **kwargs)
diff --git a/autokeras/prototype/blocks.py b/autokeras/prototype/blocks.py
new file mode 100644
index 0000000..da0083b
--- /dev/null
+++ b/autokeras/prototype/blocks.py
@@ -0,0 +1,29 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""This file is some stub blocks for testing the prototype."""
+from autokeras import nodes
+from autokeras.prototype import block
+from autokeras.prototype import preprocessor
+
+
+class StructuredDataInput(nodes.StructuredDataInput):
+    pass
+
+
+class FeatureSelection(preprocessor.Preprocessor):
+    pass
+
+
+class DenseBlock(block.Block):
+    pass
diff --git a/autokeras/prototype/flexible_block.py b/autokeras/prototype/flexible_block.py
new file mode 100644
index 0000000..a522ce7
--- /dev/null
+++ b/autokeras/prototype/flexible_block.py
@@ -0,0 +1,24 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from autokeras.prototype import base_block
+
+
+class FlexibleBlock(base_block.BaseBlock):
+    # Open for extension.
+    def _build_wrapper(self, hp, inputs, *args, **kwargs):
+        # Accept both KerasTensor & Dataset.
+        # If the inputs are datasets, build the block into a model and pass
+        # through it.
+        pass
diff --git a/autokeras/prototype/graph.py b/autokeras/prototype/graph.py
new file mode 100644
index 0000000..673c229
--- /dev/null
+++ b/autokeras/prototype/graph.py
@@ -0,0 +1,63 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from tensorflow import nest
+
+from autokeras import graph
+from autokeras.prototype import graph_state
+from autokeras.prototype import pipeline as pipeline_module
+
+
+class Graph(graph.Graph):
+    def __init__(self, inputs=None, outputs=None, **kwargs):
+        super().__init__(inputs, outputs, **kwargs)
+
+    def build(self, hp):
+        """Build the HyperModel into a Keras Model."""
+        state = graph_state.init_state()
+        self.compile()
+        keras_nodes = {}
+        # keras_input_nodes = []
+
+        # Preparing the inputs of the pipeline.
+        # for node in self.inputs:
+        #     node_id = self._node_to_id[node]
+        #     input_node = node.build_node(hp)
+        #     output_node = node.build(hp, input_node)
+        #     keras_input_nodes.append(input_node)
+        #     keras_nodes[node_id] = output_node
+
+        # Connecting through the blocks.
+        # Don't check the block type to deal with the output since the block has
+        # sub blocks of different types. The difference should all be handled in
+        # block._build_wrapper().
+        for block in self.blocks:
+            temp_inputs = [
+                keras_nodes[self._node_to_id[input_node]]
+                for input_node in block.inputs
+            ]
+            outputs = block.build(hp, inputs=temp_inputs)
+            outputs = nest.flatten(outputs)
+            for output_node, real_output_node in zip(block.outputs, outputs):
+                keras_nodes[self._node_to_id[output_node]] = real_output_node
+
+        for output_node in self.outputs:
+            node = keras_nodes[self._node_to_id[output_node]]
+            state.register_outputs(node)
+
+        model = state.build_model()
+        self._compile_keras_model(hp, model)
+
+        pipeline = pipeline_module.Pipeline.from_state(graph_state.get_state())
+        return pipeline
diff --git a/autokeras/prototype/graph_state.py b/autokeras/prototype/graph_state.py
new file mode 100644
index 0000000..7f3bd77
--- /dev/null
+++ b/autokeras/prototype/graph_state.py
@@ -0,0 +1,64 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import threading
+
+from tensorflow import keras
+
+STATE = {}
+
+
+class State:
+    # It is a global accessible object to record all the useful information of a
+    # specific build of the Graph.
+    def __init__(self):
+        self.inputs = []
+        self.outputs = []
+        # This is a stack of blocks that current running their build()
+        # functions.
+        self.blocks = []
+
+        # Passing y class info from preprocessor to postprocessor.
+        self.y_info
+
+    def register_inputs(self, inputs):
+        raise NotImplementedError
+
+    def register_outputs(self, inputs):
+        # Remember to check duplication
+        raise NotImplementedError
+
+    def register_preprocessor(self, inputs, outputs, preprocessor):
+        raise NotImplementedError
+
+    def build_model(self):
+        self.model = keras.Model(inputs=self.inputs, outputs=self.outputs)
+        return self.model
+
+    def build_scope(self, block):
+        self.blocks.append(block)
+        try:
+            yield
+        finally:
+            self.blocks.pop()
+
+
+def get_state():
+    return STATE[threading.get_ident()]
+
+
+def init_state():
+    state = State()
+    STATE[threading.get_ident()] = state
+    return state
diff --git a/autokeras/prototype/graph_test.py b/autokeras/prototype/graph_test.py
new file mode 100644
index 0000000..3d57915
--- /dev/null
+++ b/autokeras/prototype/graph_test.py
@@ -0,0 +1,13 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
diff --git a/autokeras/prototype/pipeline.py b/autokeras/prototype/pipeline.py
new file mode 100644
index 0000000..c60d48e
--- /dev/null
+++ b/autokeras/prototype/pipeline.py
@@ -0,0 +1,23 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+class Pipeline:
+    # It should include the ConcretePreprocessors, the keras.Model, and the
+    # ConcretePostprocessors.
+
+    # This class should also be exportable. It is the default export format.
+    @classmethod
+    def from_state(cls, state):
+        raise NotImplementedError
diff --git a/autokeras/prototype/postprocessor.py b/autokeras/prototype/postprocessor.py
new file mode 100644
index 0000000..11e65a0
--- /dev/null
+++ b/autokeras/prototype/postprocessor.py
@@ -0,0 +1,26 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from autokeras.prototype import base_block
+
+
+class Postprocessor(base_block.BaseBlock):
+    def _build_wrapper(self, hp, inputs, *args, **kwargs):
+        # Accept only KerasTensor.
+        # Return a KerasTensor.
+        # Register ConcretePostprocessor.
+        # How to register it to the right input node?
+        # Use the output_node (inputs in the args) as the id to push stack in
+        # graph state.
+        return super()._build_wrapper(hp, inputs, *args, **kwargs)
diff --git a/autokeras/prototype/preprocessor.py b/autokeras/prototype/preprocessor.py
new file mode 100644
index 0000000..5fd9d6b
--- /dev/null
+++ b/autokeras/prototype/preprocessor.py
@@ -0,0 +1,52 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from autokeras.prototype import base_block
+from autokeras.prototype import graph_state
+
+
+class Preprocessor(base_block.BaseBlock):
+    def _build_wrapper(self, hp, inputs, *args, **kwargs):
+        # Accept only Dataset.
+        # Return a Dataset.
+        # Register ConcretePreprocessor.
+
+        # How to register it to the right input node?
+
+        # How do we register the ConcretePreprocessor?
+        # Just get the return value of .build(). Register it to graph state
+        # together with the input and output dataset.
+
+        # What do we do when there are Preprocessors within Preprocessors?
+        # We don't register all of them. Only register the outter most one.
+        # It is more convenient to just have this one preprocessor to do all the
+        # inside steps.
+        # To judge if the current one is the outter most one, we need to use the
+        # "with" statement to create a scope when a HyperModel.build() is
+        # called. Record a stack of HyperModel, whose .build() is running. The
+        # lower in the stack, the outter the HyperModel is.
+        concrete_preprocessor = super()._build_wrapper(
+            hp, inputs, *args, **kwargs
+        )
+        outputs = concrete_preprocessor.fit_transform(inputs)
+
+        state = graph_state.get_state()
+        if not any([isinstance(block, Preprocessor) for block in state.blocks]):
+            state.register_preprocessor(inputs, outputs, concrete_preprocessor)
+
+        return concrete_preprocessor
+
+    def build(self, hp, dataset):
+        # Should return a ConcretePreprocessor.
+        pass
diff --git a/autokeras/tasks/__init__.py b/autokeras/tasks/__init__.py
index 7eaa82b..edc340b 100644
--- a/autokeras/tasks/__init__.py
+++ b/autokeras/tasks/__init__.py
@@ -14,5 +14,8 @@
 
 from autokeras.tasks.image import ImageClassifier
 from autokeras.tasks.image import ImageRegressor
+from autokeras.tasks.structured_data import StructuredDataClassifier
+from autokeras.tasks.structured_data import StructuredDataRegressor
 from autokeras.tasks.text import TextClassifier
 from autokeras.tasks.text import TextRegressor
+from autokeras.tasks.time_series_forecaster import TimeseriesForecaster
diff --git a/autokeras/tasks/image.py b/autokeras/tasks/image.py
index 73a2e59..ad6fe8f 100644
--- a/autokeras/tasks/image.py
+++ b/autokeras/tasks/image.py
@@ -19,8 +19,8 @@ from typing import Tuple
 from typing import Type
 from typing import Union
 
-import keras
 import tensorflow as tf
+from tensorflow import keras
 
 from autokeras import auto_model
 from autokeras import blocks
@@ -309,3 +309,271 @@ class ImageRegressor(SupervisedImagePipeline):
             **kwargs
         )
         return history
+
+
+class ImageSegmenter(SupervisedImagePipeline):
+    """AutoKeras image segmentation class.
+    # Arguments
+        num_classes: Int. Defaults to None. If None, it will be inferred from
+            the data.
+        loss: A Keras loss function. Defaults to use 'binary_crossentropy' or
+            'categorical_crossentropy' based on the number of classes.
+        metrics: A list of metrics used to measure the accuracy of the model,
+            default to 'accuracy'.
+        project_name: String. The name of the AutoModel.
+            Defaults to 'image_segmenter'.
+        max_trials: Int. The maximum number of different Keras Models to try.
+            The search may finish before reaching the max_trials. Defaults to
+            100.
+        directory: String. The path to a directory for storing the search
+            outputs. Defaults to None, which would create a folder with the
+            name of the AutoModel in the current directory.
+        objective: String. Name of model metric to minimize
+            or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.
+        tuner: String or subclass of AutoTuner. If string, it should be one of
+            'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a
+            subclass of AutoTuner. If left unspecified, it uses a task specific
+            tuner, which first evaluates the most commonly used models for the
+            task before exploring other models.
+        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing
+            project of the same name if one is found. Otherwise, overwrites the
+            project.
+        seed: Int. Random seed.
+        **kwargs: Any arguments supported by AutoModel.
+    """
+
+    def __init__(
+        self,
+        num_classes: Optional[int] = None,
+        loss: types.LossType = None,
+        metrics: Optional[types.MetricsType] = None,
+        project_name: str = "image_segmenter",
+        max_trials: int = 100,
+        directory: Union[str, Path, None] = None,
+        objective: str = "val_loss",
+        tuner: Union[str, Type[tuner.AutoTuner]] = None,
+        overwrite: bool = False,
+        seed: Optional[int] = None,
+        **kwargs
+    ):
+        if tuner is None:
+            tuner = greedy.Greedy
+        super().__init__(
+            outputs=blocks.SegmentationHead(
+                num_classes=num_classes, loss=loss, metrics=metrics
+            ),
+            max_trials=max_trials,
+            directory=directory,
+            project_name=project_name,
+            objective=objective,
+            tuner=tuner,
+            overwrite=overwrite,
+            seed=seed,
+            **kwargs
+        )
+
+    def fit(
+        self,
+        x: Optional[types.DatasetType] = None,
+        y: Optional[types.DatasetType] = None,
+        epochs: Optional[int] = None,
+        callbacks: Optional[List[keras.callbacks.Callback]] = None,
+        validation_split: Optional[float] = 0.2,
+        validation_data: Union[
+            types.DatasetType, Tuple[types.DatasetType], None
+        ] = None,
+        **kwargs
+    ):
+        """Search for the best model and hyperparameters for the AutoModel.
+
+        It will search for the best model based on the performances on
+        validation data.
+
+        # Arguments
+            x: numpy.ndarray or tensorflow.Dataset. Training image dataset x.
+                The shape of the data should be (samples, width, height) or
+                (samples, width, height, channels).
+            y: numpy.ndarray or tensorflow.Dataset. Training image data set y.
+                It should be a tensor and the height and width should be the
+                same as x. Each element in the tensor is the label of the
+                corresponding pixel.
+            epochs: Int. The number of epochs to train each model during the
+                search. If unspecified, by default we train for a maximum of
+                1000 epochs, but we stop training if the validation loss stops
+                improving for 10 epochs (unless you specified an EarlyStopping
+                callback as part of the callbacks argument, in which case the
+                EarlyStopping callback you specified will determine early
+                stopping).
+            callbacks: List of Keras callbacks to apply during training and
+                validation.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch. The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset.  The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data.  The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        # Returns
+            history: A Keras History object corresponding to the best model.
+                Its History.history attribute is a record of training
+                loss values and metrics values at successive epochs, as well as
+                validation loss values and validation metrics values (if
+                applicable).
+        """
+        history = super().fit(
+            x=x,
+            y=y,
+            epochs=epochs,
+            callbacks=callbacks,
+            validation_split=validation_split,
+            validation_data=validation_data,
+            **kwargs
+        )
+        return history
+
+
+class ImageObjectDetector(SupervisedImagePipeline):
+    """AutoKeras image object detector class.
+
+    # Arguments
+        num_classes: Int. Defaults to None. If None, it will be inferred from
+            the data.
+        multi_label: Boolean. Defaults to False.
+        loss: A Keras loss function. Defaults to use 'binary_crossentropy' or
+            'categorical_crossentropy' based on the number of classes.
+        metrics: A list of Keras metrics. Defaults to use 'accuracy'.
+        project_name: String. The name of the AutoModel.
+            Defaults to 'image_classifier'.
+        max_trials: Int. The maximum number of different Keras Models to try.
+            The search may finish before reaching the max_trials. Defaults to
+            100.
+        directory: String. The path to a directory for storing the search
+            outputs. Defaults to None, which would create a folder with the
+            name of the AutoModel in the current directory.
+        objective: String. Name of model metric to minimize
+            or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.
+        tuner: String or subclass of AutoTuner. If string, it should be one of
+            'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a
+            subclass of AutoTuner. If left unspecified, it uses a task specific
+            tuner, which first evaluates the most commonly used models for the
+            task before exploring other models.
+        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing
+            project of the same name if one is found. Otherwise, overwrites the
+            project.
+        seed: Int. Random seed.
+        max_model_size: Int. Maximum number of scalars in the parameters of a
+            model. Models larger than this are rejected.
+        **kwargs: Any arguments supported by AutoModel.
+    """
+
+    def __init__(
+        self,
+        num_classes: Optional[int] = None,
+        multi_label: bool = False,
+        loss: types.LossType = None,
+        metrics: Optional[types.MetricsType] = None,
+        project_name: str = "image_classifier",
+        max_trials: int = 100,
+        directory: Union[str, Path, None] = None,
+        objective: str = "val_loss",
+        tuner: Union[str, Type[tuner.AutoTuner]] = None,
+        overwrite: bool = False,
+        seed: Optional[int] = None,
+        max_model_size: Optional[int] = None,
+        **kwargs
+    ):
+        pass  # pragma: no cover
+
+    def fit(
+        self,
+        x: Optional[types.DatasetType] = None,
+        y: Optional[types.DatasetType] = None,
+        epochs: Optional[int] = None,
+        callbacks: Optional[List[keras.callbacks.Callback]] = None,
+        validation_split: Optional[float] = 0.2,
+        validation_data: Union[
+            tf.data.Dataset, Tuple[types.DatasetType, types.DatasetType], None
+        ] = None,
+        **kwargs
+    ):
+        """Search for the best model and hyperparameters for the AutoModel.
+
+        It will search for the best model based on the performances on
+        validation data.
+
+        # Arguments
+            x: numpy.ndarray or tensorflow.Dataset. Training data x. The shape
+                of the data should be (samples, width, height) or (samples,
+                width, height, channels). If it's a tensorflow.Dataset only x is
+                used, and each sample has an image, and corresponding (bboxes,
+                classIDs).
+            y: numpy.ndarray. Training data y. They are the
+                tuples of bounding boxes and their corresponding class IDs
+                w.r.t. the images in x. Each bounding box is defined by 4
+                values [ymin, xmin, ymax, xmax]. Box coordinates are measured
+                from top left image corner, are 0-indexed and proportional to
+                sides i.e. between [0,1]. Shape of the bounding boxes should be
+                (None, 4), and shape of the classIDs should be (None,) in each
+                tuple, where None represents the number of bounding boxes in a
+                single image.
+            epochs: Int. The number of epochs to train each model during the
+                search.  If unspecified, by default we train for a maximum of
+                1000 epochs, but we stop training if the validation loss stops
+                improving for 10 epochs (unless you specified an EarlyStopping
+                callback as part of the callbacks argument, in which case the
+                EarlyStopping callback you specified will determine early
+                stopping).
+            callbacks: List of Keras callbacks to apply during training and
+                validation.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch. The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset. The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data. The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        """
+        pass  # pragma: no cover
+
+    def predict(self, x, **kwargs):
+        """Predict the output for a given testing data.
+
+        # Arguments
+            x: numpy.ndarray or tensorflow.Dataset. Testing data x. The shape of
+                the data should be (samples, width, height) or (samples, width,
+                height, channels).
+            **kwargs: Any arguments supported by keras.Model.predict.
+
+        # Returns
+            labels: [batch_size, 3] shaped tensor containing tuples of (bboxes,
+                classIDs, scores) for each image in the testing data x, where
+                each bounding box is defined by 4 values [ymin, xmin, ymax,
+                xmax]. Box coordinates are measured from top left image corner,
+                are 0-indexed and proportional to sides i.e. between [0,1].
+                Shape of the bounding boxes should be (None, 4), and shape of
+                the classIDs should be (None,) in each tuple, where None
+                represents the number of bounding boxes detected in an image.
+                The scores denote the probability with which a class is detected
+                in the corresponding bounding box.
+        """
+        pass  # pragma: no cover
diff --git a/autokeras/tasks/image_test.py b/autokeras/tasks/image_test.py
index 4e3010d..7a5f384 100644
--- a/autokeras/tasks/image_test.py
+++ b/autokeras/tasks/image_test.py
@@ -40,3 +40,33 @@ def test_img_reg_fit_call_auto_model_fit(fit, tmp_path):
     )
 
     assert fit.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_img_seg_fit_call_auto_model_fit(fit, tmp_path):
+    auto_model = ak.tasks.image.ImageSegmenter(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(
+        x=test_utils.generate_data(num_instances=100, shape=(32, 32, 3)),
+        y=test_utils.generate_data(num_instances=100, shape=(32, 32)),
+    )
+
+    assert fit.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_img_obj_det_fit_call_auto_model_fit(fit, tmp_path):
+    auto_model = ak.tasks.image.ImageObjectDetector(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    images, labels = test_utils.get_object_detection_data()
+
+    auto_model.fit(
+        x=images,
+        y=labels,
+    )
+
+    assert fit.is_called
diff --git a/autokeras/tasks/structured_data.py b/autokeras/tasks/structured_data.py
new file mode 100644
index 0000000..8596059
--- /dev/null
+++ b/autokeras/tasks/structured_data.py
@@ -0,0 +1,416 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import pathlib
+from typing import Dict
+from typing import List
+from typing import Optional
+from typing import Type
+from typing import Union
+
+import pandas as pd
+import tensorflow as tf
+from tensorflow import nest
+
+from autokeras import auto_model
+from autokeras import blocks
+from autokeras import nodes as input_module
+from autokeras.engine import tuner
+from autokeras.tuners import task_specific
+from autokeras.utils import types
+
+
+class BaseStructuredDataPipeline(auto_model.AutoModel):
+    def __init__(self, inputs, outputs, **kwargs):
+        self.check(inputs.column_names, inputs.column_types)
+        super().__init__(inputs=inputs, outputs=outputs, **kwargs)
+        self._target_col_name = None
+
+    @staticmethod
+    def _read_from_csv(x, y):
+        df = pd.read_csv(x)
+        target = df.pop(y).to_numpy()
+        return df, target
+
+    def check(self, column_names, column_types):
+        if column_types:
+            for column_type in column_types.values():
+                if column_type not in ["categorical", "numerical"]:
+                    raise ValueError(
+                        'column_types should be either "categorical" '
+                        'or "numerical", but got {name}'.format(
+                            name=column_type
+                        )
+                    )
+
+    def check_in_fit(self, x):
+        input_node = nest.flatten(self.inputs)[0]
+        # Extract column_names from pd.DataFrame.
+        if isinstance(x, pd.DataFrame) and input_node.column_names is None:
+            input_node.column_names = list(x.columns)
+
+        if input_node.column_names and input_node.column_types:
+            for column_name in input_node.column_types:
+                if column_name not in input_node.column_names:
+                    raise ValueError(
+                        "column_names and column_types are "
+                        "mismatched. Cannot find column name "
+                        "{name} in the data.".format(name=column_name)
+                    )
+
+    def read_for_predict(self, x):
+        if isinstance(x, str):
+            x = pd.read_csv(x)
+            if self._target_col_name in x:
+                x.pop(self._target_col_name)
+        return x
+
+    def fit(
+        self,
+        x=None,
+        y=None,
+        epochs=None,
+        callbacks=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        """Search for the best model and hyperparameters for the AutoModel.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Training data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the training data.
+            y: String, numpy.ndarray, or tensorflow.Dataset. Training data y.
+                If the data is from a csv file, it should be a string, which is
+                the name of the target column. Otherwise, it can be
+                single-column or multi-column. The values should all be
+                numerical.
+            epochs: Int. The number of epochs to train each model during the
+                search. If unspecified, we would use epochs equal to 1000 and
+                early stopping with patience equal to 10.
+            callbacks: List of Keras callbacks to apply during training and
+                validation.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch. The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset. The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data. The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        # Returns
+            history: A Keras History object corresponding to the best model.
+                Its History.history attribute is a record of training
+                loss values and metrics values at successive epochs, as well as
+                validation loss values and validation metrics values (if
+                applicable).
+        """
+        # x is file path of training data
+        if isinstance(x, str):
+            self._target_col_name = y
+            x, y = self._read_from_csv(x, y)
+
+        if validation_data and not isinstance(validation_data, tf.data.Dataset):
+            x_val, y_val = validation_data
+            if isinstance(x_val, str):
+                validation_data = self._read_from_csv(x_val, y_val)
+
+        self.check_in_fit(x)
+
+        history = super().fit(
+            x=x,
+            y=y,
+            epochs=epochs,
+            callbacks=callbacks,
+            validation_split=validation_split,
+            validation_data=validation_data,
+            **kwargs
+        )
+        return history
+
+    def predict(self, x, **kwargs):
+        """Predict the output for a given testing data.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Testing data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the testing data.
+            **kwargs: Any arguments supported by keras.Model.predict.
+
+        # Returns
+            A list of numpy.ndarray objects or a single numpy.ndarray.
+            The predicted results.
+        """
+        x = self.read_for_predict(x)
+
+        return super().predict(x=x, **kwargs)
+
+    def evaluate(self, x, y=None, **kwargs):
+        """Evaluate the best model for the given data.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Testing data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the testing data.
+            y: String, numpy.ndarray, or tensorflow.Dataset. Testing data y.
+                If the data is from a csv file, it should be a string
+                corresponding to the label column.
+            **kwargs: Any arguments supported by keras.Model.evaluate.
+
+        # Returns
+            Scalar test loss (if the model has a single output and no metrics)
+            or list of scalars (if the model has multiple outputs and/or
+            metrics). The attribute model.metrics_names will give you the
+            display labels for the scalar outputs.
+        """
+        if isinstance(x, str):
+            x, y = self._read_from_csv(x, y)
+        return super().evaluate(x=x, y=y, **kwargs)
+
+
+class SupervisedStructuredDataPipeline(BaseStructuredDataPipeline):
+    def __init__(self, outputs, column_names, column_types, **kwargs):
+        inputs = input_module.StructuredDataInput(
+            column_names=column_names, column_types=column_types
+        )
+        super().__init__(inputs=inputs, outputs=outputs, **kwargs)
+
+
+class StructuredDataClassifier(SupervisedStructuredDataPipeline):
+    """AutoKeras structured data classification class.
+
+    # Arguments
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data excluding the target column. Defaults to None. If None, it will
+            obtained from the header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column.  Defaults to None. If not None, the column_names need to be
+            specified.  If None, it will be inferred from the data.
+        num_classes: Int. Defaults to None. If None, it will be inferred from
+            the data.
+        multi_label: Boolean. Defaults to False.
+        loss: A Keras loss function. Defaults to use 'binary_crossentropy' or
+            'categorical_crossentropy' based on the number of classes.
+        metrics: A list of Keras metrics. Defaults to use 'accuracy'.
+        project_name: String. The name of the AutoModel. Defaults to
+            'structured_data_classifier'.
+        max_trials: Int. The maximum number of different Keras Models to try.
+            The search may finish before reaching the max_trials. Defaults to
+            100.
+        directory: String. The path to a directory for storing the search
+            outputs. Defaults to None, which would create a folder with the
+            name of the AutoModel in the current directory.
+        objective: String. Name of model metric to minimize
+            or maximize. Defaults to 'val_accuracy'.
+        tuner: String or subclass of AutoTuner. If string, it should be one of
+            'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a
+            subclass of AutoTuner. If left unspecified, it uses a task specific
+            tuner, which first evaluates the most commonly used models for the
+            task before exploring other models.
+        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing
+            project of the same name if one is found. Otherwise, overwrites the
+            project.
+        seed: Int. Random seed.
+        max_model_size: Int. Maximum number of scalars in the parameters of a
+            model. Models larger than this are rejected.
+        **kwargs: Any arguments supported by AutoModel.
+    """
+
+    def __init__(
+        self,
+        column_names: Optional[List[str]] = None,
+        column_types: Optional[Dict] = None,
+        num_classes: Optional[int] = None,
+        multi_label: bool = False,
+        loss: Optional[types.LossType] = None,
+        metrics: Optional[types.MetricsType] = None,
+        project_name: str = "structured_data_classifier",
+        max_trials: int = 100,
+        directory: Optional[Union[str, pathlib.Path]] = None,
+        objective: str = "val_accuracy",
+        tuner: Union[str, Type[tuner.AutoTuner]] = None,
+        overwrite: bool = False,
+        seed: Optional[int] = None,
+        max_model_size: Optional[int] = None,
+        **kwargs
+    ):
+        if tuner is None:
+            tuner = task_specific.StructuredDataClassifierTuner
+        super().__init__(
+            outputs=blocks.ClassificationHead(
+                num_classes=num_classes,
+                multi_label=multi_label,
+                loss=loss,
+                metrics=metrics,
+            ),
+            column_names=column_names,
+            column_types=column_types,
+            max_trials=max_trials,
+            directory=directory,
+            project_name=project_name,
+            objective=objective,
+            tuner=tuner,
+            overwrite=overwrite,
+            seed=seed,
+            max_model_size=max_model_size,
+            **kwargs
+        )
+
+    def fit(
+        self,
+        x=None,
+        y=None,
+        epochs=None,
+        callbacks=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        """Search for the best model and hyperparameters for the AutoModel.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Training data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the training data.
+            y: String, numpy.ndarray, or tensorflow.Dataset. Training data y.
+                If the data is from a csv file, it should be a string, which is
+                the name of the target column. Otherwise, It can be raw labels,
+                one-hot encoded if more than two classes, or binary encoded for
+                binary classification.
+            epochs: Int. The number of epochs to train each model during the
+                search. If unspecified, we would use epochs equal to 1000 and
+                early stopping with patience equal to 10.
+            callbacks: List of Keras callbacks to apply during training and
+                validation.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data.  The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch.  The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data.  `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        # Returns
+            history: A Keras History object corresponding to the best model.
+                Its History.history attribute is a record of training
+                loss values and metrics values at successive epochs, as well as
+                validation loss values and validation metrics values (if
+                applicable).
+        """
+        history = super().fit(
+            x=x,
+            y=y,
+            epochs=epochs,
+            callbacks=callbacks,
+            validation_split=validation_split,
+            validation_data=validation_data,
+            **kwargs
+        )
+        return history
+
+
+class StructuredDataRegressor(SupervisedStructuredDataPipeline):
+    """AutoKeras structured data regression class.
+
+    # Arguments
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data excluding the target column. Defaults to None. If None, it will
+            obtained from the header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column. Defaults to None. If not None, the column_names need to be
+            specified. If None, it will be inferred from the data.
+        output_dim: Int. The number of output dimensions. Defaults to None.
+            If None, it will be inferred from the data.
+        loss: A Keras loss function. Defaults to use 'mean_squared_error'.
+        metrics: A list of Keras metrics. Defaults to use 'mean_squared_error'.
+        project_name: String. The name of the AutoModel. Defaults to
+            'structured_data_regressor'.
+        max_trials: Int. The maximum number of different Keras Models to try.
+            The search may finish before reaching the max_trials. Defaults to
+            100.
+        directory: String. The path to a directory for storing the search
+            outputs. Defaults to None, which would create a folder with the
+            name of the AutoModel in the current directory.
+        objective: String. Name of model metric to minimize
+            or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.
+        tuner: String or subclass of AutoTuner. If string, it should be one of
+            'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a
+            subclass of AutoTuner. If left unspecified, it uses a task specific
+            tuner, which first evaluates the most commonly used models for the
+            task before exploring other models.
+        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing
+            project of the same name if one is found. Otherwise, overwrites the
+            project.
+        seed: Int. Random seed.
+        max_model_size: Int. Maximum number of scalars in the parameters of a
+            model. Models larger than this are rejected.
+        **kwargs: Any arguments supported by AutoModel.
+    """
+
+    def __init__(
+        self,
+        column_names: Optional[List[str]] = None,
+        column_types: Optional[Dict[str, str]] = None,
+        output_dim: Optional[int] = None,
+        loss: types.LossType = "mean_squared_error",
+        metrics: Optional[types.MetricsType] = None,
+        project_name: str = "structured_data_regressor",
+        max_trials: int = 100,
+        directory: Union[str, pathlib.Path, None] = None,
+        objective: str = "val_loss",
+        tuner: Union[str, Type[tuner.AutoTuner]] = None,
+        overwrite: bool = False,
+        seed: Optional[int] = None,
+        max_model_size: Optional[int] = None,
+        **kwargs
+    ):
+        if tuner is None:
+            tuner = task_specific.StructuredDataRegressorTuner
+        super().__init__(
+            outputs=blocks.RegressionHead(
+                output_dim=output_dim, loss=loss, metrics=metrics
+            ),
+            column_names=column_names,
+            column_types=column_types,
+            max_trials=max_trials,
+            directory=directory,
+            project_name=project_name,
+            objective=objective,
+            tuner=tuner,
+            overwrite=overwrite,
+            seed=seed,
+            max_model_size=max_model_size,
+            **kwargs
+        )
diff --git a/autokeras/tasks/structured_data_test.py b/autokeras/tasks/structured_data_test.py
new file mode 100644
index 0000000..b3db653
--- /dev/null
+++ b/autokeras/tasks/structured_data_test.py
@@ -0,0 +1,146 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from unittest import mock
+
+import numpy as np
+import pandas as pd
+import pytest
+from tensorflow import nest
+
+import autokeras as ak
+from autokeras import test_utils
+
+
+def test_raise_error_unknown_str_in_col_type(tmp_path):
+    with pytest.raises(ValueError) as info:
+        ak.StructuredDataClassifier(
+            column_types={"age": "num", "parch": "categorical"},
+            directory=tmp_path,
+            seed=test_utils.SEED,
+        )
+
+    assert 'column_types should be either "categorical"' in str(info.value)
+
+
+def test_structured_data_input_name_type_mismatch_error(tmp_path):
+    with pytest.raises(ValueError) as info:
+        clf = ak.StructuredDataClassifier(
+            column_types={"_age": "numerical", "parch": "categorical"},
+            column_names=["age", "fare"],
+            directory=tmp_path,
+            seed=test_utils.SEED,
+        )
+        clf.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+
+    assert "column_names and column_types are mismatched." in str(info.value)
+
+
+def test_structured_data_col_type_no_name_error(tmp_path):
+    with pytest.raises(ValueError) as info:
+        clf = ak.StructuredDataClassifier(
+            column_types={"age": "numerical", "parch": "categorical"},
+            directory=tmp_path,
+            seed=test_utils.SEED,
+        )
+        clf.fit(x=np.random.rand(100, 30), y=np.random.rand(100, 1))
+
+    assert "column_names must be specified" in str(info.value)
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_structured_data_get_col_names_from_df(fit, tmp_path):
+    clf = ak.StructuredDataClassifier(
+        directory=tmp_path,
+        seed=test_utils.SEED,
+    )
+    clf.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+
+    assert nest.flatten(clf.inputs)[0].column_names[0] == "sex"
+
+
+@mock.patch("autokeras.AutoModel.fit")
+@mock.patch("autokeras.AutoModel.evaluate")
+def test_structured_clf_evaluate_call_automodel_evaluate(
+    evaluate, fit, tmp_path
+):
+    auto_model = ak.StructuredDataClassifier(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+    auto_model.evaluate(x=test_utils.TRAIN_CSV_PATH, y="survived")
+
+    assert evaluate.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+@mock.patch("autokeras.AutoModel.predict")
+def test_structured_clf_predict_csv_call_automodel_predict(
+    predict, fit, tmp_path
+):
+    auto_model = ak.StructuredDataClassifier(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+    auto_model.predict(x=test_utils.TEST_CSV_PATH)
+
+    assert predict.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_structured_clf_fit_call_auto_model_fit(fit, tmp_path):
+    auto_model = ak.StructuredDataClassifier(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(
+        x=pd.read_csv(test_utils.TRAIN_CSV_PATH).to_numpy().astype(str)[:100],
+        y=test_utils.generate_one_hot_labels(num_instances=100, num_classes=3),
+    )
+
+    assert fit.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_structured_reg_fit_call_auto_model_fit(fit, tmp_path):
+    auto_model = ak.StructuredDataRegressor(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(
+        x=pd.read_csv(test_utils.TRAIN_CSV_PATH).to_numpy().astype(str)[:100],
+        y=test_utils.generate_data(num_instances=100, shape=(1,)),
+    )
+
+    assert fit.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_structured_data_clf_convert_csv_to_df_and_np(fit, tmp_path):
+    auto_model = ak.StructuredDataClassifier(
+        directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(
+        x=test_utils.TRAIN_CSV_PATH,
+        y="survived",
+        epochs=2,
+        validation_data=(test_utils.TEST_CSV_PATH, "survived"),
+    )
+
+    _, kwargs = fit.call_args_list[0]
+    assert isinstance(kwargs["x"], pd.DataFrame)
+    assert isinstance(kwargs["y"], np.ndarray)
diff --git a/autokeras/tasks/time_series_forecaster.py b/autokeras/tasks/time_series_forecaster.py
new file mode 100644
index 0000000..42e4042
--- /dev/null
+++ b/autokeras/tasks/time_series_forecaster.py
@@ -0,0 +1,517 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from pathlib import Path
+from typing import Dict
+from typing import List
+from typing import Optional
+from typing import Type
+from typing import Union
+
+import pandas as pd
+
+from autokeras import blocks
+from autokeras import nodes as input_module
+from autokeras.engine import tuner
+from autokeras.tasks import structured_data
+from autokeras.tuners import greedy
+from autokeras.utils import types
+
+
+class SupervisedTimeseriesDataPipeline(
+    structured_data.BaseStructuredDataPipeline
+):
+    def __init__(
+        self,
+        outputs,
+        column_names=None,
+        column_types=None,
+        lookback=None,
+        predict_from=1,
+        predict_until=None,
+        **kwargs
+    ):
+        inputs = input_module.TimeseriesInput(
+            lookback=lookback,
+            column_names=column_names,
+            column_types=column_types,
+        )
+        super().__init__(inputs=inputs, outputs=outputs, **kwargs)
+        self.predict_from = predict_from
+        self.predict_until = predict_until
+        self._target_col_name = None
+        self.train_len = 0
+
+    @staticmethod
+    def _read_from_csv(x, y):
+        df = pd.read_csv(x)
+        target = df.pop(y).dropna().to_numpy()
+        return df, target
+
+    def fit(
+        self,
+        x=None,
+        y=None,
+        epochs=None,
+        callbacks=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        # x is file path of training data
+        if isinstance(x, str):
+            self._target_col_name = y
+            x, y = self._read_from_csv(x, y)
+
+        if validation_data:
+            x_val, y_val = validation_data
+            if isinstance(x_val, str):
+                validation_data = self._read_from_csv(x_val, y_val)
+
+        self.check_in_fit(x)
+        self.train_len = len(y)
+
+        if validation_data:
+            x_val, y_val = validation_data
+            train_len = len(y_val)
+            x_val = x_val[:train_len]
+            y_val = y_val[self.lookback - 1 :]
+            validation_data = x_val, y_val
+
+        history = super().fit(
+            x=x[: self.train_len],
+            y=y[self.lookback - 1 :],
+            epochs=epochs,
+            callbacks=callbacks,
+            validation_split=validation_split,
+            validation_data=validation_data,
+            **kwargs
+        )
+        return history
+
+    def predict(self, x, **kwargs):
+        x = self.read_for_predict(x)
+        if len(x) < self.train_len:
+            raise ValueError(
+                "The prediction data requires the original training"
+                " data to make predictions on subsequent data points"
+            )
+        y_pred = super().predict(x=x, **kwargs)
+        lower_bound = self.train_len + self.predict_from
+        if self.predict_until is None:
+            self.predict_until = len(y_pred)
+        upper_bound = min(self.train_len + self.predict_until + 1, len(y_pred))
+        return y_pred[lower_bound:upper_bound]
+
+    def evaluate(self, x, y=None, **kwargs):
+        """Evaluate the best model for the given data.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Testing data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the testing data.
+            y: String, numpy.ndarray, or tensorflow.Dataset. Testing data y.
+                If the data is from a csv file, it should be a string
+                corresponding to the label column.
+            **kwargs: Any arguments supported by keras.Model.evaluate.
+
+        # Returns
+            Scalar test loss (if the model has a single output and no metrics)
+            or list of scalars (if the model has multiple outputs and/or
+            metrics).  The attribute model.metrics_names will give you the
+            display labels for the scalar outputs.
+        """
+        if isinstance(x, str):
+            x, y = self._read_from_csv(x, y)
+        return super().evaluate(
+            x=x[: len(y)], y=y[self.lookback - 1 :], **kwargs
+        )
+
+
+class TimeseriesForecaster(SupervisedTimeseriesDataPipeline):
+    """AutoKeras time series data forecast class.
+
+    # Arguments
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data.  Defaults to None. If None, it will be obtained from the
+            header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column. Defaults to None. If not None, the column_names need to be
+            specified.  If None, it will be inferred from the data.
+        lookback: Int. The range of history steps to consider for each
+            prediction. For example, if lookback=n, the data in the range of [i
+            - n, i - 1] is used to predict the value of step i. If unspecified,
+            it will be tuned automatically.
+        predict_from: Int. The starting point of the forecast for each sample
+            (in number of steps) after the last time step in the input. If N is
+            the last step in the input, then the first step of the predicted
+            output will be N + predict_from. Defaults to 1 (which corresponds to
+            starting the forecast immediately after the last step in the input).
+        predict_until: Int. The end point of the forecast for each sample (in
+            number of steps) after the last time step in the input. If N is the
+            last step in the input, then the last step of the predicted output
+            will be N + predict_until. If unspecified, it will predict till end
+            of dataset. Defaults to None.
+        loss: A Keras loss function. Defaults to use 'mean_squared_error'.
+        metrics: A list of Keras metrics. Defaults to use 'mean_squared_error'.
+        project_name: String. The name of the AutoModel. Defaults to
+            'time_series_forecaster'.
+        max_trials: Int. The maximum number of different Keras Models to try.
+            The search may finish before reaching the max_trials. Defaults to
+            100.
+        directory: String. The path to a directory for storing the search
+            outputs. Defaults to None, which would create a folder with the
+            name of the AutoModel in the current directory.
+        objective: String. Name of model metric to minimize
+            or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.
+        tuner: String or subclass of AutoTuner. If string, it should be one of
+            'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a
+            subclass of AutoTuner. If left unspecified, it uses a task specific
+            tuner, which first evaluates the most commonly used models for the
+            task before exploring other models.
+        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing
+            project of the same name if one is found. Otherwise, overwrites the
+            project.
+        seed: Int. Random seed.
+        max_model_size: Int. Maximum number of scalars in the parameters of a
+            model. Models larger than this are rejected.
+        **kwargs: Any arguments supported by AutoModel.
+    """
+
+    def __init__(
+        self,
+        output_dim: Optional[int] = None,
+        column_names: Optional[List[str]] = None,
+        column_types: Optional[Dict[str, str]] = None,
+        lookback: Optional[int] = None,
+        predict_from: int = 1,
+        predict_until: Optional[int] = None,
+        loss: types.LossType = "mean_squared_error",
+        metrics: Optional[types.MetricsType] = None,
+        project_name: str = "time_series_forecaster",
+        max_trials: int = 100,
+        directory: Union[str, Path, None] = None,
+        objective: str = "val_loss",
+        tuner: Union[str, Type[tuner.AutoTuner]] = None,
+        overwrite: bool = False,
+        seed: Optional[int] = None,
+        max_model_size: Optional[int] = None,
+        **kwargs
+    ):
+        if tuner is None:
+            tuner = greedy.Greedy
+        super().__init__(
+            outputs=blocks.RegressionHead(
+                output_dim=output_dim, loss=loss, metrics=metrics
+            ),
+            column_names=column_names,
+            column_types=column_types,
+            lookback=lookback,
+            predict_from=predict_from,
+            predict_until=predict_until,
+            project_name=project_name,
+            max_trials=max_trials,
+            directory=directory,
+            objective=objective,
+            tuner=tuner,
+            overwrite=overwrite,
+            seed=seed,
+            max_model_size=max_model_size,
+            **kwargs
+        )
+        self.lookback = lookback
+        self.predict_from = predict_from
+        self.predict_until = predict_until
+
+    def fit(
+        self,
+        x=None,
+        y=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        """Search for the best model and hyperparameters for the AutoModel.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Training data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the training data.
+            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or
+                tensorflow.Dataset. Training data y.
+                If the data is from a csv file, it should be a list of string(s)
+                specifying the name(s) of the column(s) need to be forecasted.
+                If it is multivariate forecasting, y should be a list of more
+                than one column names. If it is univariate forecasting, y should
+                be a string or a list of one string.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch.  The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset.  The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data. The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        """
+        super().fit(
+            x=x,
+            y=y,
+            validation_split=validation_split,
+            validation_data=validation_data,
+            **kwargs
+        )
+
+    def predict(self, x=None, **kwargs):
+        """Predict the output for a given testing data.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Testing data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the testing data.
+            **kwargs: Any arguments supported by keras.Model.predict.
+
+        # Returns
+            A list of numpy.ndarray objects or a single numpy.ndarray.
+            The predicted results.
+        """
+        return super().predict(x=x, **kwargs)
+
+    def fit_and_predict(
+        self,
+        x=None,
+        y=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        """Search for the best model and then predict for remaining data points.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Training data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the training data.
+            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or
+                tensorflow.Dataset. Training data y. If the data is from a csv
+                file, it should be a list of string(s) specifying the name(s) of
+                the column(s) need to be forecasted. If it is multivariate
+                forecasting, y should be a list of more than one column names.
+                If it is univariate forecasting, y should be a string or a list
+                of one string.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch. The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset. The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data. The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        """
+        self.fit(
+            x=x,
+            y=y,
+            validation_split=validation_split,
+            validation_data=validation_data,
+            **kwargs
+        )
+
+        return self.predict(x=x)
+
+
+class TimeseriesClassifier(SupervisedTimeseriesDataPipeline):
+    """ "AutoKeras time series data classification class.
+
+    # Arguments
+        column_names: A list of strings specifying the names of the columns. The
+            length of the list should be equal to the number of columns of the
+            data. Defaults to None. If None, it will be obtained from the
+            header of the csv file or the pandas.DataFrame.
+        column_types: Dict. The keys are the column names. The values should
+            either be 'numerical' or 'categorical', indicating the type of that
+            column. Defaults to None. If not None, the column_names need to be
+            specified.  If None, it will be inferred from the data.
+        lookback: Int. The range of history steps to consider for each
+            prediction. For example, if lookback=n, the data in the range of [i
+            - n, i - 1] is used to predict the value of step i. If unspecified,
+            it will be tuned automatically.
+        predict_from: Int. The starting point of the forecast for each sample
+            (in number of steps) after the last time step in the input. If N is
+            the last step in the input, then the first step of the predicted
+            output will be N + predict_from. Defaults to 1 (which corresponds to
+            starting the forecast immediately after the last step in the input).
+        predict_until: Int. The end point of the forecast for each sample (in
+            number of steps) after the last time step in the input. If N is the
+            last step in the input, then the last step of the predicted output
+            will be N + predict_until. If unspecified, it will predict till end
+            of dataset.  Defaults to None.
+        loss: A Keras loss function. Defaults to use 'mean_squared_error'.
+        metrics: A list of Keras metrics. Defaults to use 'mean_squared_error'.
+        project_name: String. The name of the AutoModel. Defaults to
+            'time_series_forecaster'.
+        max_trials: Int. The maximum number of different Keras Models to try.
+            The search may finish before reaching the max_trials. Defaults to
+            100.
+        directory: String. The path to a directory for storing the search
+            outputs. Defaults to None, which would create a folder with the
+            name of the AutoModel in the current directory.
+        objective: String. Name of model metric to minimize
+            or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.
+        overwrite: Boolean. Defaults to `False`. If `False`, reloads an existing
+            project of the same name if one is found. Otherwise, overwrites the
+            project.
+        seed: Int. Random seed.
+        max_model_size: Int. Maximum number of scalars in the parameters of a
+            model. Models larger than this are rejected.
+        **kwargs: Any arguments supported by AutoModel.
+    """
+
+    def __init__(
+        self,
+        output_dim=None,
+        column_names=None,
+        column_types=None,
+        lookback=None,
+        predict_from=1,
+        predict_until=None,
+        loss="mean_squared_error",
+        metrics=None,
+        project_name="time_series_classifier",
+        max_trials=100,
+        directory=None,
+        objective="val_loss",
+        overwrite=False,
+        seed=None,
+        max_model_size: Optional[int] = None,
+        **kwargs
+    ):
+        raise NotImplementedError
+
+    def fit(
+        self,
+        x=None,
+        y=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        """Search for the best model and hyperparameters for the AutoModel.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Training data x. If the data is from a csv file, it should be a
+                string specifying the path of the csv file of the training data.
+            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or
+                tensorflow.Dataset. Training data y. If the data is from a csv
+                file, it should be a list of string(s) specifying the name(s) of
+                the column(s) need to be forecasted. If it is multivariate
+                forecasting, y should be a list of more than one column names.
+                If it is univariate forecasting, y should be a string or a list
+                of one string.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch. The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset. The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data. The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        """
+        raise NotImplementedError
+
+    def predict(self, x=None, **kwargs):
+        """Predict the output for a given testing data.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Testing data x, it should also contain the training data used
+                as, subsequent predictions depend on them. If the data is from a
+                csv file, it should be a string specifying the path of the csv
+                file of the testing data.
+            **kwargs: Any arguments supported by keras.Model.predict.
+
+        # Returns
+            A list of numpy.ndarray objects or a single numpy.ndarray.
+            The predicted results.
+        """
+        raise NotImplementedError
+
+    def fit_and_predict(
+        self,
+        x=None,
+        y=None,
+        validation_split=0.2,
+        validation_data=None,
+        **kwargs
+    ):
+        """Search for the best model and then predict for remaining data points.
+
+        # Arguments
+            x: String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset.
+                Training and Test data x. If the data is from a csv file, it
+                should be a string specifying the path of the csv file of the
+                training data.
+            y: String, a list of string(s), numpy.ndarray, pandas.DataFrame or
+                tensorflow.Dataset. Training data y. If the data is from a csv
+                file, it should be a list of string(s) specifying the name(s) of
+                the column(s) need to be forecasted.  If it is multivariate
+                forecasting, y should be a list of more than one column names.
+                If it is univariate forecasting, y should be a string or a list
+                of one string.
+            validation_split: Float between 0 and 1. Defaults to 0.2. Fraction
+                of the training data to be used as validation data. The model
+                will set apart this fraction of the training data, will not
+                train on it, and will evaluate the loss and any model metrics on
+                this data at the end of each epoch. The validation data is
+                selected from the last samples in the `x` and `y` data provided,
+                before shuffling. This argument is not supported when `x` is a
+                dataset. The best model found would be fit on the entire
+                dataset including the validation data.
+            validation_data: Data on which to evaluate the loss and any model
+                metrics at the end of each epoch. The model will not be trained
+                on this data. `validation_data` will override
+                `validation_split`. The type of the validation data should be
+                the same as the training data. The best model found would be
+                fit on the training dataset without the validation data.
+            **kwargs: Any arguments supported by
+                [keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).
+        """
+        raise NotImplementedError
diff --git a/autokeras/tasks/time_series_forecaster_test.py b/autokeras/tasks/time_series_forecaster_test.py
new file mode 100644
index 0000000..d13699e
--- /dev/null
+++ b/autokeras/tasks/time_series_forecaster_test.py
@@ -0,0 +1,76 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from unittest import mock
+
+import autokeras as ak
+from autokeras import test_utils
+
+
+@mock.patch("autokeras.AutoModel.fit")
+@mock.patch("autokeras.AutoModel.evaluate")
+def test_tsf_evaluate_call_automodel_evaluate(evaluate, fit, tmp_path):
+    auto_model = ak.TimeseriesForecaster(
+        lookback=10, directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+    auto_model.evaluate(x=test_utils.TRAIN_CSV_PATH, y="survived")
+
+    assert evaluate.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+@mock.patch("autokeras.AutoModel.predict")
+def test_tsf_predict_call_automodel_predict(predict, fit, tmp_path):
+    auto_model = ak.TimeseriesForecaster(
+        lookback=10, directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+    auto_model.predict(x=test_utils.TRAIN_CSV_PATH, y="survived")
+
+    assert predict.is_called
+
+
+@mock.patch("autokeras.AutoModel.fit")
+@mock.patch("autokeras.AutoModel.predict")
+def test_tsf_predict_call_automodel_predict_fails(predict, fit, tmp_path):
+    auto_model = ak.TimeseriesForecaster(
+        lookback=10, directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(x=test_utils.TRAIN_CSV_PATH, y="survived")
+    # Predict data doesn't contain train time steps
+    try:
+        auto_model.predict(x=test_utils.TEST_CSV_PATH, y="survived")
+    except ValueError as e:
+        assert fit.is_called
+        assert "The prediction data requires the original training data to make"
+        " predictions on subsequent data points" in str(e)
+
+
+@mock.patch("autokeras.AutoModel.fit")
+def test_tsf_fit_call_automodel_fit(fit, tmp_path):
+    auto_model = ak.TimeseriesForecaster(
+        lookback=10, directory=tmp_path, seed=test_utils.SEED
+    )
+
+    auto_model.fit(
+        x=test_utils.TRAIN_CSV_PATH,
+        y="survived",
+        validation_data=(test_utils.TRAIN_CSV_PATH, "survived"),
+    )
+
+    assert fit.is_called
diff --git a/autokeras/test_utils.py b/autokeras/test_utils.py
index 951b203..1cf4eeb 100644
--- a/autokeras/test_utils.py
+++ b/autokeras/test_utils.py
@@ -13,14 +13,46 @@
 # limitations under the License.
 
 import inspect
+import os
 
-import keras
 import numpy as np
 import tensorflow as tf
+from tensorflow import keras
 
 import autokeras as ak
 
 SEED = 5
+COLUMN_NAMES = [
+    "sex",
+    "age",
+    "n_siblings_spouses",
+    "parch",
+    "fare",
+    "class",
+    "deck",
+    "embark_town",
+    "alone",
+]
+COLUMN_TYPES = {
+    "sex": "categorical",
+    "age": "numerical",
+    "n_siblings_spouses": "categorical",
+    "parch": "categorical",
+    "fare": "numerical",
+    "class": "categorical",
+    "deck": "categorical",
+    "embark_town": "categorical",
+    "alone": "categorical",
+}
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+TRAIN_CSV_PATH = keras.utils.get_file(
+    fname=os.path.basename(TRAIN_DATA_URL), origin=TRAIN_DATA_URL
+)
+TEST_CSV_PATH = keras.utils.get_file(
+    fname=os.path.basename(TEST_DATA_URL), origin=TEST_DATA_URL
+)
 
 
 def generate_data(num_instances=100, shape=(32, 32, 3), dtype="np"):
@@ -67,6 +99,26 @@ def generate_text_data(num_instances=100):
     )
 
 
+def generate_data_with_categorical(
+    num_instances=100,
+    num_numerical=10,
+    num_categorical=3,
+    num_classes=5,
+    dtype="np",
+):
+    categorical_data = np.random.randint(
+        num_classes, size=(num_instances, num_categorical)
+    )
+    numerical_data = np.random.rand(num_instances, num_numerical)
+    data = np.concatenate((numerical_data, categorical_data), axis=1)
+    if data.dtype == np.float64:
+        data = data.astype(np.float32)
+    if dtype == "np":
+        return data
+    if dtype == "dataset":
+        return tf.data.Dataset.from_tensor_slices(data)
+
+
 def build_graph():
     keras.backend.clear_session()
     image_input = ak.ImageInput(shape=(32, 32, 3))
diff --git a/autokeras/tuners/greedy_test.py b/autokeras/tuners/greedy_test.py
index ba47e4c..947425c 100644
--- a/autokeras/tuners/greedy_test.py
+++ b/autokeras/tuners/greedy_test.py
@@ -14,8 +14,8 @@
 
 from unittest import mock
 
-import keras
 import keras_tuner
+from tensorflow import keras
 
 import autokeras as ak
 from autokeras import test_utils
diff --git a/autokeras/tuners/task_specific.py b/autokeras/tuners/task_specific.py
index 5e2a17c..6d5de89 100644
--- a/autokeras/tuners/task_specific.py
+++ b/autokeras/tuners/task_specific.py
@@ -73,13 +73,84 @@ IMAGE_CLASSIFIER = [
 
 TEXT_CLASSIFIER = [
     {
+        "text_block_1/block_type": "vanilla",
+        "classification_head_1/dropout": 0,
+        "text_block_1/max_tokens": 5000,
+        "text_block_1/conv_block_1/separable": False,
+        "text_block_1/text_to_int_sequence_1/output_sequence_length": 512,
+        "text_block_1/embedding_1/pretraining": "none",
+        "text_block_1/embedding_1/embedding_dim": 64,
+        "text_block_1/embedding_1/dropout": 0.25,
+        "text_block_1/conv_block_1/kernel_size": 5,
+        "text_block_1/conv_block_1/num_blocks": 1,
+        "text_block_1/conv_block_1/num_layers": 1,
+        "text_block_1/conv_block_1/max_pooling": False,
+        "text_block_1/conv_block_1/dropout": 0,
+        "text_block_1/conv_block_1/filters_0_0": 256,
+        "text_block_1/spatial_reduction_1/reduction_type": "global_max",
+        "text_block_1/dense_block_1/num_layers": 1,
+        "text_block_1/dense_block_1/use_batchnorm": False,
+        "text_block_1/dense_block_1/dropout": 0.5,
+        "text_block_1/dense_block_1/units_0": 256,
+        "optimizer": "adam",
+        "learning_rate": 1e-3,
+    },
+    {
+        "text_block_1/block_type": "transformer",
+        "classification_head_1/dropout": 0,
+        "optimizer": "adam",
+        "learning_rate": 1e-3,
+        "text_block_1/max_tokens": 20000,
+        "text_block_1/text_to_int_sequence_1/output_sequence_length": 200,
+        "text_block_1/transformer_1/pretraining": "none",
+        "text_block_1/transformer_1/embedding_dim": 32,
+        "text_block_1/transformer_1/num_heads": 2,
+        "text_block_1/transformer_1/dense_dim": 32,
+        "text_block_1/transformer_1/dropout": 0.25,
+        "text_block_1/spatial_reduction_1/reduction_type": "global_avg",
+        "text_block_1/dense_block_1/num_layers": 1,
+        "text_block_1/dense_block_1/use_batchnorm": False,
+        "text_block_1/dense_block_1/dropout": 0.5,
+        "text_block_1/dense_block_1/units_0": 20,
+    },
+    {
+        "text_block_1/block_type": "bert",
         "classification_head_1/dropout": 0,
         "optimizer": "adam_weight_decay",
         "learning_rate": 2e-5,
         "text_block_1/bert_block_1/max_sequence_length": 512,
+        "text_block_1/max_tokens": 20000,
     },
 ]
 
+STRUCTURED_DATA_CLASSIFIER = [
+    {
+        "structured_data_block_1/normalize": True,
+        "structured_data_block_1/dense_block_1/num_layers": 2,
+        "structured_data_block_1/dense_block_1/use_batchnorm": False,
+        "structured_data_block_1/dense_block_1/dropout": 0,
+        "structured_data_block_1/dense_block_1/units_0": 32,
+        "structured_data_block_1/dense_block_1/units_1": 32,
+        "classification_head_1/dropout": 0.0,
+        "optimizer": "adam",
+        "learning_rate": 0.001,
+    }
+]
+
+STRUCTURED_DATA_REGRESSOR = [
+    {
+        "structured_data_block_1/normalize": True,
+        "structured_data_block_1/dense_block_1/num_layers": 2,
+        "structured_data_block_1/dense_block_1/use_batchnorm": False,
+        "structured_data_block_1/dense_block_1/dropout": 0,
+        "structured_data_block_1/dense_block_1/units_0": 32,
+        "structured_data_block_1/dense_block_1/units_1": 32,
+        "regression_head_1/dropout": 0.0,
+        "optimizer": "adam",
+        "learning_rate": 0.001,
+    }
+]
+
 
 class ImageClassifierTuner(greedy.Greedy):
     def __init__(self, **kwargs):
@@ -89,3 +160,13 @@ class ImageClassifierTuner(greedy.Greedy):
 class TextClassifierTuner(greedy.Greedy):
     def __init__(self, **kwargs):
         super().__init__(initial_hps=TEXT_CLASSIFIER, **kwargs)
+
+
+class StructuredDataClassifierTuner(greedy.Greedy):
+    def __init__(self, **kwargs):
+        super().__init__(initial_hps=STRUCTURED_DATA_CLASSIFIER, **kwargs)
+
+
+class StructuredDataRegressorTuner(greedy.Greedy):
+    def __init__(self, **kwargs):
+        super().__init__(initial_hps=STRUCTURED_DATA_REGRESSOR, **kwargs)
diff --git a/autokeras/tuners/task_specific_test.py b/autokeras/tuners/task_specific_test.py
index f23b1cd..ffafee4 100644
--- a/autokeras/tuners/task_specific_test.py
+++ b/autokeras/tuners/task_specific_test.py
@@ -59,7 +59,7 @@ def test_img_clf_init_hp2_equals_hp_of_a_model(tmp_path):
     assert set(init_hp.keys()) == set(hp._hps.keys())
 
 
-def test_txt_clf_init_hp0_equals_hp_of_a_model(tmp_path):
+def test_txt_clf_init_hp2_equals_hp_of_a_model(tmp_path):
     clf = ak.TextClassifier(directory=tmp_path)
     clf.inputs[0].shape = (1,)
     clf.inputs[0].batch_size = 6
@@ -67,10 +67,69 @@ def test_txt_clf_init_hp0_equals_hp_of_a_model(tmp_path):
     clf.outputs[0].in_blocks[0].shape = (10,)
     clf.tuner.hypermodel.epochs = 1000
     clf.tuner.hypermodel.num_samples = 20000
+    init_hp = task_specific.TEXT_CLASSIFIER[2]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
+    clf.tuner.hypermodel.build(hp)
+
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_txt_clf_init_hp1_equals_hp_of_a_model(tmp_path):
+    clf = ak.TextClassifier(directory=tmp_path)
+    clf.inputs[0].shape = (1,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
+    init_hp = task_specific.TEXT_CLASSIFIER[1]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
+    clf.tuner.hypermodel.build(hp)
+
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_txt_clf_init_hp0_equals_hp_of_a_model(tmp_path):
+    clf = ak.TextClassifier(directory=tmp_path)
+    clf.inputs[0].shape = (1,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
     init_hp = task_specific.TEXT_CLASSIFIER[0]
     hp = keras_tuner.HyperParameters()
     hp.values = copy.copy(init_hp)
 
+    clf.tuner.hypermodel.build(hp)
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_sd_clf_init_hp0_equals_hp_of_a_model(tmp_path):
+    clf = ak.StructuredDataClassifier(
+        directory=tmp_path,
+        column_names=["a", "b"],
+        column_types={"a": "numerical", "b": "numerical"},
+    )
+    clf.inputs[0].shape = (2,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
+    init_hp = task_specific.STRUCTURED_DATA_CLASSIFIER[0]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
+    clf.tuner.hypermodel.build(hp)
+
+    assert set(init_hp.keys()) == set(hp._hps.keys())
+
+
+def test_sd_reg_init_hp0_equals_hp_of_a_model(tmp_path):
+    clf = ak.StructuredDataRegressor(
+        directory=tmp_path,
+        column_names=["a", "b"],
+        column_types={"a": "numerical", "b": "numerical"},
+    )
+    clf.inputs[0].shape = (2,)
+    clf.outputs[0].in_blocks[0].shape = (10,)
+    init_hp = task_specific.STRUCTURED_DATA_REGRESSOR[0]
+    hp = keras_tuner.HyperParameters()
+    hp.values = copy.copy(init_hp)
+
     clf.tuner.hypermodel.build(hp)
 
     assert set(init_hp.keys()) == set(hp._hps.keys())
diff --git a/autokeras/utils/data_utils.py b/autokeras/utils/data_utils.py
index 06a0630..94c25c6 100644
--- a/autokeras/utils/data_utils.py
+++ b/autokeras/utils/data_utils.py
@@ -12,15 +12,13 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import keras
 import numpy as np
 import tensorflow as tf
-import tree
-from keras import ops
+from tensorflow import nest
 
 
 def batched(dataset):
-    shape = tree.flatten(dataset_shape(dataset))[0]
+    shape = nest.flatten(dataset_shape(dataset))[0]
     return len(shape) > 0 and shape[0] is None
 
 
@@ -62,23 +60,23 @@ def dataset_shape(dataset):
 
 
 def unzip_dataset(dataset):
-    return tree.flatten(
+    return nest.flatten(
         [
-            dataset.map(lambda *a: tree.flatten(a)[index])
-            for index in range(len(tree.flatten(dataset_shape(dataset))))
+            dataset.map(lambda *a: nest.flatten(a)[index])
+            for index in range(len(nest.flatten(dataset_shape(dataset))))
         ]
     )
 
 
 def cast_to_string(tensor):
-    if keras.backend.standardize_dtype(tensor.dtype) == "string":
-        return tensor  # pragma: no cover
+    if tensor.dtype == tf.string:
+        return tensor
     return tf.strings.as_string(tensor)
 
 
 def cast_to_float32(tensor):
-    if keras.backend.standardize_dtype(tensor.dtype) == "float32":
+    if tensor.dtype == tf.float32:
         return tensor
-    if keras.backend.standardize_dtype(tensor.dtype) == "string":
+    if tensor.dtype == tf.string:
         return tf.strings.to_number(tensor, tf.float32)
-    return ops.cast(tensor, "float32")  # pragma: no cover
+    return tf.cast(tensor, tf.float32)
diff --git a/autokeras/utils/io_utils_test.py b/autokeras/utils/io_utils_test.py
index 111ff3c..453908f 100644
--- a/autokeras/utils/io_utils_test.py
+++ b/autokeras/utils/io_utils_test.py
@@ -15,9 +15,9 @@
 import os
 import shutil
 
-import keras
 import pytest
 import tensorflow as tf
+from tensorflow import keras
 
 from autokeras import test_utils
 from autokeras.utils import io_utils
diff --git a/autokeras/utils/layer_utils.py b/autokeras/utils/layer_utils.py
index 8245bf4..8da71f3 100644
--- a/autokeras/utils/layer_utils.py
+++ b/autokeras/utils/layer_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras import layers
+from tensorflow.keras import layers
 
 
 def get_global_average_pooling(shape):
@@ -44,7 +44,7 @@ def get_conv(shape):
 
 
 def get_sep_conv(shape):
-    return [  # pragma: no cover
+    return [
         layers.SeparableConv1D,
         layers.SeparableConv2D,
         layers.Conv3D,
diff --git a/autokeras/utils/types.py b/autokeras/utils/types.py
index b3bc674..b256e2f 100644
--- a/autokeras/utils/types.py
+++ b/autokeras/utils/types.py
@@ -19,8 +19,8 @@ from typing import Union
 
 import numpy as np
 import tensorflow as tf
-from keras.losses import Loss
-from keras.metrics import Metric
+from tensorflow.keras.losses import Loss
+from tensorflow.keras.metrics import Metric
 
 DatasetType = Union[np.ndarray, tf.data.Dataset]
 LossType = Union[str, Callable, Loss]
diff --git a/autokeras/utils/utils.py b/autokeras/utils/utils.py
index 26c2b25..bb85a63 100644
--- a/autokeras/utils/utils.py
+++ b/autokeras/utils/utils.py
@@ -13,15 +13,16 @@
 # limitations under the License.
 
 import re
+import warnings
 
-import keras
 import keras_tuner
 import tensorflow as tf
-import tree
+from packaging.version import parse
+from tensorflow import nest
 
 
 def validate_num_inputs(inputs, num):
-    inputs = tree.flatten(inputs)
+    inputs = nest.flatten(inputs)
     if not len(inputs) == num:
         raise ValueError(
             "Expected {num} elements in the inputs list "
@@ -35,6 +36,32 @@ def to_snake_case(name):
     return insecure
 
 
+def check_tf_version() -> None:
+    if parse(tf.__version__) < parse("2.7.0"):
+        warnings.warn(
+            "The Tensorflow package version needs to be at least 2.7.0 \n"
+            "for AutoKeras to run. Currently, your TensorFlow version is \n"
+            f"{tf.__version__}. Please upgrade with \n"
+            "`$ pip install --upgrade tensorflow`. \n"
+            "You can use `pip freeze` to check afterwards "
+            "that everything is ok.",
+            ImportWarning,
+        )
+
+
+def check_kt_version() -> None:
+    if parse(keras_tuner.__version__) < parse("1.1.0"):
+        warnings.warn(
+            "The Keras Tuner package version needs to be at least 1.1.0 \n"
+            "for AutoKeras to run. Currently, your Keras Tuner version is \n"
+            f"{keras_tuner.__version__}. Please upgrade with \n"
+            "`$ pip install --upgrade keras-tuner`. \n"
+            "You can use `pip freeze` to check afterwards "
+            "that everything is ok.",
+            ImportWarning,
+        )
+
+
 def contain_instance(instance_list, instance_type):
     return any(
         [isinstance(instance, instance_type) for instance in instance_list]
@@ -122,10 +149,24 @@ def add_to_hp(hp, hps, name=None):
 
 
 def serialize_keras_object(obj):
-    return keras.utils.serialize_keras_object(obj)  # pragma: no cover
+    if hasattr(tf.keras.utils, "legacy"):
+        return tf.keras.utils.legacy.serialize_keras_object(
+            obj
+        )  # pragma: no cover
+    else:
+        return tf.keras.utils.serialize_keras_object(obj)  # pragma: no cover
 
 
-def deserialize_keras_object(config, module_objects=None, custom_objects=None):
-    return keras.utils.deserialize_keras_object(
-        config, custom_objects, module_objects
-    )
+def deserialize_keras_object(
+    config, module_objects=None, custom_objects=None, printable_module_name=None
+):
+    if hasattr(tf.keras.utils, "legacy"):
+        return (
+            tf.keras.utils.legacy.deserialize_keras_object(  # pragma: no cover
+                config, custom_objects, module_objects, printable_module_name
+            )
+        )
+    else:
+        return tf.keras.utils.deserialize_keras_object(  # pragma: no cover
+            config, custom_objects, module_objects, printable_module_name
+        )
diff --git a/autokeras/utils/utils_test.py b/autokeras/utils/utils_test.py
index 668a102..c589547 100644
--- a/autokeras/utils/utils_test.py
+++ b/autokeras/utils/utils_test.py
@@ -27,6 +27,32 @@ def test_validate_num_inputs_error():
     assert "Expected 2 elements in the inputs list" in str(info.value)
 
 
+def test_check_tf_version_error():
+    utils.tf.__version__ = "2.1.0"
+
+    with pytest.warns(ImportWarning) as record:
+        utils.check_tf_version()
+
+    assert len(record) == 1
+    assert (
+        "Tensorflow package version needs to be at least"
+        in record[0].message.args[0]
+    )
+
+
+def test_check_kt_version_error():
+    utils.keras_tuner.__version__ = "1.0.0"
+
+    with pytest.warns(ImportWarning) as record:
+        utils.check_kt_version()
+
+    assert len(record) == 1
+    assert (
+        "Keras Tuner package version needs to be at least"
+        in record[0].message.args[0]
+    )
+
+
 def test_run_with_adaptive_batch_size_raise_error():
     def func(**kwargs):
         raise tf.errors.ResourceExhaustedError(0, "", None)
diff --git a/benchmark/experiments/image.py b/benchmark/experiments/image.py
index 8f2f377..67a32b4 100644
--- a/benchmark/experiments/image.py
+++ b/benchmark/experiments/image.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from keras import datasets
+from tensorflow.keras import datasets
 
 import autokeras as ak
 from benchmark.experiments import experiment
diff --git a/benchmark/experiments/structured_data.py b/benchmark/experiments/structured_data.py
new file mode 100644
index 0000000..27a3675
--- /dev/null
+++ b/benchmark/experiments/structured_data.py
@@ -0,0 +1,116 @@
+# Copyright 2020 The AutoKeras Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import numpy as np
+import pandas as pd
+import sklearn
+import tensorflow as tf
+
+import autokeras as ak
+from benchmark.experiments import experiment
+
+
+class StructuredDataClassifierExperiment(experiment.Experiment):
+    def get_auto_model(self):
+        return ak.StructuredDataClassifier(
+            max_trials=10, directory=self.tmp_dir, overwrite=True
+        )
+
+
+class Titanic(StructuredDataClassifierExperiment):
+    def __init__(self):
+        super().__init__(name="Titanic")
+
+    @staticmethod
+    def load_data():
+        TRAIN_DATA_URL = (
+            "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+        )
+        TEST_DATA_URL = (
+            "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+        )
+        x_train = tf.keras.utils.get_file("titanic_train.csv", TRAIN_DATA_URL)
+        x_test = tf.keras.utils.get_file("titanic_eval.csv", TEST_DATA_URL)
+
+        return (x_train, "survived"), (x_test, "survived")
+
+
+class Iris(StructuredDataClassifierExperiment):
+    def __init__(self):
+        super().__init__(name="Iris")
+
+    @staticmethod
+    def load_data():
+        # Prepare the dataset.
+        TRAIN_DATA_URL = (
+            "https://storage.googleapis.com/"
+            "download.tensorflow.org/data/iris_training.csv"
+        )
+        x_train = tf.keras.utils.get_file("iris_train.csv", TRAIN_DATA_URL)
+
+        TEST_DATA_URL = (
+            "https://storage.googleapis.com/"
+            "download.tensorflow.org/data/iris_test.csv"
+        )
+        x_test = tf.keras.utils.get_file("iris_test.csv", TEST_DATA_URL)
+
+        return (x_train, "virginica"), (x_test, "virginica")
+
+
+class Wine(StructuredDataClassifierExperiment):
+    def __init__(self):
+        super().__init__(name="Wine")
+
+    @staticmethod
+    def load_data():
+        DATASET_URL = (
+            "https://archive.ics.uci.edu/ml/"
+            "machine-learning-databases/wine/wine.data"
+        )
+
+        # save data
+        dataset = tf.keras.utils.get_file("wine.csv", DATASET_URL)
+
+        data = pd.read_csv(dataset, header=None).sample(frac=1, random_state=5)
+        split_length = int(data.shape[0] * 0.8)  # 141
+
+        return (data.iloc[:split_length, 1:], data.iloc[:split_length, 0]), (
+            data.iloc[split_length:, 1:],
+            data.iloc[split_length:, 0],
+        )
+
+
+class StructuredDataRegressorExperiment(experiment.Experiment):
+    def get_auto_model(self):
+        return ak.StructuredDataRegressor(
+            max_trials=10, directory=self.tmp_dir, overwrite=True
+        )
+
+
+class CaliforniaHousing(StructuredDataRegressorExperiment):
+    @staticmethod
+    def load_data():
+        house_dataset = sklearn.datasets.fetch_california_housing()
+        (
+            x_train,
+            x_test,
+            y_train,
+            y_test,
+        ) = sklearn.model_selection.train_test_split(
+            house_dataset.data,
+            np.array(house_dataset.target),
+            test_size=0.2,
+            random_state=42,
+        )
+        return (x_train, y_train), (x_test, y_test)
diff --git a/benchmark/experiments/text.py b/benchmark/experiments/text.py
index 7a510eb..14e6ef5 100644
--- a/benchmark/experiments/text.py
+++ b/benchmark/experiments/text.py
@@ -14,8 +14,8 @@
 
 import os
 
-import keras
 import numpy as np
+import tensorflow as tf
 from sklearn.datasets import load_files
 
 import autokeras as ak
@@ -33,7 +33,7 @@ class IMDB(experiment.Experiment):
 
     @staticmethod
     def load_data():
-        dataset = keras.utils.get_file(
+        dataset = tf.keras.utils.get_file(
             fname="aclImdb.tar.gz",
             origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",  # noqa: E501
             extract=True,
diff --git a/benchmark/performance.py b/benchmark/performance.py
index 77ad170..678b805 100644
--- a/benchmark/performance.py
+++ b/benchmark/performance.py
@@ -13,17 +13,17 @@
 # limitations under the License.
 import os
 
-import keras
 import numpy as np
-from keras.datasets import cifar10
-from keras.datasets import mnist
+import tensorflow as tf
 from sklearn.datasets import load_files
+from tensorflow.keras.datasets import cifar10
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
 
 def imdb_raw(num_instances=100):
-    dataset = keras.utils.get_file(
+    dataset = tf.keras.utils.get_file(
         fname="aclImdb.tar.gz",
         origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
         extract=True,
@@ -75,3 +75,21 @@ def test_imdb_accuracy_over_92(tmp_path):
     clf.fit(x_train, y_train, batch_size=6, epochs=1)
     accuracy = clf.evaluate(x_test, y_test)[1]
     assert accuracy >= 0.92
+
+
+def test_titaninc_accuracy_over_77(tmp_path):
+    TRAIN_DATA_URL = (
+        "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+    )
+    TEST_DATA_URL = (
+        "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+    )
+
+    train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+    test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+    clf = ak.StructuredDataClassifier(max_trials=10, directory=tmp_path)
+
+    clf.fit(train_file_path, "survived")
+
+    accuracy = clf.evaluate(test_file_path, "survived")[1]
+    assert accuracy >= 0.77
diff --git a/docs/autogen.py b/docs/autogen.py
index b97c9cf..16a02a6 100644
--- a/docs/autogen.py
+++ b/docs/autogen.py
@@ -34,6 +34,20 @@ PAGES = {
         "autokeras.TextRegressor.evaluate",
         "autokeras.TextRegressor.export_model",
     ],
+    "structured_data_classifier.md": [
+        "autokeras.StructuredDataClassifier",
+        "autokeras.StructuredDataClassifier.fit",
+        "autokeras.StructuredDataClassifier.predict",
+        "autokeras.StructuredDataClassifier.evaluate",
+        "autokeras.StructuredDataClassifier.export_model",
+    ],
+    "structured_data_regressor.md": [
+        "autokeras.StructuredDataRegressor",
+        "autokeras.StructuredDataRegressor.fit",
+        "autokeras.StructuredDataRegressor.predict",
+        "autokeras.StructuredDataRegressor.evaluate",
+        "autokeras.StructuredDataRegressor.export_model",
+    ],
     "auto_model.md": [
         "autokeras.AutoModel",
         "autokeras.AutoModel.fit",
@@ -50,11 +64,13 @@ PAGES = {
     "node.md": [
         "autokeras.ImageInput",
         "autokeras.Input",
+        "autokeras.StructuredDataInput",
         "autokeras.TextInput",
     ],
     "block.md": [
         "autokeras.ConvBlock",
         "autokeras.DenseBlock",
+        "autokeras.Embedding",
         "autokeras.Merge",
         "autokeras.ResNetBlock",
         "autokeras.RNNBlock",
@@ -62,9 +78,13 @@ PAGES = {
         "autokeras.TemporalReduction",
         "autokeras.XceptionBlock",
         "autokeras.ImageBlock",
+        "autokeras.StructuredDataBlock",
         "autokeras.TextBlock",
         "autokeras.ImageAugmentation",
         "autokeras.Normalization",
+        "autokeras.TextToIntSequence",
+        "autokeras.TextToNgramVector",
+        "autokeras.CategoricalToNumerical",
         "autokeras.ClassificationHead",
         "autokeras.RegressionHead",
     ],
diff --git a/docs/ipynb/customized.ipynb b/docs/ipynb/customized.ipynb
index da250aa..c323f2e 100644
--- a/docs/ipynb/customized.ipynb
+++ b/docs/ipynb/customized.ipynb
@@ -19,10 +19,9 @@
    },
    "outputs": [],
    "source": [
-    "import keras\n",
     "import numpy as np\n",
-    "import tree\n",
-    "from keras.datasets import mnist\n",
+    "import tensorflow as tf\n",
+    "from tensorflow.keras.datasets import mnist\n",
     "\n",
     "import autokeras as ak"
    ]
@@ -135,19 +134,24 @@
     "the validation data, please refer to the Validation Data section of the\n",
     "tutorials of [Image\n",
     "Classification](/tutorial/image_classification/#validation-data), [Text\n",
-    "Classification](/tutorial/text_classification/#validation-data),\n",
+    "Classification](/tutorial/text_classification/#validation-data), [Structured\n",
+    "Data\n",
+    "Classification](/tutorial/structured_data_classification/#validation-data),\n",
     "[Multi-task and Multiple Validation](/tutorial/multi/#validation-data).\n",
     "\n",
     "## Data Format\n",
     "You can refer to the documentation of\n",
     "[ImageInput](/node/#imageinput-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[RegressionHead](/block/#regressionhead-class),\n",
     "[ClassificationHead](/block/#classificationhead-class),\n",
     "for the format of different types of data.\n",
     "You can also refer to the Data Format section of the tutorials of\n",
     "[Image Classification](/tutorial/image_classification/#data-format),\n",
-    "[Text Classification](/tutorial/text_classification/#data-format).\n",
+    "[Text Classification](/tutorial/text_classification/#data-format),\n",
+    "[Structured Data\n",
+    "Classification](/tutorial/structured_data_classification/#data-format).\n",
     "\n",
     "## Implement New Block\n",
     "\n",
@@ -173,8 +177,8 @@
     "class SingleDenseLayerBlock(ak.Block):\n",
     "    def build(self, hp, inputs=None):\n",
     "        # Get the input_node from inputs.\n",
-    "        input_node = tree.flatten(inputs)[0]\n",
-    "        layer = keras.layers.Dense(\n",
+    "        input_node = tf.nest.flatten(inputs)[0]\n",
+    "        layer = tf.keras.layers.Dense(\n",
     "            hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
     "        )\n",
     "        output_node = layer(input_node)\n",
@@ -228,6 +232,7 @@
     "**Nodes**:\n",
     "[ImageInput](/node/#imageinput-class),\n",
     "[Input](/node/#input-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[TextInput](/node/#textinput-class).\n",
     "\n",
     "**Preprocessors**:\n",
@@ -235,10 +240,13 @@
     "[ImageAugmentation](/block/#imageaugmentation-class),\n",
     "[LightGBM](/block/#lightgbm-class),\n",
     "[Normalization](/block/#normalization-class),\n",
+    "[TextToIntSequence](/block/#texttointsequence-class),\n",
+    "[TextToNgramVector](/block/#texttongramvector-class).\n",
     "\n",
     "**Blocks**:\n",
     "[ConvBlock](/block/#convblock-class),\n",
     "[DenseBlock](/block/#denseblock-class),\n",
+    "[Embedding](/block/#embedding-class),\n",
     "[Merge](/block/#merge-class),\n",
     "[ResNetBlock](/block/#resnetblock-class),\n",
     "[RNNBlock](/block/#rnnblock-class),\n",
@@ -246,6 +254,7 @@
     "[TemporalReduction](/block/#temporalreduction-class),\n",
     "[XceptionBlock](/block/#xceptionblock-class),\n",
     "[ImageBlock](/block/#imageblock-class),\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class),\n",
     "[TextBlock](/block/#textblock-class).\n"
    ]
   }
diff --git a/docs/ipynb/export.ipynb b/docs/ipynb/export.ipynb
index 5a31d1a..34b7199 100644
--- a/docs/ipynb/export.ipynb
+++ b/docs/ipynb/export.ipynb
@@ -19,9 +19,9 @@
    },
    "outputs": [],
    "source": [
-    "import numpy as np\n",
-    "from keras.datasets import mnist\n",
-    "from keras.models import load_model\n",
+    "import tensorflow as tf\n",
+    "from tensorflow.keras.datasets import mnist\n",
+    "from tensorflow.keras.models import load_model\n",
     "\n",
     "import autokeras as ak"
    ]
@@ -32,8 +32,7 @@
     "colab_type": "text"
    },
    "source": [
-    "You can easily export your model the best model found by AutoKeras as a Keras\n",
-    "Model.\n",
+    "You can easily export your model the best model found by AutoKeras as a Keras Model.\n",
     "\n",
     "The following example uses [ImageClassifier](/image_classifier) as an example.\n",
     "All the tasks and the [AutoModel](/auto_model/#automodel-class) has this\n",
@@ -48,6 +47,7 @@
    },
    "outputs": [],
    "source": [
+    "print(tf.__version__)\n",
     "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
     "\n",
     "# Initialize the image classifier.\n",
@@ -69,7 +69,7 @@
     "\n",
     "loaded_model = load_model(\"model_autokeras\", custom_objects=ak.CUSTOM_OBJECTS)\n",
     "\n",
-    "predicted_y = loaded_model.predict(np.expand_dims(x_test, -1))\n",
+    "predicted_y = loaded_model.predict(tf.expand_dims(x_test, -1))\n",
     "print(predicted_y)"
    ]
   }
diff --git a/docs/ipynb/image_classification.ipynb b/docs/ipynb/image_classification.ipynb
index 190373e..c48dbdb 100644
--- a/docs/ipynb/image_classification.ipynb
+++ b/docs/ipynb/image_classification.ipynb
@@ -21,7 +21,7 @@
    "source": [
     "import numpy as np\n",
     "import tensorflow as tf\n",
-    "from keras.datasets import mnist\n",
+    "from tensorflow.keras.datasets import mnist\n",
     "\n",
     "import autokeras as ak"
    ]
@@ -33,8 +33,7 @@
    },
    "source": [
     "## A Simple Example\n",
-    "The first step is to prepare your data. Here we use the MNIST dataset as an\n",
-    "example\n"
+    "The first step is to prepare your data. Here we use the MNIST dataset as an example\n"
    ]
   },
   {
@@ -194,13 +193,13 @@
     "colab_type": "text"
    },
    "source": [
-    "The usage of AutoModel is similar to the functional API of Keras. Basically, you\n",
-    "are building a graph, whose edges are blocks and the nodes are intermediate\n",
-    "outputs of blocks. To add an edge from input_node to output_node with\n",
-    "output_node = ak.[some_block]([block_args])(input_node).\n",
+    "The usage of AutoModel is similar to the functional API of Keras. Basically, you are\n",
+    "building a graph, whose edges are blocks and the nodes are intermediate outputs of\n",
+    "blocks. To add an edge from input_node to output_node with output_node =\n",
+    "ak.[some_block]([block_args])(input_node).\n",
     "\n",
-    "You can even also use more fine grained blocks to customize the search space\n",
-    "even further. See the following example.\n"
+    "You can even also use more fine grained blocks to customize the search space even\n",
+    "further. See the following example.\n"
    ]
   },
   {
diff --git a/docs/ipynb/image_regression.ipynb b/docs/ipynb/image_regression.ipynb
index 7a2cfe1..6288f7e 100644
--- a/docs/ipynb/image_regression.ipynb
+++ b/docs/ipynb/image_regression.ipynb
@@ -20,7 +20,7 @@
    "outputs": [],
    "source": [
     "import tensorflow as tf\n",
-    "from keras.datasets import mnist\n",
+    "from tensorflow.keras.datasets import mnist\n",
     "\n",
     "import autokeras as ak"
    ]
diff --git a/docs/ipynb/load.ipynb b/docs/ipynb/load.ipynb
index 1294927..32bdc67 100644
--- a/docs/ipynb/load.ipynb
+++ b/docs/ipynb/load.ipynb
@@ -22,7 +22,6 @@
     "import os\n",
     "import shutil\n",
     "\n",
-    "import keras\n",
     "import numpy as np\n",
     "import tensorflow as tf\n",
     "\n",
@@ -55,7 +54,7 @@
    "outputs": [],
    "source": [
     "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"  # noqa: E501\n",
-    "local_file_path = keras.utils.get_file(\n",
+    "local_file_path = tf.keras.utils.get_file(\n",
     "    origin=dataset_url, fname=\"image_data\", extract=True\n",
     ")\n",
     "# The file is extracted in the same directory as the downloaded file.\n",
@@ -161,7 +160,7 @@
    "source": [
     "dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
     "\n",
-    "local_file_path = keras.utils.get_file(\n",
+    "local_file_path = tf.keras.utils.get_file(\n",
     "    fname=\"text_data\",\n",
     "    origin=dataset_url,\n",
     "    extract=True,\n",
@@ -227,27 +226,31 @@
    "source": [
     "N_BATCHES = 30\n",
     "BATCH_SIZE = 100\n",
+    "N_FEATURES = 10\n",
     "\n",
     "\n",
-    "def get_data_generator(n_batches, batch_size):\n",
-    "    \"\"\"Get a generator returning n_batches random data.\"\"\"\n",
+    "def get_data_generator(n_batches, batch_size, n_features):\n",
+    "    \"\"\"Get a generator returning n_batches random data.\n",
+    "\n",
+    "    The shape of the data is (batch_size, n_features).\n",
+    "    \"\"\"\n",
     "\n",
     "    def data_generator():\n",
     "        for _ in range(n_batches * batch_size):\n",
-    "            x = np.random.randn(32, 32, 3)\n",
-    "            y = x.sum() / 32 * 32 * 3 > 0.5\n",
+    "            x = np.random.randn(n_features)\n",
+    "            y = x.sum(axis=0) / n_features > 0.5\n",
     "            yield x, y\n",
     "\n",
     "    return data_generator\n",
     "\n",
     "\n",
     "dataset = tf.data.Dataset.from_generator(\n",
-    "    get_data_generator(N_BATCHES, BATCH_SIZE),\n",
+    "    get_data_generator(N_BATCHES, BATCH_SIZE, N_FEATURES),\n",
     "    output_types=(tf.float32, tf.float32),\n",
-    "    output_shapes=((32, 32, 3), tuple()),\n",
+    "    output_shapes=((N_FEATURES,), tuple()),\n",
     ").batch(BATCH_SIZE)\n",
     "\n",
-    "clf = ak.ImageDataClassifier(overwrite=True, max_trials=1, seed=5)\n",
+    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=1, seed=5)\n",
     "clf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)\n",
     "print(clf.evaluate(dataset))"
    ]
diff --git a/docs/ipynb/multi.ipynb b/docs/ipynb/multi.ipynb
index 4409822..92dea6a 100644
--- a/docs/ipynb/multi.ipynb
+++ b/docs/ipynb/multi.ipynb
@@ -39,7 +39,7 @@
     "Multi-modal data means each data instance has multiple forms of information.\n",
     "For example, a photo can be saved as a image. Besides the image, it may also\n",
     "have when and where it was taken as its attributes, which can be represented as\n",
-    "numerical data.\n",
+    "structured data.\n",
     "\n",
     "## What is multi-task?\n",
     "\n",
@@ -54,19 +54,18 @@
     "<div class=\"mermaid\">\n",
     "graph TD\n",
     "    id1(ImageInput) --> id3(Some Neural Network Model)\n",
-    "    id2(Input) --> id3\n",
+    "    id2(StructuredDataInput) --> id3\n",
     "    id3 --> id4(ClassificationHead)\n",
     "    id3 --> id5(RegressionHead)\n",
     "</div>\n",
     "\n",
-    "It has two inputs the images and the numerical input data. Each image is\n",
-    "associated with a set of attributes in the numerical input data. From these\n",
-    "data, we are trying to predict the classification label and the regression value\n",
-    "at the same time.\n",
+    "It has two inputs the images and the structured data. Each image is associated\n",
+    "with a set of attributes in the structured data. From these data, we are trying\n",
+    "to predict the classification label and the regression value at the same time.\n",
     "\n",
     "## Data Preparation\n",
     "\n",
-    "To illustrate our idea, we generate some random image and numerical data as\n",
+    "To illustrate our idea, we generate some random image and structured data as\n",
     "the multi-modal data.\n"
    ]
   },
@@ -81,8 +80,8 @@
     "num_instances = 100\n",
     "# Generate image data.\n",
     "image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n",
-    "# Generate numerical data.\n",
-    "numerical_data = np.random.rand(num_instances, 20).astype(np.float32)"
+    "# Generate structured data.\n",
+    "structured_data = np.random.rand(num_instances, 20).astype(np.float32)"
    ]
   },
   {
@@ -130,7 +129,7 @@
    "source": [
     "# Initialize the multi with multiple inputs and outputs.\n",
     "model = ak.AutoModel(\n",
-    "    inputs=[ak.ImageInput(), ak.Input()],\n",
+    "    inputs=[ak.ImageInput(), ak.StructuredDataInput()],\n",
     "    outputs=[\n",
     "        ak.RegressionHead(metrics=[\"mae\"]),\n",
     "        ak.ClassificationHead(\n",
@@ -142,7 +141,7 @@
     ")\n",
     "# Fit the model with prepared data.\n",
     "model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [regression_target, classification_target],\n",
     "    epochs=3,\n",
     ")"
@@ -169,7 +168,7 @@
    "outputs": [],
    "source": [
     "model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [regression_target, classification_target],\n",
     "    # Split the training data and use the last 15% as validation data.\n",
     "    validation_split=0.15,\n",
@@ -198,21 +197,21 @@
     "split = 20\n",
     "\n",
     "image_val = image_data[split:]\n",
-    "numerical_val = numerical_data[split:]\n",
+    "structured_val = structured_data[split:]\n",
     "regression_val = regression_target[split:]\n",
     "classification_val = classification_target[split:]\n",
     "\n",
     "image_data = image_data[:split]\n",
-    "numerical_data = numerical_data[:split]\n",
+    "structured_data = structured_data[:split]\n",
     "regression_target = regression_target[:split]\n",
     "classification_target = classification_target[:split]\n",
     "\n",
     "model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [regression_target, classification_target],\n",
     "    # Use your own validation set.\n",
     "    validation_data=(\n",
-    "        [image_val, numerical_val],\n",
+    "        [image_val, structured_val],\n",
     "        [regression_val, classification_val],\n",
     "    ),\n",
     "    epochs=2,\n",
@@ -237,7 +236,8 @@
     "    id3 --> id5(ResNet V2)\n",
     "    id4 --> id6(Merge)\n",
     "    id5 --> id6\n",
-    "    id7(Input) --> id9(DenseBlock)\n",
+    "    id7(StructuredDataInput) --> id8(CategoricalToNumerical)\n",
+    "    id8 --> id9(DenseBlock)\n",
     "    id6 --> id10(Merge)\n",
     "    id9 --> id10\n",
     "    id10 --> id11(Classification Head)\n",
@@ -260,7 +260,8 @@
     "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
     "output_node1 = ak.Merge()([output_node1, output_node2])\n",
     "\n",
-    "input_node2 = ak.Input()\n",
+    "input_node2 = ak.StructuredDataInput()\n",
+    "output_node = ak.CategoricalToNumerical()(input_node2)\n",
     "output_node2 = ak.DenseBlock()(output_node)\n",
     "\n",
     "output_node = ak.Merge()([output_node1, output_node2])\n",
@@ -275,12 +276,12 @@
     ")\n",
     "\n",
     "image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n",
-    "numerical_data = np.random.rand(num_instances, 20).astype(np.float32)\n",
+    "structured_data = np.random.rand(num_instances, 20).astype(np.float32)\n",
     "regression_target = np.random.rand(num_instances, 1).astype(np.float32)\n",
     "classification_target = np.random.randint(5, size=num_instances)\n",
     "\n",
     "auto_model.fit(\n",
-    "    [image_data, numerical_data],\n",
+    "    [image_data, structured_data],\n",
     "    [classification_target, regression_target],\n",
     "    batch_size=32,\n",
     "    epochs=3,\n",
@@ -296,7 +297,7 @@
     "## Data Format\n",
     "You can refer to the documentation of\n",
     "[ImageInput](/node/#imageinput-class),\n",
-    "[Input](/node/#input-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[RegressionHead](/block/#regressionhead-class),\n",
     "[ClassificationHead](/block/#classificationhead-class),\n",
@@ -304,11 +305,13 @@
     "You can also refer to the Data Format section of the tutorials of\n",
     "[Image Classification](/tutorial/image_classification/#data-format),\n",
     "[Text Classification](/tutorial/text_classification/#data-format),\n",
+    "[Structured Data Classification](\n",
+    "/tutorial/structured_data_classification/#data-format).\n",
     "\n",
     "## Reference\n",
     "[AutoModel](/auto_model/#automodel-class),\n",
     "[ImageInput](/node/#imageinput-class),\n",
-    "[Input](/node/#input-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
     "[DenseBlock](/block/#denseblock-class),\n",
     "[RegressionHead](/block/#regressionhead-class),\n",
     "[ClassificationHead](/block/#classificationhead-class),\n",
diff --git a/docs/ipynb/structured_data_classification.ipynb b/docs/ipynb/structured_data_classification.ipynb
new file mode 100644
index 0000000..1205b30
--- /dev/null
+++ b/docs/ipynb/structured_data_classification.ipynb
@@ -0,0 +1,424 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install autokeras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "import tensorflow as tf\n",
+    "\n",
+    "import autokeras as ak"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## A Simple Example\n",
+    "The first step is to prepare your data. Here we use the [Titanic\n",
+    "dataset](https://www.kaggle.com/c/titanic) as an example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
+    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
+    "\n",
+    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
+    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The second step is to run the\n",
+    "[StructuredDataClassifier](/structured_data_classifier).\n",
+    "As a quick demo, we set epochs to 10.\n",
+    "You can also leave the epochs unspecified for an adaptive number of epochs.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data classifier.\n",
+    "clf = ak.StructuredDataClassifier(\n",
+    "    overwrite=True, max_trials=3\n",
+    ")  # It tries 3 different models.\n",
+    "# Feed the structured data classifier with training data.\n",
+    "clf.fit(\n",
+    "    # The path to the train.csv file.\n",
+    "    train_file_path,\n",
+    "    # The name of the label column.\n",
+    "    \"survived\",\n",
+    "    epochs=10,\n",
+    ")\n",
+    "# Predict with the best model.\n",
+    "predicted_y = clf.predict(test_file_path)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(test_file_path, \"survived\"))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Data Format\n",
+    "The AutoKeras StructuredDataClassifier is quite flexible for the data format.\n",
+    "\n",
+    "The example above shows how to use the CSV files directly. Besides CSV files,\n",
+    "it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](\n",
+    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The\n",
+    "data should be two-dimensional with numerical or categorical values.\n",
+    "\n",
+    "For the classification labels,\n",
+    "AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded\n",
+    "encoded labels, i.e. vectors of 0s and 1s.\n",
+    "The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series.\n",
+    "\n",
+    "The following examples show how the data can be prepared with numpy.ndarray,\n",
+    "pandas.DataFrame, and tensorflow.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# x_train as pandas.DataFrame, y_train as pandas.Series\n",
+    "x_train = pd.read_csv(train_file_path)\n",
+    "print(type(x_train))  # pandas.DataFrame\n",
+    "y_train = x_train.pop(\"survived\")\n",
+    "print(type(y_train))  # pandas.Series\n",
+    "\n",
+    "# You can also use pandas.DataFrame for y_train.\n",
+    "y_train = pd.DataFrame(y_train)\n",
+    "print(type(y_train))  # pandas.DataFrame\n",
+    "\n",
+    "# You can also use numpy.ndarray for x_train and y_train.\n",
+    "x_train = x_train.to_numpy()\n",
+    "y_train = y_train.to_numpy()\n",
+    "print(type(x_train))  # numpy.ndarray\n",
+    "print(type(y_train))  # numpy.ndarray\n",
+    "\n",
+    "# Preparing testing data.\n",
+    "x_test = pd.read_csv(test_file_path)\n",
+    "y_test = x_test.pop(\"survived\")\n",
+    "\n",
+    "# It tries 10 different models.\n",
+    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)\n",
+    "# Feed the structured data classifier with training data.\n",
+    "clf.fit(x_train, y_train, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = clf.predict(x_test)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(x_test, y_test))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The following code shows how to convert numpy.ndarray to tf.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "train_set = tf.data.Dataset.from_tensor_slices((x_train.astype(str), y_train))\n",
+    "test_set = tf.data.Dataset.from_tensor_slices(\n",
+    "    (x_test.to_numpy().astype(str), y_test)\n",
+    ")\n",
+    "\n",
+    "clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)\n",
+    "# Feed the tensorflow Dataset to the classifier.\n",
+    "clf.fit(train_set, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = clf.predict(test_set)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(test_set))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also specify the column names and types for the data as follows.  The\n",
+    "`column_names` is optional if the training data already have the column names,\n",
+    "e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will\n",
+    "be inferred from the training data.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data classifier.\n",
+    "clf = ak.StructuredDataClassifier(\n",
+    "    column_names=[\n",
+    "        \"sex\",\n",
+    "        \"age\",\n",
+    "        \"n_siblings_spouses\",\n",
+    "        \"parch\",\n",
+    "        \"fare\",\n",
+    "        \"class\",\n",
+    "        \"deck\",\n",
+    "        \"embark_town\",\n",
+    "        \"alone\",\n",
+    "    ],\n",
+    "    column_types={\"sex\": \"categorical\", \"fare\": \"numerical\"},\n",
+    "    max_trials=10,  # It tries 10 different models.\n",
+    "    overwrite=True,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Validation Data\n",
+    "By default, AutoKeras use the last 20% of training data as validation data.  As\n",
+    "shown in the example below, you can use `validation_split` to specify the\n",
+    "percentage.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "clf.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Split the training data and use the last 15% as validation data.\n",
+    "    validation_split=0.15,\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also use your own validation set\n",
+    "instead of splitting it from the training data with `validation_data`.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "split = 500\n",
+    "x_val = x_train[split:]\n",
+    "y_val = y_train[split:]\n",
+    "x_train = x_train[:split]\n",
+    "y_train = y_train[:split]\n",
+    "clf.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Use your own validation set.\n",
+    "    validation_data=(x_val, y_val),\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Customized Search Space\n",
+    "For advanced users, you may customize your search space by using\n",
+    "[AutoModel](/auto_model/#automodel-class) instead of\n",
+    "[StructuredDataClassifier](/structured_data_classifier). You can configure the\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class) for some high-level\n",
+    "configurations, e.g., `categorical_encoding` for whether to use the\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do\n",
+    "not specify these arguments, which would leave the different choices to be\n",
+    "tuned automatically. See the following example for detail.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n",
+    "output_node = ak.ClassificationHead()(output_node)\n",
+    "clf = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3\n",
+    ")\n",
+    "clf.fit(x_train, y_train, epochs=10)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.\n",
+    "To add an edge from `input_node` to `output_node` with\n",
+    "`output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space even\n",
+    "further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.CategoricalToNumerical()(input_node)\n",
+    "output_node = ak.DenseBlock()(output_node)\n",
+    "output_node = ak.ClassificationHead()(output_node)\n",
+    "clf = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
+    ")\n",
+    "clf.fit(x_train, y_train, epochs=1)\n",
+    "clf.predict(x_train)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also export the best model found by AutoKeras as a Keras Model.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "model = clf.export_model()\n",
+    "model.summary()\n",
+    "print(x_train.dtype)\n",
+    "# numpy array in object (mixed type) is not supported.\n",
+    "# convert it to unicode.\n",
+    "model.predict(x_train.astype(str))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Reference\n",
+    "[StructuredDataClassifier](/structured_data_classifier),\n",
+    "[AutoModel](/auto_model/#automodel-class),\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class),\n",
+    "[DenseBlock](/block/#denseblock-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
+    "[ClassificationHead](/block/#classificationhead-class),\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class).\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "collapsed_sections": [],
+   "name": "structured_data_classification",
+   "private_outputs": false,
+   "provenance": [],
+   "toc_visible": true
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
diff --git a/docs/ipynb/structured_data_regression.ipynb b/docs/ipynb/structured_data_regression.ipynb
new file mode 100644
index 0000000..ff20cf1
--- /dev/null
+++ b/docs/ipynb/structured_data_regression.ipynb
@@ -0,0 +1,427 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install autokeras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "import numpy as np\n",
+    "import pandas as pd\n",
+    "import tensorflow as tf\n",
+    "from sklearn.datasets import fetch_california_housing\n",
+    "\n",
+    "import autokeras as ak"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## A Simple Example\n",
+    "The first step is to prepare your data. Here we use the [California housing\n",
+    "dataset](\n",
+    "https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)\n",
+    "as an example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "house_dataset = fetch_california_housing()\n",
+    "df = pd.DataFrame(\n",
+    "    np.concatenate(\n",
+    "        (house_dataset.data, house_dataset.target.reshape(-1, 1)), axis=1\n",
+    "    ),\n",
+    "    columns=house_dataset.feature_names + [\"Price\"],\n",
+    ")\n",
+    "train_size = int(df.shape[0] * 0.9)\n",
+    "df[:train_size].to_csv(\"train.csv\", index=False)\n",
+    "df[train_size:].to_csv(\"eval.csv\", index=False)\n",
+    "train_file_path = \"train.csv\"\n",
+    "test_file_path = \"eval.csv\""
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The second step is to run the\n",
+    "[StructuredDataRegressor](/structured_data_regressor).\n",
+    "As a quick demo, we set epochs to 10.\n",
+    "You can also leave the epochs unspecified for an adaptive number of epochs.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data regressor.\n",
+    "reg = ak.StructuredDataRegressor(\n",
+    "    overwrite=True, max_trials=3\n",
+    ")  # It tries 3 different models.\n",
+    "# Feed the structured data regressor with training data.\n",
+    "reg.fit(\n",
+    "    # The path to the train.csv file.\n",
+    "    train_file_path,\n",
+    "    # The name of the label column.\n",
+    "    \"Price\",\n",
+    "    epochs=10,\n",
+    ")\n",
+    "# Predict with the best model.\n",
+    "predicted_y = reg.predict(test_file_path)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(reg.evaluate(test_file_path, \"Price\"))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Data Format\n",
+    "The AutoKeras StructuredDataRegressor is quite flexible for the data format.\n",
+    "\n",
+    "The example above shows how to use the CSV files directly. Besides CSV files,\n",
+    "it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](\n",
+    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The\n",
+    "data should be two-dimensional with numerical or categorical values.\n",
+    "\n",
+    "For the regression targets, it should be a vector of numerical values.\n",
+    "AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series.\n",
+    "\n",
+    "The following examples show how the data can be prepared with numpy.ndarray,\n",
+    "pandas.DataFrame, and tensorflow.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# x_train as pandas.DataFrame, y_train as pandas.Series\n",
+    "x_train = pd.read_csv(train_file_path)\n",
+    "print(type(x_train))  # pandas.DataFrame\n",
+    "y_train = x_train.pop(\"Price\")\n",
+    "print(type(y_train))  # pandas.Series\n",
+    "\n",
+    "# You can also use pandas.DataFrame for y_train.\n",
+    "y_train = pd.DataFrame(y_train)\n",
+    "print(type(y_train))  # pandas.DataFrame\n",
+    "\n",
+    "# You can also use numpy.ndarray for x_train and y_train.\n",
+    "x_train = x_train.to_numpy()\n",
+    "y_train = y_train.to_numpy()\n",
+    "print(type(x_train))  # numpy.ndarray\n",
+    "print(type(y_train))  # numpy.ndarray\n",
+    "\n",
+    "# Preparing testing data.\n",
+    "x_test = pd.read_csv(test_file_path)\n",
+    "y_test = x_test.pop(\"Price\")\n",
+    "\n",
+    "# It tries 10 different models.\n",
+    "reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
+    "# Feed the structured data regressor with training data.\n",
+    "reg.fit(x_train, y_train, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = reg.predict(x_test)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(reg.evaluate(x_test, y_test))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The following code shows how to convert numpy.ndarray to tf.data.Dataset.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
+    "test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
+    "\n",
+    "reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)\n",
+    "# Feed the tensorflow Dataset to the regressor.\n",
+    "reg.fit(train_set, epochs=10)\n",
+    "# Predict with the best model.\n",
+    "predicted_y = reg.predict(test_set)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(reg.evaluate(test_set))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also specify the column names and types for the data as follows.  The\n",
+    "`column_names` is optional if the training data already have the column names,\n",
+    "e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will\n",
+    "be inferred from the training data.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "# Initialize the structured data regressor.\n",
+    "reg = ak.StructuredDataRegressor(\n",
+    "    column_names=[\n",
+    "        \"MedInc\",\n",
+    "        \"HouseAge\",\n",
+    "        \"AveRooms\",\n",
+    "        \"AveBedrms\",\n",
+    "        \"Population\",\n",
+    "        \"AveOccup\",\n",
+    "        \"Latitude\",\n",
+    "        \"Longitude\",\n",
+    "    ],\n",
+    "    column_types={\"MedInc\": \"numerical\", \"Latitude\": \"numerical\"},\n",
+    "    max_trials=10,  # It tries 10 different models.\n",
+    "    overwrite=True,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Validation Data\n",
+    "By default, AutoKeras use the last 20% of training data as validation data.  As\n",
+    "shown in the example below, you can use `validation_split` to specify the\n",
+    "percentage.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "reg.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Split the training data and use the last 15% as validation data.\n",
+    "    validation_split=0.15,\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also use your own validation set\n",
+    "instead of splitting it from the training data with `validation_data`.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "split = 500\n",
+    "x_val = x_train[split:]\n",
+    "y_val = y_train[split:]\n",
+    "x_train = x_train[:split]\n",
+    "y_train = y_train[:split]\n",
+    "reg.fit(\n",
+    "    x_train,\n",
+    "    y_train,\n",
+    "    # Use your own validation set.\n",
+    "    validation_data=(x_val, y_val),\n",
+    "    epochs=10,\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Customized Search Space\n",
+    "For advanced users, you may customize your search space by using\n",
+    "[AutoModel](/auto_model/#automodel-class) instead of\n",
+    "[StructuredDataRegressor](/structured_data_regressor). You can configure the\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class) for some high-level\n",
+    "configurations, e.g., `categorical_encoding` for whether to use the\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do\n",
+    "not specify these arguments, which would leave the different choices to be\n",
+    "tuned automatically. See the following example for detail.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)\n",
+    "output_node = ak.RegressionHead()(output_node)\n",
+    "reg = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3\n",
+    ")\n",
+    "reg.fit(x_train, y_train, epochs=10)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.  To add an edge from `input_node` to\n",
+    "`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space\n",
+    "even further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.StructuredDataInput()\n",
+    "output_node = ak.CategoricalToNumerical()(input_node)\n",
+    "output_node = ak.DenseBlock()(output_node)\n",
+    "output_node = ak.RegressionHead()(output_node)\n",
+    "reg = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, max_trials=3, overwrite=True\n",
+    ")\n",
+    "reg.fit(x_train, y_train, epochs=10)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "You can also export the best model found by AutoKeras as a Keras Model.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "model = reg.export_model()\n",
+    "model.summary()\n",
+    "# numpy array in object (mixed type) is not supported.\n",
+    "# you need convert it to unicode or float first.\n",
+    "model.predict(x_train)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "## Reference\n",
+    "[StructuredDataRegressor](/structured_data_regressor),\n",
+    "[AutoModel](/auto_model/#automodel-class),\n",
+    "[StructuredDataBlock](/block/#structureddatablock-class),\n",
+    "[DenseBlock](/block/#denseblock-class),\n",
+    "[StructuredDataInput](/node/#structureddatainput-class),\n",
+    "[RegressionHead](/block/#regressionhead-class),\n",
+    "[CategoricalToNumerical](/block/#categoricaltonumerical-class).\n"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "collapsed_sections": [],
+   "name": "structured_data_regression",
+   "private_outputs": false,
+   "provenance": [],
+   "toc_visible": true
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
diff --git a/docs/ipynb/text_classification.ipynb b/docs/ipynb/text_classification.ipynb
index 1859fdd..3a9c5ea 100644
--- a/docs/ipynb/text_classification.ipynb
+++ b/docs/ipynb/text_classification.ipynb
@@ -21,7 +21,6 @@
    "source": [
     "import os\n",
     "\n",
-    "import keras\n",
     "import numpy as np\n",
     "import tensorflow as tf\n",
     "from sklearn.datasets import load_files\n",
@@ -49,7 +48,7 @@
    },
    "outputs": [],
    "source": [
-    "dataset = keras.utils.get_file(\n",
+    "dataset = tf.keras.utils.get_file(\n",
     "    fname=\"aclImdb.tar.gz\",\n",
     "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
     "    extract=True,\n",
@@ -177,9 +176,15 @@
     "For advanced users, you may customize your search space by using\n",
     "[AutoModel](/auto_model/#automodel-class) instead of\n",
     "[TextClassifier](/text_classifier). You can configure the\n",
-    "[TextBlock](/block/#textblock-class) for some high-level configurations. You can\n",
-    "also do not specify these arguments, which would leave the different choices to\n",
-    "be tuned automatically.  See the following example for detail.\n"
+    "[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,\n",
+    "`vectorizer` for the type of text vectorization method to use.  You can use\n",
+    "'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to\n",
+    "convert the words to integers and use [Embedding](/block/#embedding-class) for\n",
+    "embedding the integer sequences, or you can use 'ngram', which uses\n",
+    "[TextToNgramVector](/block/#texttongramvector-class) to vectorize the\n",
+    "sentences.  You can also do not specify these arguments, which would leave the\n",
+    "different choices to be tuned automatically.  See the following example for\n",
+    "detail.\n"
    ]
   },
   {
@@ -191,7 +196,43 @@
    "outputs": [],
    "source": [
     "input_node = ak.TextInput()\n",
-    "output_node = ak.TextBlock()(input_node)\n",
+    "output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n",
+    "output_node = ak.ClassificationHead()(output_node)\n",
+    "clf = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
+    ")\n",
+    "clf.fit(x_train, y_train, epochs=2)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.  To add an edge from `input_node` to\n",
+    "`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space\n",
+    "even further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.TextInput()\n",
+    "output_node = ak.TextToIntSequence()(input_node)\n",
+    "output_node = ak.Embedding()(output_node)\n",
+    "# Use separable Conv layers in Keras.\n",
+    "output_node = ak.ConvBlock(separable=True)(output_node)\n",
     "output_node = ak.ClassificationHead()(output_node)\n",
     "clf = ak.AutoModel(\n",
     "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
@@ -248,6 +289,10 @@
     "## Reference\n",
     "[TextClassifier](/text_classifier),\n",
     "[AutoModel](/auto_model/#automodel-class),\n",
+    "[TextBlock](/block/#textblock-class),\n",
+    "[TextToInteSequence](/block/#texttointsequence-class),\n",
+    "[Embedding](/block/#embedding-class),\n",
+    "[TextToNgramVector](/block/#texttongramvector-class),\n",
     "[ConvBlock](/block/#convblock-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[ClassificationHead](/block/#classificationhead-class).\n"
diff --git a/docs/ipynb/text_regression.ipynb b/docs/ipynb/text_regression.ipynb
index ba7f2be..4af6d5a 100644
--- a/docs/ipynb/text_regression.ipynb
+++ b/docs/ipynb/text_regression.ipynb
@@ -181,9 +181,15 @@
     "For advanced users, you may customize your search space by using\n",
     "[AutoModel](/auto_model/#automodel-class) instead of\n",
     "[TextRegressor](/text_regressor). You can configure the\n",
-    "[TextBlock](/block/#textblock-class) for some high-level configurations. You can\n",
-    "also do not specify these arguments, which would leave the different choices to\n",
-    "be tuned automatically.  See the following example for detail.\n"
+    "[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,\n",
+    "`vectorizer` for the type of text vectorization method to use.  You can use\n",
+    "'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to\n",
+    "convert the words to integers and use [Embedding](/block/#embedding-class) for\n",
+    "embedding the integer sequences, or you can use 'ngram', which uses\n",
+    "[TextToNgramVector](/block/#texttongramvector-class) to vectorize the\n",
+    "sentences.  You can also do not specify these arguments, which would leave the\n",
+    "different choices to be tuned automatically.  See the following example for\n",
+    "detail.\n"
    ]
   },
   {
@@ -195,7 +201,43 @@
    "outputs": [],
    "source": [
     "input_node = ak.TextInput()\n",
-    "output_node = ak.TextBlock()(input_node)\n",
+    "output_node = ak.TextBlock(block_type=\"ngram\")(input_node)\n",
+    "output_node = ak.RegressionHead()(output_node)\n",
+    "reg = ak.AutoModel(\n",
+    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
+    ")\n",
+    "reg.fit(x_train, y_train, epochs=2)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The usage of [AutoModel](/auto_model/#automodel-class) is similar to the\n",
+    "[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.\n",
+    "Basically, you are building a graph, whose edges are blocks and the nodes are\n",
+    "intermediate outputs of blocks.  To add an edge from `input_node` to\n",
+    "`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.\n",
+    "\n",
+    "You can even also use more fine grained blocks to customize the search space\n",
+    "even further. See the following example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "input_node = ak.TextInput()\n",
+    "output_node = ak.TextToIntSequence()(input_node)\n",
+    "output_node = ak.Embedding()(output_node)\n",
+    "# Use separable Conv layers in Keras.\n",
+    "output_node = ak.ConvBlock(separable=True)(output_node)\n",
     "output_node = ak.RegressionHead()(output_node)\n",
     "reg = ak.AutoModel(\n",
     "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
@@ -253,6 +295,9 @@
     "[TextRegressor](/text_regressor),\n",
     "[AutoModel](/auto_model/#automodel-class),\n",
     "[TextBlock](/block/#textblock-class),\n",
+    "[TextToInteSequence](/block/#texttointsequence-class),\n",
+    "[Embedding](/block/#embedding-class),\n",
+    "[TextToNgramVector](/block/#texttongramvector-class),\n",
     "[ConvBlock](/block/#convblock-class),\n",
     "[TextInput](/node/#textinput-class),\n",
     "[RegressionHead](/block/#regressionhead-class).\n"
diff --git a/docs/ipynb/timeseries_forecaster.ipynb b/docs/ipynb/timeseries_forecaster.ipynb
new file mode 100644
index 0000000..e5ae676
--- /dev/null
+++ b/docs/ipynb/timeseries_forecaster.ipynb
@@ -0,0 +1,200 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install autokeras"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "import tensorflow as tf\n",
+    "\n",
+    "import autokeras as ak"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "To make this tutorial easy to follow, we use the UCI Airquality dataset, and try to\n",
+    "forecast the AH value at the different timesteps. Some basic preprocessing has also\n",
+    "been performed on the dataset as it required cleanup.\n",
+    "\n",
+    "## A Simple Example\n",
+    "The first step is to prepare your data. Here we use the [UCI Airquality\n",
+    "dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality) as an example.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "dataset = tf.keras.utils.get_file(\n",
+    "    fname=\"AirQualityUCI.csv\",\n",
+    "    origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/\"\n",
+    "    \"AirQualityUCI.zip\",\n",
+    "    extract=True,\n",
+    ")\n",
+    "\n",
+    "dataset = pd.read_csv(dataset, sep=\";\")\n",
+    "dataset = dataset[dataset.columns[:-2]]\n",
+    "dataset = dataset.dropna()\n",
+    "dataset = dataset.replace(\",\", \".\", regex=True)\n",
+    "\n",
+    "val_split = int(len(dataset) * 0.7)\n",
+    "data_train = dataset[:val_split]\n",
+    "validation_data = dataset[val_split:]\n",
+    "\n",
+    "data_x = data_train[\n",
+    "    [\n",
+    "        \"CO(GT)\",\n",
+    "        \"PT08.S1(CO)\",\n",
+    "        \"NMHC(GT)\",\n",
+    "        \"C6H6(GT)\",\n",
+    "        \"PT08.S2(NMHC)\",\n",
+    "        \"NOx(GT)\",\n",
+    "        \"PT08.S3(NOx)\",\n",
+    "        \"NO2(GT)\",\n",
+    "        \"PT08.S4(NO2)\",\n",
+    "        \"PT08.S5(O3)\",\n",
+    "        \"T\",\n",
+    "        \"RH\",\n",
+    "    ]\n",
+    "].astype(\"float64\")\n",
+    "\n",
+    "data_x_val = validation_data[\n",
+    "    [\n",
+    "        \"CO(GT)\",\n",
+    "        \"PT08.S1(CO)\",\n",
+    "        \"NMHC(GT)\",\n",
+    "        \"C6H6(GT)\",\n",
+    "        \"PT08.S2(NMHC)\",\n",
+    "        \"NOx(GT)\",\n",
+    "        \"PT08.S3(NOx)\",\n",
+    "        \"NO2(GT)\",\n",
+    "        \"PT08.S4(NO2)\",\n",
+    "        \"PT08.S5(O3)\",\n",
+    "        \"T\",\n",
+    "        \"RH\",\n",
+    "    ]\n",
+    "].astype(\"float64\")\n",
+    "\n",
+    "# Data with train data and the unseen data from subsequent time steps.\n",
+    "data_x_test = dataset[\n",
+    "    [\n",
+    "        \"CO(GT)\",\n",
+    "        \"PT08.S1(CO)\",\n",
+    "        \"NMHC(GT)\",\n",
+    "        \"C6H6(GT)\",\n",
+    "        \"PT08.S2(NMHC)\",\n",
+    "        \"NOx(GT)\",\n",
+    "        \"PT08.S3(NOx)\",\n",
+    "        \"NO2(GT)\",\n",
+    "        \"PT08.S4(NO2)\",\n",
+    "        \"PT08.S5(O3)\",\n",
+    "        \"T\",\n",
+    "        \"RH\",\n",
+    "    ]\n",
+    "].astype(\"float64\")\n",
+    "\n",
+    "data_y = data_train[\"AH\"].astype(\"float64\")\n",
+    "\n",
+    "data_y_val = validation_data[\"AH\"].astype(\"float64\")\n",
+    "\n",
+    "print(data_x.shape)  # (6549, 12)\n",
+    "print(data_y.shape)  # (6549,)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "colab_type": "text"
+   },
+   "source": [
+    "The second step is to run the [TimeSeriesForecaster](/time_series_forecaster).\n",
+    "As a quick demo, we set epochs to 10.\n",
+    "You can also leave the epochs unspecified for an adaptive number of epochs.\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 0,
+   "metadata": {
+    "colab_type": "code"
+   },
+   "outputs": [],
+   "source": [
+    "predict_from = 1\n",
+    "predict_until = 10\n",
+    "lookback = 3\n",
+    "clf = ak.TimeseriesForecaster(\n",
+    "    lookback=lookback,\n",
+    "    predict_from=predict_from,\n",
+    "    predict_until=predict_until,\n",
+    "    max_trials=1,\n",
+    "    objective=\"val_loss\",\n",
+    ")\n",
+    "# Train the TimeSeriesForecaster with train data\n",
+    "clf.fit(\n",
+    "    x=data_x,\n",
+    "    y=data_y,\n",
+    "    validation_data=(data_x_val, data_y_val),\n",
+    "    batch_size=32,\n",
+    "    epochs=10,\n",
+    ")\n",
+    "# Predict with the best model(includes original training data).\n",
+    "predictions = clf.predict(data_x_test)\n",
+    "print(predictions.shape)\n",
+    "# Evaluate the best model with testing data.\n",
+    "print(clf.evaluate(data_x_val, data_y_val))"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "collapsed_sections": [],
+   "name": "timeseries_forecaster",
+   "private_outputs": false,
+   "provenance": [],
+   "toc_visible": true
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
diff --git a/docs/py/customized.py b/docs/py/customized.py
index 86a1fc8..acf76ed 100644
--- a/docs/py/customized.py
+++ b/docs/py/customized.py
@@ -1,11 +1,9 @@
 """shell
 pip install autokeras
 """
-
-import keras
 import numpy as np
-import tree
-from keras.datasets import mnist
+import tensorflow as tf
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -86,19 +84,24 @@ If you would like to provide your own validation data or change the ratio of
 the validation data, please refer to the Validation Data section of the
 tutorials of [Image
 Classification](/tutorial/image_classification/#validation-data), [Text
-Classification](/tutorial/text_classification/#validation-data),
+Classification](/tutorial/text_classification/#validation-data), [Structured
+Data
+Classification](/tutorial/structured_data_classification/#validation-data),
 [Multi-task and Multiple Validation](/tutorial/multi/#validation-data).
 
 ## Data Format
 You can refer to the documentation of
 [ImageInput](/node/#imageinput-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
 for the format of different types of data.
 You can also refer to the Data Format section of the tutorials of
 [Image Classification](/tutorial/image_classification/#data-format),
-[Text Classification](/tutorial/text_classification/#data-format).
+[Text Classification](/tutorial/text_classification/#data-format),
+[Structured Data
+Classification](/tutorial/structured_data_classification/#data-format).
 
 ## Implement New Block
 
@@ -117,8 +120,8 @@ number of neurons is tunable.
 class SingleDenseLayerBlock(ak.Block):
     def build(self, hp, inputs=None):
         # Get the input_node from inputs.
-        input_node = tree.flatten(inputs)[0]
-        layer = keras.layers.Dense(
+        input_node = tf.nest.flatten(inputs)[0]
+        layer = tf.keras.layers.Dense(
             hp.Int("num_units", min_value=32, max_value=512, step=32)
         )
         output_node = layer(input_node)
@@ -153,6 +156,7 @@ print(auto_model.evaluate(x_test, y_test))
 **Nodes**:
 [ImageInput](/node/#imageinput-class),
 [Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class).
 
 **Preprocessors**:
@@ -160,10 +164,13 @@ print(auto_model.evaluate(x_test, y_test))
 [ImageAugmentation](/block/#imageaugmentation-class),
 [LightGBM](/block/#lightgbm-class),
 [Normalization](/block/#normalization-class),
+[TextToIntSequence](/block/#texttointsequence-class),
+[TextToNgramVector](/block/#texttongramvector-class).
 
 **Blocks**:
 [ConvBlock](/block/#convblock-class),
 [DenseBlock](/block/#denseblock-class),
+[Embedding](/block/#embedding-class),
 [Merge](/block/#merge-class),
 [ResNetBlock](/block/#resnetblock-class),
 [RNNBlock](/block/#rnnblock-class),
@@ -171,5 +178,6 @@ print(auto_model.evaluate(x_test, y_test))
 [TemporalReduction](/block/#temporalreduction-class),
 [XceptionBlock](/block/#xceptionblock-class),
 [ImageBlock](/block/#imageblock-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
 [TextBlock](/block/#textblock-class).
 """
diff --git a/docs/py/export.py b/docs/py/export.py
index c648fa2..7c600c2 100644
--- a/docs/py/export.py
+++ b/docs/py/export.py
@@ -1,10 +1,9 @@
 """shell
 pip install autokeras
 """
-
-import numpy as np
-from keras.datasets import mnist
-from keras.models import load_model
+import tensorflow as tf
+from tensorflow.keras.datasets import mnist
+from tensorflow.keras.models import load_model
 
 import autokeras as ak
 
@@ -19,6 +18,7 @@ All the tasks and the [AutoModel](/auto_model/#automodel-class) has this
 """
 
 
+print(tf.__version__)
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 
 # Initialize the image classifier.
@@ -32,12 +32,13 @@ model = clf.export_model()
 
 print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>
 
-model.save("model_autokeras.keras")
+try:
+    model.save("model_autokeras", save_format="tf")
+except Exception:
+    model.save("model_autokeras.h5")
 
 
-loaded_model = load_model(
-    "model_autokeras.keras", custom_objects=ak.CUSTOM_OBJECTS
-)
+loaded_model = load_model("model_autokeras", custom_objects=ak.CUSTOM_OBJECTS)
 
-predicted_y = loaded_model.predict(np.expand_dims(x_test, -1))
+predicted_y = loaded_model.predict(tf.expand_dims(x_test, -1))
 print(predicted_y)
diff --git a/docs/py/image_classification.py b/docs/py/image_classification.py
index 79c929e..6886227 100644
--- a/docs/py/image_classification.py
+++ b/docs/py/image_classification.py
@@ -1,10 +1,9 @@
 """shell
 pip install autokeras
 """
-
 import numpy as np
 import tensorflow as tf
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -15,10 +14,6 @@ example
 """
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
-x_train = x_train[:100]
-y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 print(x_train.shape)  # (60000, 28, 28)
 print(y_train.shape)  # (60000,)
 print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
@@ -34,7 +29,7 @@ You can also leave the epochs unspecified for an adaptive number of epochs.
 # Initialize the image classifier.
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
 # Feed the image classifier with training data.
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 
 # Predict with the best model.
@@ -57,7 +52,7 @@ clf.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
+    epochs=10,
 )
 
 """
@@ -75,7 +70,7 @@ clf.fit(
     y_train,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    epochs=1,
+    epochs=10,
 )
 
 """
@@ -102,7 +97,7 @@ output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 """
 The usage of AutoModel is similar to the functional API of Keras. Basically, you
@@ -122,7 +117,7 @@ output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1)
+clf.fit(x_train, y_train, epochs=10)
 
 """
 ## Data Format
@@ -169,7 +164,7 @@ test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))
 
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
 # Feed the tensorflow Dataset to the classifier.
-clf.fit(train_set, epochs=1)
+clf.fit(train_set, epochs=10)
 # Predict with the best model.
 predicted_y = clf.predict(test_set)
 # Evaluate the best model with testing data.
diff --git a/docs/py/image_regression.py b/docs/py/image_regression.py
index dfe4781..aaf847f 100644
--- a/docs/py/image_regression.py
+++ b/docs/py/image_regression.py
@@ -3,7 +3,7 @@ pip install autokeras
 """
 
 import tensorflow as tf
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
@@ -22,8 +22,6 @@ example
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 x_train = x_train[:100]
 y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 print(x_train.shape)  # (60000, 28, 28)
 print(y_train.shape)  # (60000,)
 print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
@@ -31,14 +29,14 @@ print(y_train[:3])  # array([7, 2, 1], dtype=uint8)
 """
 The second step is to run the ImageRegressor.  It is recommended have more
 trials for more complicated datasets.  This is just a quick demo of MNIST, so
-we set max_trials to 1.  For the same reason, we set epochs to 1.  You can also
+we set max_trials to 1.  For the same reason, we set epochs to 2.  You can also
 leave the epochs unspecified for an adaptive number of epochs.
 """
 
 # Initialize the image regressor.
 reg = ak.ImageRegressor(overwrite=True, max_trials=1)
 # Feed the image regressor with training data.
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 
 # Predict with the best model.
@@ -61,7 +59,7 @@ reg.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
+    epochs=2,
 )
 
 """
@@ -106,7 +104,7 @@ output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 The usage of AutoModel is similar to the functional API of Keras. Basically,
@@ -126,7 +124,7 @@ output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -146,10 +144,6 @@ case, the images would have to be 3-dimentional.
 """
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
-x_train = x_train[:100]
-y_train = y_train[:100]
-x_test = x_test[:100]
-y_test = y_test[:100]
 
 # Reshape the images to have the channel dimension.
 x_train = x_train.reshape(x_train.shape + (1,))
@@ -165,7 +159,7 @@ test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))
 
 reg = ak.ImageRegressor(overwrite=True, max_trials=1)
 # Feed the tensorflow Dataset to the regressor.
-reg.fit(train_set, epochs=1)
+reg.fit(train_set, epochs=2)
 # Predict with the best model.
 predicted_y = reg.predict(test_set)
 # Evaluate the best model with testing data.
diff --git a/docs/py/load.py b/docs/py/load.py
index bc43a3f..1f8df90 100644
--- a/docs/py/load.py
+++ b/docs/py/load.py
@@ -5,7 +5,6 @@ pip install autokeras
 import os
 import shutil
 
-import keras
 import numpy as np
 import tensorflow as tf
 
@@ -24,7 +23,7 @@ First, we download the data and extract the files.
 """
 
 dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"  # noqa: E501
-local_file_path = keras.utils.get_file(
+local_file_path = tf.keras.utils.get_file(
     origin=dataset_url, fname="image_data", extract=True
 )
 # The file is extracted in the same directory as the downloaded file.
@@ -49,7 +48,7 @@ flowers_photos/
 We can split the data into training and testing as we load them.
 """
 
-batch_size = 2
+batch_size = 32
 img_height = 180
 img_width = 180
 
@@ -78,8 +77,8 @@ Then we just do one quick demo of AutoKeras to make sure the dataset works.
 """
 
 clf = ak.ImageClassifier(overwrite=True, max_trials=1)
-clf.fit(train_data.take(100), epochs=1)
-print(clf.evaluate(test_data.take(2)))
+clf.fit(train_data, epochs=1)
+print(clf.evaluate(test_data))
 
 """
 ## Load Texts from Disk
@@ -88,7 +87,7 @@ You can also load text datasets in the same way.
 
 dataset_url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
 
-local_file_path = keras.utils.get_file(
+local_file_path = tf.keras.utils.get_file(
     fname="text_data",
     origin=dataset_url,
     extract=True,
@@ -117,8 +116,8 @@ test_data = ak.text_dataset_from_directory(
 )
 
 clf = ak.TextClassifier(overwrite=True, max_trials=1)
-clf.fit(train_data.take(2), epochs=1)
-print(clf.evaluate(test_data.take(2)))
+clf.fit(train_data, epochs=2)
+print(clf.evaluate(test_data))
 
 
 """
@@ -127,29 +126,33 @@ If you want to use generators, you can refer to the following code.
 """
 
 
-N_BATCHES = 2
-BATCH_SIZE = 10
+N_BATCHES = 30
+BATCH_SIZE = 100
+N_FEATURES = 10
 
 
-def get_data_generator(n_batches, batch_size):
-    """Get a generator returning n_batches random data."""
+def get_data_generator(n_batches, batch_size, n_features):
+    """Get a generator returning n_batches random data.
+
+    The shape of the data is (batch_size, n_features).
+    """
 
     def data_generator():
         for _ in range(n_batches * batch_size):
-            x = np.random.randn(32, 32, 3)
-            y = x.sum() / 32 * 32 * 3 > 0.5
+            x = np.random.randn(n_features)
+            y = x.sum(axis=0) / n_features > 0.5
             yield x, y
 
     return data_generator
 
 
 dataset = tf.data.Dataset.from_generator(
-    get_data_generator(N_BATCHES, BATCH_SIZE),
+    get_data_generator(N_BATCHES, BATCH_SIZE, N_FEATURES),
     output_types=(tf.float32, tf.float32),
-    output_shapes=((32, 32, 3), tuple()),
+    output_shapes=((N_FEATURES,), tuple()),
 ).batch(BATCH_SIZE)
 
-clf = ak.ImageClassifier(overwrite=True, max_trials=1, seed=5)
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=1, seed=5)
 clf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)
 print(clf.evaluate(dataset))
 
diff --git a/docs/py/multi.py b/docs/py/multi.py
index c88e5e4..31e7284 100644
--- a/docs/py/multi.py
+++ b/docs/py/multi.py
@@ -16,7 +16,7 @@ In this tutorial we are making use of the
 Multi-modal data means each data instance has multiple forms of information.
 For example, a photo can be saved as a image. Besides the image, it may also
 have when and where it was taken as its attributes, which can be represented as
-numerical data.
+structured data.
 
 ## What is multi-task?
 
@@ -31,28 +31,27 @@ network model.
 <div class="mermaid">
 graph TD
     id1(ImageInput) --> id3(Some Neural Network Model)
-    id2(Input) --> id3
+    id2(StructuredDataInput) --> id3
     id3 --> id4(ClassificationHead)
     id3 --> id5(RegressionHead)
 </div>
 
-It has two inputs the images and the numerical input data. Each image is
-associated with a set of attributes in the numerical input data. From these
-data, we are trying to predict the classification label and the regression value
-at the same time.
+It has two inputs the images and the structured data. Each image is associated
+with a set of attributes in the structured data. From these data, we are trying
+to predict the classification label and the regression value at the same time.
 
 ## Data Preparation
 
-To illustrate our idea, we generate some random image and numerical data as
+To illustrate our idea, we generate some random image and structured data as
 the multi-modal data.
 """
 
 
-num_instances = 10
+num_instances = 100
 # Generate image data.
 image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)
-# Generate numerical data.
-numerical_data = np.random.rand(num_instances, 20).astype(np.float32)
+# Generate structured data.
+structured_data = np.random.rand(num_instances, 20).astype(np.float32)
 
 """
 We also generate some multi-task targets for classification and regression.
@@ -73,7 +72,7 @@ Since this is just a demo, we use small amount of `max_trials` and `epochs`.
 
 # Initialize the multi with multiple inputs and outputs.
 model = ak.AutoModel(
-    inputs=[ak.ImageInput(), ak.Input()],
+    inputs=[ak.ImageInput(), ak.StructuredDataInput()],
     outputs=[
         ak.RegressionHead(metrics=["mae"]),
         ak.ClassificationHead(
@@ -85,10 +84,9 @@ model = ak.AutoModel(
 )
 # Fit the model with prepared data.
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
-    epochs=1,
-    batch_size=3,
+    epochs=3,
 )
 
 """
@@ -99,12 +97,11 @@ percentage.
 """
 
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
-    batch_size=3,
+    epochs=2,
 )
 
 """
@@ -112,28 +109,27 @@ You can also use your own validation set
 instead of splitting it from the training data with `validation_data`.
 """
 
-split = 5
+split = 20
 
 image_val = image_data[split:]
-numerical_val = numerical_data[split:]
+structured_val = structured_data[split:]
 regression_val = regression_target[split:]
 classification_val = classification_target[split:]
 
 image_data = image_data[:split]
-numerical_data = numerical_data[:split]
+structured_data = structured_data[:split]
 regression_target = regression_target[:split]
 classification_target = classification_target[:split]
 
 model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [regression_target, classification_target],
     # Use your own validation set.
     validation_data=(
-        [image_val, numerical_val],
+        [image_val, structured_val],
         [regression_val, classification_val],
     ),
-    epochs=1,
-    batch_size=3,
+    epochs=2,
 )
 
 """
@@ -149,7 +145,8 @@ graph LR
     id3 --> id5(ResNet V2)
     id4 --> id6(Merge)
     id5 --> id6
-    id7(Input) --> id9(DenseBlock)
+    id7(StructuredDataInput) --> id8(CategoricalToNumerical)
+    id8 --> id9(DenseBlock)
     id6 --> id10(Merge)
     id9 --> id10
     id10 --> id11(Classification Head)
@@ -164,8 +161,9 @@ output_node1 = ak.ConvBlock()(output_node)
 output_node2 = ak.ResNetBlock(version="v2")(output_node)
 output_node1 = ak.Merge()([output_node1, output_node2])
 
-input_node2 = ak.Input()
-output_node2 = ak.DenseBlock()(input_node2)
+input_node2 = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node2)
+output_node2 = ak.DenseBlock()(output_node)
 
 output_node = ak.Merge()([output_node1, output_node2])
 output_node1 = ak.ClassificationHead()(output_node)
@@ -179,22 +177,22 @@ auto_model = ak.AutoModel(
 )
 
 image_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)
-numerical_data = np.random.rand(num_instances, 20).astype(np.float32)
+structured_data = np.random.rand(num_instances, 20).astype(np.float32)
 regression_target = np.random.rand(num_instances, 1).astype(np.float32)
 classification_target = np.random.randint(5, size=num_instances)
 
 auto_model.fit(
-    [image_data, numerical_data],
+    [image_data, structured_data],
     [classification_target, regression_target],
-    batch_size=3,
-    epochs=1,
+    batch_size=32,
+    epochs=3,
 )
 
 """
 ## Data Format
 You can refer to the documentation of
 [ImageInput](/node/#imageinput-class),
-[Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
@@ -202,11 +200,13 @@ for the format of different types of data.
 You can also refer to the Data Format section of the tutorials of
 [Image Classification](/tutorial/image_classification/#data-format),
 [Text Classification](/tutorial/text_classification/#data-format),
+[Structured Data Classification](
+/tutorial/structured_data_classification/#data-format).
 
 ## Reference
 [AutoModel](/auto_model/#automodel-class),
 [ImageInput](/node/#imageinput-class),
-[Input](/node/#input-class),
+[StructuredDataInput](/node/#structureddatainput-class),
 [DenseBlock](/block/#denseblock-class),
 [RegressionHead](/block/#regressionhead-class),
 [ClassificationHead](/block/#classificationhead-class),
diff --git a/docs/py/structured_data_classification.py b/docs/py/structured_data_classification.py
new file mode 100644
index 0000000..30be1f3
--- /dev/null
+++ b/docs/py/structured_data_classification.py
@@ -0,0 +1,234 @@
+"""shell
+pip install autokeras
+"""
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+## A Simple Example
+The first step is to prepare your data. Here we use the [Titanic
+dataset](https://www.kaggle.com/c/titanic) as an example.
+"""
+
+
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+
+"""
+The second step is to run the
+[StructuredDataClassifier](/structured_data_classifier).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+# Initialize the structured data classifier.
+clf = ak.StructuredDataClassifier(
+    overwrite=True, max_trials=3
+)  # It tries 3 different models.
+# Feed the structured data classifier with training data.
+clf.fit(
+    # The path to the train.csv file.
+    train_file_path,
+    # The name of the label column.
+    "survived",
+    epochs=10,
+)
+# Predict with the best model.
+predicted_y = clf.predict(test_file_path)
+# Evaluate the best model with testing data.
+print(clf.evaluate(test_file_path, "survived"))
+
+"""
+## Data Format
+The AutoKeras StructuredDataClassifier is quite flexible for the data format.
+
+The example above shows how to use the CSV files directly. Besides CSV files,
+it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](
+https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The
+data should be two-dimensional with numerical or categorical values.
+
+For the classification labels, AutoKeras accepts both plain labels, i.e. strings
+or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The
+labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series.
+
+The following examples show how the data can be prepared with numpy.ndarray,
+pandas.DataFrame, and tensorflow.data.Dataset.
+"""
+
+
+# x_train as pandas.DataFrame, y_train as pandas.Series
+x_train = pd.read_csv(train_file_path)
+print(type(x_train))  # pandas.DataFrame
+y_train = x_train.pop("survived")
+print(type(y_train))  # pandas.Series
+
+# You can also use pandas.DataFrame for y_train.
+y_train = pd.DataFrame(y_train)
+print(type(y_train))  # pandas.DataFrame
+
+# You can also use numpy.ndarray for x_train and y_train.
+x_train = x_train.to_numpy()
+y_train = y_train.to_numpy()
+print(type(x_train))  # numpy.ndarray
+print(type(y_train))  # numpy.ndarray
+
+# Preparing testing data.
+x_test = pd.read_csv(test_file_path)
+y_test = x_test.pop("survived")
+
+# It tries 10 different models.
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)
+# Feed the structured data classifier with training data.
+clf.fit(x_train, y_train, epochs=10)
+# Predict with the best model.
+predicted_y = clf.predict(x_test)
+# Evaluate the best model with testing data.
+print(clf.evaluate(x_test, y_test))
+
+"""
+The following code shows how to convert numpy.ndarray to tf.data.Dataset.
+"""
+
+train_set = tf.data.Dataset.from_tensor_slices((x_train.astype(str), y_train))
+test_set = tf.data.Dataset.from_tensor_slices(
+    (x_test.to_numpy().astype(str), y_test)
+)
+
+clf = ak.StructuredDataClassifier(overwrite=True, max_trials=3)
+# Feed the tensorflow Dataset to the classifier.
+clf.fit(train_set, epochs=10)
+# Predict with the best model.
+predicted_y = clf.predict(test_set)
+# Evaluate the best model with testing data.
+print(clf.evaluate(test_set))
+
+"""
+You can also specify the column names and types for the data as follows.  The
+`column_names` is optional if the training data already have the column names,
+e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will
+be inferred from the training data.
+"""
+
+# Initialize the structured data classifier.
+clf = ak.StructuredDataClassifier(
+    column_names=[
+        "sex",
+        "age",
+        "n_siblings_spouses",
+        "parch",
+        "fare",
+        "class",
+        "deck",
+        "embark_town",
+        "alone",
+    ],
+    column_types={"sex": "categorical", "fare": "numerical"},
+    max_trials=10,  # It tries 10 different models.
+    overwrite=True,
+)
+
+
+"""
+## Validation Data
+By default, AutoKeras use the last 20% of training data as validation data.  As
+shown in the example below, you can use `validation_split` to specify the
+percentage.
+"""
+
+clf.fit(
+    x_train,
+    y_train,
+    # Split the training data and use the last 15% as validation data.
+    validation_split=0.15,
+    epochs=10,
+)
+
+"""
+You can also use your own validation set
+instead of splitting it from the training data with `validation_data`.
+"""
+
+split = 500
+x_val = x_train[split:]
+y_val = y_train[split:]
+x_train = x_train[:split]
+y_train = y_train[:split]
+clf.fit(
+    x_train,
+    y_train,
+    # Use your own validation set.
+    validation_data=(x_val, y_val),
+    epochs=10,
+)
+
+"""
+## Customized Search Space
+For advanced users, you may customize your search space by using
+[AutoModel](/auto_model/#automodel-class) instead of
+[StructuredDataClassifier](/structured_data_classifier). You can configure the
+[StructuredDataBlock](/block/#structureddatablock-class) for some high-level
+configurations, e.g., `categorical_encoding` for whether to use the
+[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do
+not specify these arguments, which would leave the different choices to be
+tuned automatically. See the following example for detail.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3
+)
+clf.fit(x_train, y_train, epochs=10)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.
+To add an edge from `input_node` to `output_node` with
+`output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node)
+output_node = ak.DenseBlock()(output_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+clf.fit(x_train, y_train, epochs=1)
+clf.predict(x_train)
+
+"""
+You can also export the best model found by AutoKeras as a Keras Model.
+"""
+
+model = clf.export_model()
+model.summary()
+print(x_train.dtype)
+# numpy array in object (mixed type) is not supported.
+# convert it to unicode.
+model.predict(x_train.astype(str))
+
+"""
+## Reference
+[StructuredDataClassifier](/structured_data_classifier),
+[AutoModel](/auto_model/#automodel-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
+[DenseBlock](/block/#denseblock-class),
+[StructuredDataInput](/node/#structureddatainput-class),
+[ClassificationHead](/block/#classificationhead-class),
+[CategoricalToNumerical](/block/#categoricaltonumerical-class).
+"""
diff --git a/docs/py/structured_data_regression.py b/docs/py/structured_data_regression.py
new file mode 100644
index 0000000..236fd0f
--- /dev/null
+++ b/docs/py/structured_data_regression.py
@@ -0,0 +1,239 @@
+"""shell
+pip install autokeras
+"""
+
+import numpy as np
+import pandas as pd
+import tensorflow as tf
+from sklearn.datasets import fetch_california_housing
+
+import autokeras as ak
+
+"""
+## A Simple Example
+The first step is to prepare your data. Here we use the [California housing
+dataset](
+https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)
+as an example.
+"""
+
+
+house_dataset = fetch_california_housing()
+df = pd.DataFrame(
+    np.concatenate(
+        (house_dataset.data, house_dataset.target.reshape(-1, 1)), axis=1
+    ),
+    columns=house_dataset.feature_names + ["Price"],
+)
+train_size = int(df.shape[0] * 0.9)
+df[:train_size].to_csv("train.csv", index=False)
+df[train_size:].to_csv("eval.csv", index=False)
+train_file_path = "train.csv"
+test_file_path = "eval.csv"
+
+"""
+The second step is to run the
+[StructuredDataRegressor](/structured_data_regressor).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+# Initialize the structured data regressor.
+reg = ak.StructuredDataRegressor(
+    overwrite=True, max_trials=3
+)  # It tries 3 different models.
+# Feed the structured data regressor with training data.
+reg.fit(
+    # The path to the train.csv file.
+    train_file_path,
+    # The name of the label column.
+    "Price",
+    epochs=10,
+)
+# Predict with the best model.
+predicted_y = reg.predict(test_file_path)
+# Evaluate the best model with testing data.
+print(reg.evaluate(test_file_path, "Price"))
+
+"""
+## Data Format
+The AutoKeras StructuredDataRegressor is quite flexible for the data format.
+
+The example above shows how to use the CSV files directly. Besides CSV files,
+it also supports numpy.ndarray, pandas.DataFrame or [tf.data.Dataset](
+https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable). The
+data should be two-dimensional with numerical or categorical values.
+
+For the regression targets, it should be a vector of numerical values.
+AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series.
+
+The following examples show how the data can be prepared with numpy.ndarray,
+pandas.DataFrame, and tensorflow.data.Dataset.
+"""
+
+
+# x_train as pandas.DataFrame, y_train as pandas.Series
+x_train = pd.read_csv(train_file_path)
+print(type(x_train))  # pandas.DataFrame
+y_train = x_train.pop("Price")
+print(type(y_train))  # pandas.Series
+
+# You can also use pandas.DataFrame for y_train.
+y_train = pd.DataFrame(y_train)
+print(type(y_train))  # pandas.DataFrame
+
+# You can also use numpy.ndarray for x_train and y_train.
+x_train = x_train.to_numpy()
+y_train = y_train.to_numpy()
+print(type(x_train))  # numpy.ndarray
+print(type(y_train))  # numpy.ndarray
+
+# Preparing testing data.
+x_test = pd.read_csv(test_file_path)
+y_test = x_test.pop("Price")
+
+# It tries 10 different models.
+reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)
+# Feed the structured data regressor with training data.
+reg.fit(x_train, y_train, epochs=10)
+# Predict with the best model.
+predicted_y = reg.predict(x_test)
+# Evaluate the best model with testing data.
+print(reg.evaluate(x_test, y_test))
+
+"""
+The following code shows how to convert numpy.ndarray to tf.data.Dataset.
+"""
+
+train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train))
+test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test))
+
+reg = ak.StructuredDataRegressor(max_trials=3, overwrite=True)
+# Feed the tensorflow Dataset to the regressor.
+reg.fit(train_set, epochs=10)
+# Predict with the best model.
+predicted_y = reg.predict(test_set)
+# Evaluate the best model with testing data.
+print(reg.evaluate(test_set))
+
+"""
+You can also specify the column names and types for the data as follows.  The
+`column_names` is optional if the training data already have the column names,
+e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will
+be inferred from the training data.
+"""
+
+# Initialize the structured data regressor.
+reg = ak.StructuredDataRegressor(
+    column_names=[
+        "MedInc",
+        "HouseAge",
+        "AveRooms",
+        "AveBedrms",
+        "Population",
+        "AveOccup",
+        "Latitude",
+        "Longitude",
+    ],
+    column_types={"MedInc": "numerical", "Latitude": "numerical"},
+    max_trials=10,  # It tries 10 different models.
+    overwrite=True,
+)
+
+
+"""
+## Validation Data
+By default, AutoKeras use the last 20% of training data as validation data.  As
+shown in the example below, you can use `validation_split` to specify the
+percentage.
+"""
+
+reg.fit(
+    x_train,
+    y_train,
+    # Split the training data and use the last 15% as validation data.
+    validation_split=0.15,
+    epochs=10,
+)
+
+"""
+You can also use your own validation set
+instead of splitting it from the training data with `validation_data`.
+"""
+
+split = 500
+x_val = x_train[split:]
+y_val = y_train[split:]
+x_train = x_train[:split]
+y_train = y_train[:split]
+reg.fit(
+    x_train,
+    y_train,
+    # Use your own validation set.
+    validation_data=(x_val, y_val),
+    epochs=10,
+)
+
+"""
+## Customized Search Space
+For advanced users, you may customize your search space by using
+[AutoModel](/auto_model/#automodel-class) instead of
+[StructuredDataRegressor](/structured_data_regressor). You can configure the
+[StructuredDataBlock](/block/#structureddatablock-class) for some high-level
+configurations, e.g., `categorical_encoding` for whether to use the
+[CategoricalToNumerical](/block/#categoricaltonumerical-class). You can also do
+not specify these arguments, which would leave the different choices to be
+tuned automatically. See the following example for detail.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.StructuredDataBlock(categorical_encoding=True)(input_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=3
+)
+reg.fit(x_train, y_train, epochs=10)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.StructuredDataInput()
+output_node = ak.CategoricalToNumerical()(input_node)
+output_node = ak.DenseBlock()(output_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, max_trials=3, overwrite=True
+)
+reg.fit(x_train, y_train, epochs=10)
+
+"""
+You can also export the best model found by AutoKeras as a Keras Model.
+"""
+
+model = reg.export_model()
+model.summary()
+# numpy array in object (mixed type) is not supported.
+# you need convert it to unicode or float first.
+model.predict(x_train)
+
+
+"""
+## Reference
+[StructuredDataRegressor](/structured_data_regressor),
+[AutoModel](/auto_model/#automodel-class),
+[StructuredDataBlock](/block/#structureddatablock-class),
+[DenseBlock](/block/#denseblock-class),
+[StructuredDataInput](/node/#structureddatainput-class),
+[RegressionHead](/block/#regressionhead-class),
+[CategoricalToNumerical](/block/#categoricaltonumerical-class).
+"""
diff --git a/docs/py/text_classification.py b/docs/py/text_classification.py
index 3365df6..fae3def 100644
--- a/docs/py/text_classification.py
+++ b/docs/py/text_classification.py
@@ -4,7 +4,6 @@ pip install autokeras
 
 import os
 
-import keras
 import numpy as np
 import tensorflow as tf
 from sklearn.datasets import load_files
@@ -19,7 +18,7 @@ as an example.
 """
 
 
-dataset = keras.utils.get_file(
+dataset = tf.keras.utils.get_file(
     fname="aclImdb.tar.gz",
     origin="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz",
     extract=True,
@@ -36,10 +35,10 @@ test_data = load_files(
     os.path.join(IMDB_DATADIR, "test"), shuffle=False, categories=classes
 )
 
-x_train = np.array(train_data.data)[:100]
-y_train = np.array(train_data.target)[:100]
-x_test = np.array(test_data.data)[:100]
-y_test = np.array(test_data.target)[:100]
+x_train = np.array(train_data.data)
+y_train = np.array(train_data.target)
+x_test = np.array(test_data.data)
+y_test = np.array(test_data.target)
 
 print(x_train.shape)  # (25000,)
 print(y_train.shape)  # (25000, 1)
@@ -57,7 +56,7 @@ clf = ak.TextClassifier(
     overwrite=True, max_trials=1
 )  # It only tries 1 model as a quick demo.
 # Feed the text classifier with training data.
-clf.fit(x_train, y_train, epochs=1, batch_size=2)
+clf.fit(x_train, y_train, epochs=2)
 # Predict with the best model.
 predicted_y = clf.predict(x_test)
 # Evaluate the best model with testing data.
@@ -76,8 +75,6 @@ clf.fit(
     y_train,
     # Split the training data and use the last 15% as validation data.
     validation_split=0.15,
-    epochs=1,
-    batch_size=2,
 )
 
 """
@@ -85,7 +82,7 @@ You can also use your own validation set instead of splitting it from the
 training data with `validation_data`.
 """
 
-split = 5
+split = 5000
 x_val = x_train[split:]
 y_val = y_train[split:]
 x_train = x_train[:split]
@@ -93,10 +90,9 @@ y_train = y_train[:split]
 clf.fit(
     x_train,
     y_train,
-    epochs=1,
+    epochs=2,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    batch_size=2,
 )
 
 """
@@ -104,19 +100,48 @@ clf.fit(
 For advanced users, you may customize your search space by using
 [AutoModel](/auto_model/#automodel-class) instead of
 [TextClassifier](/text_classifier). You can configure the
-[TextBlock](/block/#textblock-class) for some high-level configurations. You can
-also do not specify these arguments, which would leave the different choices to
-be tuned automatically.  See the following example for detail.
+[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,
+`vectorizer` for the type of text vectorization method to use.  You can use
+'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to
+convert the words to integers and use [Embedding](/block/#embedding-class) for
+embedding the integer sequences, or you can use 'ngram', which uses
+[TextToNgramVector](/block/#texttongramvector-class) to vectorize the
+sentences.  You can also do not specify these arguments, which would leave the
+different choices to be tuned automatically.  See the following example for
+detail.
 """
 
 
 input_node = ak.TextInput()
-output_node = ak.TextBlock()(input_node)
+output_node = ak.TextBlock(block_type="ngram")(input_node)
 output_node = ak.ClassificationHead()(output_node)
 clf = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-clf.fit(x_train, y_train, epochs=1, batch_size=2)
+clf.fit(x_train, y_train, epochs=2)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
+"""
+
+
+input_node = ak.TextInput()
+output_node = ak.TextToIntSequence()(input_node)
+output_node = ak.Embedding()(output_node)
+# Use separable Conv layers in Keras.
+output_node = ak.ConvBlock(separable=True)(output_node)
+output_node = ak.ClassificationHead()(output_node)
+clf = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+clf.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -132,22 +157,26 @@ format for the training data.
 """
 
 train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(
-    2
+    32
 )
-test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)
+test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)
 
-clf = ak.TextClassifier(overwrite=True, max_trials=1)
+clf = ak.TextClassifier(overwrite=True, max_trials=2)
 # Feed the tensorflow Dataset to the classifier.
-clf.fit(train_set.take(2), epochs=1)
+clf.fit(train_set, epochs=2)
 # Predict with the best model.
-predicted_y = clf.predict(test_set.take(2))
+predicted_y = clf.predict(test_set)
 # Evaluate the best model with testing data.
-print(clf.evaluate(test_set.take(2)))
+print(clf.evaluate(test_set))
 
 """
 ## Reference
 [TextClassifier](/text_classifier),
 [AutoModel](/auto_model/#automodel-class),
+[TextBlock](/block/#textblock-class),
+[TextToInteSequence](/block/#texttointsequence-class),
+[Embedding](/block/#embedding-class),
+[TextToNgramVector](/block/#texttongramvector-class),
 [ConvBlock](/block/#convblock-class),
 [TextInput](/node/#textinput-class),
 [ClassificationHead](/block/#classificationhead-class).
diff --git a/docs/py/text_regression.py b/docs/py/text_regression.py
index 59d1058..d934809 100644
--- a/docs/py/text_regression.py
+++ b/docs/py/text_regression.py
@@ -1,7 +1,6 @@
 """shell
 pip install autokeras
 """
-
 import os
 
 import numpy as np
@@ -40,10 +39,10 @@ test_data = load_files(
     os.path.join(IMDB_DATADIR, "test"), shuffle=False, categories=classes
 )
 
-x_train = np.array(train_data.data)[:100]
-y_train = np.array(train_data.target)[:100]
-x_test = np.array(test_data.data)[:100]
-y_test = np.array(test_data.target)[:100]
+x_train = np.array(train_data.data)
+y_train = np.array(train_data.target)
+x_test = np.array(test_data.data)
+y_test = np.array(test_data.target)
 
 print(x_train.shape)  # (25000,)
 print(y_train.shape)  # (25000, 1)
@@ -58,10 +57,10 @@ adaptive number of epochs.
 
 # Initialize the text regressor.
 reg = ak.TextRegressor(
-    overwrite=True, max_trials=1  # It tries 10 different models.
+    overwrite=True, max_trials=10  # It tries 10 different models.
 )
 # Feed the text regressor with training data.
-reg.fit(x_train, y_train, epochs=1, batch_size=2)
+reg.fit(x_train, y_train, epochs=2)
 # Predict with the best model.
 predicted_y = reg.predict(x_test)
 # Evaluate the best model with testing data.
@@ -87,7 +86,7 @@ You can also use your own validation set instead of splitting it from the
 training data with `validation_data`.
 """
 
-split = 5
+split = 5000
 x_val = x_train[split:]
 y_val = y_train[split:]
 x_train = x_train[:split]
@@ -95,10 +94,9 @@ y_train = y_train[:split]
 reg.fit(
     x_train,
     y_train,
-    epochs=1,
+    epochs=2,
     # Use your own validation set.
     validation_data=(x_val, y_val),
-    batch_size=2,
 )
 
 """
@@ -106,19 +104,48 @@ reg.fit(
 For advanced users, you may customize your search space by using
 [AutoModel](/auto_model/#automodel-class) instead of
 [TextRegressor](/text_regressor). You can configure the
-[TextBlock](/block/#textblock-class) for some high-level configurations. You can
-also do not specify these arguments, which would leave the different choices to
-be tuned automatically.  See the following example for detail.
+[TextBlock](/block/#textblock-class) for some high-level configurations, e.g.,
+`vectorizer` for the type of text vectorization method to use.  You can use
+'sequence', which uses [TextToInteSequence](/block/#texttointsequence-class) to
+convert the words to integers and use [Embedding](/block/#embedding-class) for
+embedding the integer sequences, or you can use 'ngram', which uses
+[TextToNgramVector](/block/#texttongramvector-class) to vectorize the
+sentences.  You can also do not specify these arguments, which would leave the
+different choices to be tuned automatically.  See the following example for
+detail.
+"""
+
+
+input_node = ak.TextInput()
+output_node = ak.TextBlock(block_type="ngram")(input_node)
+output_node = ak.RegressionHead()(output_node)
+reg = ak.AutoModel(
+    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
+)
+reg.fit(x_train, y_train, epochs=2)
+
+"""
+The usage of [AutoModel](/auto_model/#automodel-class) is similar to the
+[functional API](https://www.tensorflow.org/guide/keras/functional) of Keras.
+Basically, you are building a graph, whose edges are blocks and the nodes are
+intermediate outputs of blocks.  To add an edge from `input_node` to
+`output_node` with `output_node = ak.[some_block]([block_args])(input_node)`.
+
+You can even also use more fine grained blocks to customize the search space
+even further. See the following example.
 """
 
 
 input_node = ak.TextInput()
-output_node = ak.TextBlock()(input_node)
+output_node = ak.TextToIntSequence()(input_node)
+output_node = ak.Embedding()(output_node)
+# Use separable Conv layers in Keras.
+output_node = ak.ConvBlock(separable=True)(output_node)
 output_node = ak.RegressionHead()(output_node)
 reg = ak.AutoModel(
     inputs=input_node, outputs=output_node, overwrite=True, max_trials=1
 )
-reg.fit(x_train, y_train, epochs=1, batch_size=2)
+reg.fit(x_train, y_train, epochs=2)
 
 """
 ## Data Format
@@ -135,23 +162,26 @@ format for the training data.
 
 
 train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(
-    2
+    32
 )
-test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)
+test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(32)
 
 reg = ak.TextRegressor(overwrite=True, max_trials=2)
 # Feed the tensorflow Dataset to the regressor.
-reg.fit(train_set.take(2), epochs=1)
+reg.fit(train_set, epochs=2)
 # Predict with the best model.
-predicted_y = reg.predict(test_set.take(2))
+predicted_y = reg.predict(test_set)
 # Evaluate the best model with testing data.
-print(reg.evaluate(test_set.take(2)))
+print(reg.evaluate(test_set))
 
 """
 ## Reference
 [TextRegressor](/text_regressor),
 [AutoModel](/auto_model/#automodel-class),
 [TextBlock](/block/#textblock-class),
+[TextToInteSequence](/block/#texttointsequence-class),
+[Embedding](/block/#embedding-class),
+[TextToNgramVector](/block/#texttongramvector-class),
 [ConvBlock](/block/#convblock-class),
 [TextInput](/node/#textinput-class),
 [RegressionHead](/block/#regressionhead-class).
diff --git a/docs/py/timeseries_forecaster.py b/docs/py/timeseries_forecaster.py
new file mode 100644
index 0000000..5eeda09
--- /dev/null
+++ b/docs/py/timeseries_forecaster.py
@@ -0,0 +1,123 @@
+"""shell
+pip install autokeras
+"""
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+To make this tutorial easy to follow, we use the UCI Airquality dataset, and try
+to forecast the AH value at the different timesteps. Some basic preprocessing
+has also been performed on the dataset as it required cleanup.
+
+## A Simple Example
+The first step is to prepare your data. Here we use the [UCI Airquality
+dataset](https://archive.ics.uci.edu/ml/datasets/Air+Quality) as an example.
+"""
+
+dataset = tf.keras.utils.get_file(
+    fname="AirQualityUCI.csv",
+    origin="https://archive.ics.uci.edu/ml/machine-learning-databases/00360/"
+    "AirQualityUCI.zip",
+    extract=True,
+)
+
+dataset = pd.read_csv(dataset, sep=";")
+dataset = dataset[dataset.columns[:-2]]
+dataset = dataset.dropna()
+dataset = dataset.replace(",", ".", regex=True)
+
+val_split = int(len(dataset) * 0.7)
+data_train = dataset[:val_split]
+validation_data = dataset[val_split:]
+
+data_x = data_train[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+data_x_val = validation_data[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+# Data with train data and the unseen data from subsequent time steps.
+data_x_test = dataset[
+    [
+        "CO(GT)",
+        "PT08.S1(CO)",
+        "NMHC(GT)",
+        "C6H6(GT)",
+        "PT08.S2(NMHC)",
+        "NOx(GT)",
+        "PT08.S3(NOx)",
+        "NO2(GT)",
+        "PT08.S4(NO2)",
+        "PT08.S5(O3)",
+        "T",
+        "RH",
+    ]
+].astype("float64")
+
+data_y = data_train["AH"].astype("float64")
+
+data_y_val = validation_data["AH"].astype("float64")
+
+print(data_x.shape)  # (6549, 12)
+print(data_y.shape)  # (6549,)
+
+"""
+The second step is to run the [TimeSeriesForecaster](/time_series_forecaster).
+As a quick demo, we set epochs to 10.
+You can also leave the epochs unspecified for an adaptive number of epochs.
+"""
+
+predict_from = 1
+predict_until = 10
+lookback = 3
+clf = ak.TimeseriesForecaster(
+    lookback=lookback,
+    predict_from=predict_from,
+    predict_until=predict_until,
+    max_trials=1,
+    objective="val_loss",
+)
+# Train the TimeSeriesForecaster with train data
+clf.fit(
+    x=data_x,
+    y=data_y,
+    validation_data=(data_x_val, data_y_val),
+    batch_size=32,
+    epochs=10,
+)
+# Predict with the best model(includes original training data).
+predictions = clf.predict(data_x_test)
+print(predictions.shape)
+# Evaluate the best model with testing data.
+print(clf.evaluate(data_x_val, data_y_val))
diff --git a/docs/templates/benchmarks.md b/docs/templates/benchmarks.md
index d77c382..4f70a6d 100644
--- a/docs/templates/benchmarks.md
+++ b/docs/templates/benchmarks.md
@@ -7,4 +7,6 @@ Tested on a single NVIDIA Tesla V100 GPU.
 | - | - | - | - | - |
 | [MNIST](http://yann.lecun.com/exdb/mnist/)  | ImageClassifier| Accuracy | 99.04% | 0.51 |
 | [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)   | ImageClassifier| Accuracy | 97.10% | 1.8 |
-| [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)  | TextClassifier | Accuracy | 93.93% | 1.2 |
\ No newline at end of file
+| [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/)  | TextClassifier | Accuracy | 93.93% | 1.2 |
+| [Titanic](https://www.tensorflow.org/datasets/catalog/titanic)  | StructuredDataClassifier | Accuracy | 82.20% | 0.007 |
+| [California Housing](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset)  | StructuredDataRegression | MSE | 0.23 | 0.06 |
\ No newline at end of file
diff --git a/docs/templates/tutorial/faq.md b/docs/templates/tutorial/faq.md
index 57c1b2a..af4d615 100644
--- a/docs/templates/tutorial/faq.md
+++ b/docs/templates/tutorial/faq.md
@@ -55,7 +55,7 @@ clf = ak.ImageClassifier(
 
 ## How to use multiple GPUs?
 You can use the `distribution_strategy` argument when initializing any model you created with AutoKeras,
-like AutoModel, ImageClassifier and so on. This argument is supported by Keras Tuner.
+like AutoModel, ImageClassifier, StructuredDataRegressor and so on. This argument is supported by Keras Tuner.
 AutoKeras supports the arguments supported by Keras Tuner.
 Please see the discription of the argument [here](https://keras-team.github.io/keras-tuner/documentation/tuners/#tuner-class).
 
diff --git a/docs/templates/tutorial/overview.md b/docs/templates/tutorial/overview.md
index a472b8b..4776554 100644
--- a/docs/templates/tutorial/overview.md
+++ b/docs/templates/tutorial/overview.md
@@ -15,6 +15,12 @@ You can click the links below to see the detailed tutorial for each task.
 
 [Text Regression](/tutorial/text_regression)
 
+[Structured Data Classification](/tutorial/structured_data_classification)
+
+[Structured Data Regression](/tutorial/structured_data_regression)
+
+**Coming Soon**: Time Series Forecasting, Object Detection, Image Segmentation.
+
 
 ## Multi-Task and Multi-Modal Data
 
@@ -36,6 +42,8 @@ The following are the links to the documentation of the predefined input nodes a
 
 [Input](/node/#input-class)
 
+[StructuredDataInput](/node/#structureddatainput-class)
+
 [TextInput](/node/#textinput-class)
 
 **Blocks**:
@@ -44,10 +52,18 @@ The following are the links to the documentation of the predefined input nodes a
 
 [Normalization](/block/#normalization-class)
 
+[TextToIntSequence](/block/#texttointsequence-class)
+
+[TextToNgramVector](/block/#texttongramvector-class)
+
+[CategoricalToNumerical](/block/#categoricaltonumerical-class)
+
 [ConvBlock](/block/#convblock-class)
 
 [DenseBlock](/block/#denseblock-class)
 
+[Embedding](/block/#embedding-class)
+
 [Merge](/block/#merge-class)
 
 [ResNetBlock](/block/#resnetblock-class)
@@ -62,6 +78,8 @@ The following are the links to the documentation of the predefined input nodes a
 
 [ImageBlock](/block/#imageblock-class)
 
+[StructuredDataBlock](/block/#structureddatablock-class)
+
 [TextBlock](/block/#textblock-class)
 
 [ClassificationHead](/block/#classificationhead-class)
diff --git a/docs/tutobooks.py b/docs/tutobooks.py
index 76226a1..502a739 100644
--- a/docs/tutobooks.py
+++ b/docs/tutobooks.py
@@ -60,7 +60,6 @@ you expect. If not, keep editing `your_example.py` until it does.
 
 Finally, submit a PR adding `examples/your_example.py`.
 """
-
 import json
 import os
 import random
diff --git a/examples/automodel_with_cnn.py b/examples/automodel_with_cnn.py
index 9c26056..d7ec20b 100644
--- a/examples/automodel_with_cnn.py
+++ b/examples/automodel_with_cnn.py
@@ -1,6 +1,6 @@
 # Library import
-import keras
 import numpy as np
+import tensorflow as tf
 
 import autokeras as ak
 
@@ -45,6 +45,6 @@ model = auto_model.export_model()
 print(type(model.summary()))
 
 # print model as image
-keras.utils.plot_model(
-    model, show_shapes=True, expand_treeed=True, to_file="name.png"
+tf.keras.utils.plot_model(
+    model, show_shapes=True, expand_nested=True, to_file="name.png"
 )
diff --git a/examples/celeb_age.py b/examples/celeb_age.py
index 5a1bb9b..b4446ba 100644
--- a/examples/celeb_age.py
+++ b/examples/celeb_age.py
@@ -13,7 +13,6 @@ First, prepare your image data in a numpy.ndarray or tensorflow.Dataset format.
 Each image must have the same shape, meaning each has the same width, height,
 and color channels as other images in the set.
 """
-
 from datetime import datetime
 from datetime import timedelta
 
diff --git a/examples/cifar10.py b/examples/cifar10.py
index 4555a0a..75a28df 100644
--- a/examples/cifar10.py
+++ b/examples/cifar10.py
@@ -1,4 +1,4 @@
-from keras.datasets import cifar10
+from tensorflow.keras.datasets import cifar10
 
 import autokeras as ak
 
diff --git a/examples/imdb.py b/examples/imdb.py
index cb024e4..22f48fe 100644
--- a/examples/imdb.py
+++ b/examples/imdb.py
@@ -3,9 +3,8 @@ Search for a good model for the
 [IMDB](
 https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) dataset.
 """
-
-import keras
 import numpy as np
+import tensorflow as tf
 
 import autokeras as ak
 
@@ -14,7 +13,7 @@ def imdb_raw():
     max_features = 20000
     index_offset = 3  # word index offset
 
-    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(
+    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(
         num_words=max_features, index_from=index_offset
     )
     x_train = x_train
@@ -22,7 +21,7 @@ def imdb_raw():
     x_test = x_test
     y_test = y_test.reshape(-1, 1)
 
-    word_to_id = keras.datasets.imdb.get_word_index()
+    word_to_id = tf.keras.datasets.imdb.get_word_index()
     word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}
     word_to_id["<PAD>"] = 0
     word_to_id["<START>"] = 1
diff --git a/examples/iris.py b/examples/iris.py
new file mode 100644
index 0000000..70a1dda
--- /dev/null
+++ b/examples/iris.py
@@ -0,0 +1,71 @@
+"""shell
+pip install -q -U autokeras==1.0.5
+pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
+"""
+
+import os
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+"""
+Search for a good model for the
+[iris](https://www.tensorflow.org/datasets/catalog/iris) dataset.
+"""
+
+
+# Prepare the dataset.
+train_dataset_url = "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv"  # noqa: E501
+train_dataset_fp = tf.keras.utils.get_file(
+    fname=os.path.basename(train_dataset_url), origin=train_dataset_url
+)
+
+test_dataset_url = (
+    "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv"
+)
+test_dataset_fp = tf.keras.utils.get_file(
+    fname=os.path.basename(test_dataset_url), origin=test_dataset_url
+)
+
+column_names = [
+    "sepal_length",
+    "sepal_width",
+    "petal_length",
+    "petal_width",
+    "species",
+]
+feature_names = column_names[:-1]
+label_name = column_names[-1]
+class_names = ["Iris setosa", "Iris versicolor", "Iris virginica"]
+
+train = pd.read_csv(train_dataset_fp, names=column_names, header=0)
+
+test = pd.read_csv(test_dataset_fp, names=column_names, header=0)
+
+print(train.shape)  # (120, 5)
+print(test.shape)  # (30, 5)
+
+# Initialize the StructuredDataClassifier.
+clf = ak.StructuredDataClassifier(
+    max_trials=5,
+    overwrite=True,
+)
+# Search for the best model with EarlyStopping.
+cbs = [
+    tf.keras.callbacks.EarlyStopping(patience=3),
+]
+
+clf.fit(
+    x=train[feature_names],
+    y=train[label_name],
+    epochs=200,
+    callbacks=cbs,
+)
+# Evaluate on the testing data.
+print(
+    "Accuracy: {accuracy}".format(
+        accuracy=clf.evaluate(x=test[feature_names], y=test[label_name])
+    )
+)
diff --git a/examples/mnist.py b/examples/mnist.py
index 26c2472..01baaef 100644
--- a/examples/mnist.py
+++ b/examples/mnist.py
@@ -4,7 +4,7 @@ Search for a good model for the
 dataset.
 """
 
-from keras.datasets import mnist
+from tensorflow.keras.datasets import mnist
 
 import autokeras as ak
 
diff --git a/examples/new_pop.py b/examples/new_pop.py
index b385cfd..7bf9551 100644
--- a/examples/new_pop.py
+++ b/examples/new_pop.py
@@ -1,7 +1,6 @@
 """shell
 pip install autokeras
 """
-
 import pandas as pd
 
 import autokeras as ak
@@ -43,3 +42,28 @@ reg.fit(text_inputs, media_success_outputs)
 
 # Predict with the chosen model:
 predict_y = reg.predict(text_inputs)
+
+"""
+If your text source has a larger vocabulary (number of distinct words), you may
+need to create a custom pipeline in AutoKeras to increase the `max_tokens`
+parameter.
+"""
+
+text_input = (df.Title + " " + df.Headline).to_numpy(dtype="str")
+
+# text input and tokenization
+input_node = ak.TextInput()
+output_node = ak.TextToIntSequence(max_tokens=20000)(input_node)
+
+# regression output
+output_node = ak.RegressionHead()(output_node)
+
+# initialize AutoKeras and find the best model
+reg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=15)
+reg.fit(text_input, media_success_outputs)
+
+"""
+Measure the accuracy of the regressor on an independent test set:
+"""
+
+print(reg.evaluate(text_input, media_success_outputs))
diff --git a/examples/reuters.py b/examples/reuters.py
index 794ba32..27c5abe 100644
--- a/examples/reuters.py
+++ b/examples/reuters.py
@@ -3,10 +3,9 @@
 !pip install -q -U autokeras==1.0.8
 !pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
 """
-
-import keras
 import numpy as np
-from keras.datasets import reuters
+import tensorflow as tf
+from tensorflow.keras.datasets import reuters
 
 import autokeras as ak
 
@@ -60,7 +59,7 @@ clf = ak.TextClassifier(
 
 # Callback to avoid overfitting with the EarlyStopping.
 cbs = [
-    keras.callbacks.EarlyStopping(patience=3),
+    tf.keras.callbacks.EarlyStopping(patience=3),
 ]
 
 # Search for the best model.
diff --git a/examples/titanic.py b/examples/titanic.py
new file mode 100644
index 0000000..979a2b9
--- /dev/null
+++ b/examples/titanic.py
@@ -0,0 +1,41 @@
+"""
+Search for a good model for the [Titanic](https://www.kaggle.com/c/titanic)
+dataset.
+"""
+
+import timeit
+
+import tensorflow as tf
+
+import autokeras as ak
+
+TRAIN_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/train.csv"
+TEST_DATA_URL = "https://storage.googleapis.com/tf-datasets/titanic/eval.csv"
+
+
+def main():
+    # Initialize the classifier.
+    train_file_path = tf.keras.utils.get_file("train.csv", TRAIN_DATA_URL)
+    test_file_path = tf.keras.utils.get_file("eval.csv", TEST_DATA_URL)
+    clf = ak.StructuredDataClassifier(
+        max_trials=10, directory="tmp_dir", overwrite=True
+    )
+
+    start_time = timeit.default_timer()
+    # x is the path to the csv file. y is the column name of the column to
+    # predict.
+    clf.fit(train_file_path, "survived")
+    stop_time = timeit.default_timer()
+
+    # Evaluate the accuracy of the found model.
+    accuracy = clf.evaluate(test_file_path, "survived")[1]
+    print("Accuracy: {accuracy}%".format(accuracy=round(accuracy * 100, 2)))
+    print(
+        "Total time: {time} seconds.".format(
+            time=round(stop_time - start_time, 2)
+        )
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/wine.py b/examples/wine.py
new file mode 100644
index 0000000..34f0cee
--- /dev/null
+++ b/examples/wine.py
@@ -0,0 +1,67 @@
+"""
+Run the following commands first
+pip3 install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1
+pip3 install autokeras==1.0.5
+
+This Script searches for a model for the wine dataset
+Source and Description of data:
+"""
+import os
+
+import pandas as pd
+import tensorflow as tf
+
+import autokeras as ak
+
+dataset_url = (
+    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
+)
+
+# save data
+data_file_path = tf.keras.utils.get_file(
+    fname=os.path.basename(dataset_url), origin=dataset_url
+)
+
+column_names = [
+    "Wine",
+    "Alcohol",
+    "Malic.acid",
+    "Ash",
+    "Acl",
+    "Mg",
+    "Phenols",
+    "Flavanoids",
+    "Nonflavanoid.phenols",
+    "Proanth",
+    "Color.int",
+    "Hue",
+    "OD",
+    "Proline",
+]
+
+feature_names = column_names[1:]
+label_name = column_names[0]  # Wine
+
+data = pd.read_csv(data_file_path, header=0, names=column_names)
+# Shuffling
+data = data.sample(frac=1)
+
+split_length = int(data.shape[0] * 0.8)  # 141
+
+# train and test
+train_data = data.iloc[:split_length]
+test_data = data.iloc[split_length:]
+
+
+# Initialize the classifier.
+clf = ak.StructuredDataClassifier(max_trials=5)
+
+# Evaluate
+clf.fit(x=train_data[feature_names], y=train_data[label_name])
+print(
+    "Accuracy: {accuracy}".format(
+        accuracy=clf.evaluate(
+            x=test_data[feature_names], y=test_data[label_name]
+        )
+    )
+)