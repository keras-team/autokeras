{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_2","title":"Home","text":""},{"location":"#_3","title":"Home","text":"<p>AutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&amp;M University. The goal of AutoKeras is to make machine learning accessible to everyone.</p>"},{"location":"#learning-resources","title":"Learning resources","text":"<ul> <li>A short example.</li> </ul> <pre><code>import autokeras as ak\n\nclf = ak.ImageClassifier()\nclf.fit(x_train, y_train)\nresults = clf.predict(x_test)\n</code></pre> <ul> <li>Official website tutorials.</li> <li>The book of Automated Machine Learning in Action.</li> <li>The LiveProjects of Image Classification with AutoKeras.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the package, please use the <code>pip</code> installation as follows:</p> <pre><code>pip3 install autokeras\n</code></pre> <p>Please follow the installation guide for more details.</p> <p>Note: Currently, AutoKeras is only compatible with Python &gt;= 3.7 and TensorFlow &gt;= 2.8.0.</p>"},{"location":"#community","title":"Community","text":"<p>Ask your questions on our GitHub Discussions.</p>"},{"location":"#contributing-code","title":"Contributing Code","text":"<p>Here is how we manage our project.</p> <p>We pick the critical issues to work on from GitHub issues. They will be added to this Project. Some of the issues will then be added to the milestones, which are used to plan for the releases.</p> <p>Refer to our Contributing Guide to learn the best practices.</p> <p>Thank all the contributors!</p> <p></p>"},{"location":"#cite-this-work","title":"Cite this work","text":"<p>Haifeng Jin, Fran\u00e7ois Chollet, Qingquan Song, and Xia Hu. \"AutoKeras: An AutoML Library for Deep Learning.\" the Journal of machine Learning research 6 (2023): 1-6. (Download)</p> <p>Biblatex entry:</p> <pre><code>@article{JMLR:v24:20-1355,\n  author  = {Haifeng Jin and Fran\u00e7ois Chollet and Qingquan Song and Xia Hu},\n  title   = {AutoKeras: An AutoML Library for Deep Learning},\n  journal = {Journal of Machine Learning Research},\n  year    = {2023},\n  volume  = {24},\n  number  = {6},\n  pages   = {1--6},\n  url     = {http://jmlr.org/papers/v24/20-1355.html}\n}\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&amp;M College of Engineering, and Texas A&amp;M University.</p>"},{"location":"about/","title":"About","text":"<p>This package is developed by DATA LAB at Texas A&amp;M University, collaborating with keras-team for version 1.0 and above.</p>"},{"location":"about/#core-team","title":"Core Team","text":"<p>Haifeng Jin: Created, designed and implemented the AutoKeras system.  Maintainer.</p> <p>Fran\u00e7ois Chollet: The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests.</p> <p>Qingquan Song: Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module.</p> <p>Xia \"Ben\" Hu: Project lead and maintainer.</p>"},{"location":"auto_model/","title":"AutoModel","text":"<p>[source]</p>"},{"location":"auto_model/#automodel","title":"AutoModel","text":"<pre><code>autokeras.AutoModel(\n    inputs,\n    outputs,\n    project_name=\"auto_model\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=\"greedy\",\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has <code>fit()</code> and  <code>predict()</code> methods.</p> <p>The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API.</p> <p>Exampl</p>"},{"location":"auto_model/#example","title":"Example","text":"<p><pre><code>    # The user only specifies the input nodes and output heads.\n    import autokeras as ak\n    ak.AutoModel(\n        inputs=[ak.ImageInput(), ak.TextInput()],\n        outputs=[ak.ClassificationHead(), ak.RegressionHead()]\n    )\n</code></pre> <pre><code>    # The user specifies the high-level architecture.\n    import autokeras as ak\n    image_input = ak.ImageInput()\n    image_output = ak.ImageBlock()(image_input)\n    text_input = ak.TextInput()\n    text_output = ak.TextBlock()(text_input)\n    output = ak.Merge()([image_output, text_output])\n    classification_output = ak.ClassificationHead()(output)\n    regression_output = ak.RegressionHead()(output)\n    ak.AutoModel(\n        inputs=[image_input, text_input],\n        outputs=[classification_output, regression_output]\n    )\n</code></pre></p> <p>Arguments</p> <ul> <li>inputs <code>autokeras.Input | List[autokeras.Input]</code>: A list of Node instances.     The input node(s) of the AutoModel.</li> <li>outputs <code>autokeras.Head | autokeras.Node | list</code>: A list of Node or Head instances.     The output node(s) or head(s) of the AutoModel.</li> <li>project_name <code>str</code>: String. The name of the AutoModel. Defaults to     'auto_model'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner]</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. Defaults to 'greedy'.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by keras_tuner.Tuner.</li> </ul> <p>[source]</p>"},{"location":"auto_model/#fit","title":"fit","text":"<pre><code>AutoModel.fit(\n    x=None,\n    y=None,\n    batch_size=32,\n    epochs=None,\n    callbacks=None,\n    validation_split=0.2,\n    validation_data=None,\n    verbose=1,\n    **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray. Training data x.</li> <li>y: numpy.ndarray. Training data y.</li> <li>batch_size: Int. Number of samples per gradient update. Defaults to     32.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2.     Fraction of the training data to be used as validation data.     The model will set apart this fraction of the training data,     will not train on it, and will evaluate the loss and any model     metrics on this data at the end of each epoch.  The validation     data is selected from the last samples in the <code>x</code> and <code>y</code> data     provided, before shuffling. This argument is not supported when     <code>x</code> is a dataset. The best model found would be fit on the     entire dataset including the validation data.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar,     2 = one line per epoch. Note that the progress bar is not     particularly useful when logged to a file, so verbose=2 is     recommended when not running interactively (eg, in a production     environment). Controls the verbosity of both KerasTuner search     and     keras.Model.fit</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"auto_model/#predict","title":"predict","text":"<pre><code>AutoModel.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"auto_model/#evaluate","title":"evaluate","text":"<pre><code>AutoModel.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"auto_model/#export_model","title":"export_model","text":"<pre><code>AutoModel.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"base/","title":"Base Class","text":"<p>[source]</p>"},{"location":"base/#node","title":"Node","text":"<pre><code>autokeras.Node(**kwargs)\n</code></pre> <p>The nodes in a network connecting the blocks.</p> <p>[source]</p>"},{"location":"base/#block","title":"Block","text":"<pre><code>autokeras.Block(**kwargs)\n</code></pre> <p>The base class for different Block.</p> <p>The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user.</p> <p>[source]</p>"},{"location":"base/#build","title":"build","text":"<pre><code>Block.build(hp, inputs=None)\n</code></pre> <p>Build the Block into a real Keras Model.</p> <p>The subclasses should override this function and return the output node.</p> <p>Arguments</p> <ul> <li>hp: HyperParameters. The hyperparameters for building the model.</li> <li>inputs: A list of input node(s).</li> </ul> <p>[source]</p>"},{"location":"base/#head","title":"Head","text":"<pre><code>autokeras.Head(loss=None, metrics=None, **kwargs)\n</code></pre> <p>Base class for the heads, e.g. classification, regression.</p> <p>Arguments</p> <ul> <li>loss <code>str | Callable | keras.losses.Loss | None</code>: A Keras loss function. Defaults to None. If None, the loss will be     inferred from the AutoModel.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to None. If None, the metrics     will be inferred from the AutoModel.</li> </ul>"},{"location":"block/","title":"Block","text":"<p>[source]</p>"},{"location":"block/#convblock","title":"ConvBlock","text":"<pre><code>autokeras.ConvBlock(\n    kernel_size=None,\n    num_blocks=None,\n    num_layers=None,\n    filters=None,\n    max_pooling=None,\n    separable=None,\n    dropout=None,\n    **kwargs\n)\n</code></pre> <p>Block for vanilla ConvNets.</p> <p>Arguments</p> <ul> <li>kernel_size <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The size of the kernel.     If left unspecified, it will be tuned automatically.</li> <li>num_blocks <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of conv blocks, each of which may contain     convolutional, max pooling, dropout, and activation. If left     unspecified, it will be tuned automatically.</li> <li>num_layers <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or hyperparameters.Choice.     The number of convolutional layers in each block. If left     unspecified, it will be tuned automatically.</li> <li>filters <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice. The number of     filters in the convolutional layers. If left unspecified, it will     be tuned automatically.</li> <li>max_pooling <code>bool | None</code>: Boolean. Whether to use max pooling layer in each block. If     left unspecified, it will be tuned automatically.</li> <li>separable <code>bool | None</code>: Boolean. Whether to use separable conv layers.     If left unspecified, it will be tuned automatically.</li> <li>dropout <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or kerastuner.engine.hyperparameters.     Choice range Between 0 and 1.     The dropout rate after convolutional layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#denseblock","title":"DenseBlock","text":"<pre><code>autokeras.DenseBlock(num_layers=None, num_units=None, use_batchnorm=None, dropout=None, **kwargs)\n</code></pre> <p>Block for Dense layers.</p> <p>Arguments</p> <ul> <li>num_layers <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of Dense layers in the block.     If left unspecified, it will be tuned automatically.</li> <li>num_units <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of units in each dense layer.     If left unspecified, it will be tuned automatically.</li> <li>use_bn: Boolean. Whether to use BatchNormalization layers.     If left unspecified, it will be tuned automatically.</li> <li>dropout <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or keras_tuner.engine.hyperparameters.Choice.     The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#embedding","title":"Embedding","text":"<pre><code>autokeras.Embedding(max_features=20001, embedding_dim=None, dropout=None, **kwargs)\n</code></pre> <p>Word embedding block for sequences.</p> <p>The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word.</p> <p>Arguments</p> <ul> <li>max_features <code>int</code>: Int. Size of the vocabulary. Must be set if not using     TextToIntSequence before this block. Defaults to 20001.</li> <li>embedding_dim <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     Output dimension of the Attention block.     If left unspecified, it will be tuned automatically.</li> <li>dropout <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or keras_tuner.engine.hyperparameters.Choice.     The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#merge","title":"Merge","text":"<pre><code>autokeras.Merge(merge_type=None, **kwargs)\n</code></pre> <p>Merge block to merge multiple nodes into one.</p> <p>Arguments</p> <ul> <li>merge_type <code>str | None</code>: String. 'add' or 'concatenate'. If left unspecified, it will     be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#resnetblock","title":"ResNetBlock","text":"<pre><code>autokeras.ResNetBlock(version=None, pretrained=None, **kwargs)\n</code></pre> <p>Block for ResNet.</p> <p>Arguments</p> <ul> <li>version <code>str | None</code>: String. 'v1', 'v2'. The type of ResNet to use.     If left unspecified, it will be tuned automatically.</li> <li>pretrained <code>bool | None</code>: Boolean. Whether to use ImageNet pretrained weights.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#rnnblock","title":"RNNBlock","text":"<pre><code>autokeras.RNNBlock(\n    return_sequences=False, bidirectional=None, num_layers=None, layer_type=None, **kwargs\n)\n</code></pre> <p>An RNN Block.</p> <p>Arguments</p> <ul> <li>return_sequences <code>bool</code>: Boolean. Whether to return the last output in the     output sequence, or the full sequence. Defaults to False.</li> <li>bidirectional <code>bool | keras_tuner.src.engine.hyperparameters.hp_types.boolean_hp.Boolean | None</code>: Boolean or keras_tuner.engine.hyperparameters.Boolean.     Bidirectional RNN. If left unspecified, it will be     tuned automatically.</li> <li>num_layers <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of layers in RNN. If left unspecified, it will     be tuned automatically.</li> <li>layer_type <code>str | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: String or or keras_tuner.engine.hyperparameters.Choice.     'gru' or 'lstm'. If left unspecified, it will be tuned     automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#spatialreduction","title":"SpatialReduction","text":"<pre><code>autokeras.SpatialReduction(reduction_type=None, **kwargs)\n</code></pre> <p>Reduce the dimension of a spatial tensor, e.g. image, to a vector.</p> <p>Arguments</p> <ul> <li>reduction_type <code>str | None</code>: String. 'flatten', 'global_max' or 'global_avg'.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#temporalreduction","title":"TemporalReduction","text":"<pre><code>autokeras.TemporalReduction(reduction_type=None, **kwargs)\n</code></pre> <p>Reduce the dim of a temporal tensor, e.g. output of RNN, to a vector.</p> <p>Arguments</p> <ul> <li>reduction_type <code>str | None</code>: String. 'flatten', 'global_max' or 'global_avg'. If left     unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#xceptionblock","title":"XceptionBlock","text":"<pre><code>autokeras.XceptionBlock(pretrained=None, **kwargs)\n</code></pre> <p>Block for XceptionNet.</p> <p>An Xception structure, used for specifying your model with specific datasets.</p> <p>The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow.</p> <p>This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by <code>HyperParameters</code>, to get an architecture with a half, an identical, or a double size of the original one.</p> <p>Arguments</p> <ul> <li>pretrained <code>bool | None</code>: Boolean. Whether to use ImageNet pretrained weights.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#structureddatablock","title":"StructuredDataBlock","text":"<pre><code>autokeras.StructuredDataBlock(normalize=None, **kwargs)\n</code></pre> <p>Block for structured data.</p> <p>Arguments</p> <ul> <li>categorical_encoding: Boolean. Whether to use the CategoricalToNumerical     to encode the categorical features to numerical features. Defaults     to True.</li> <li>normalize <code>bool | None</code>: Boolean. Whether to normalize the features.     If unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#imageblock","title":"ImageBlock","text":"<pre><code>autokeras.ImageBlock(block_type=None, normalize=None, augment=None, **kwargs)\n</code></pre> <p>Block for image data.</p> <p>The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'.</p> <p>Arguments</p> <ul> <li>block_type <code>str | None</code>: String. 'resnet', 'xception', 'vanilla'. The type of Block     to use. If unspecified, it will be tuned automatically.</li> <li>normalize <code>bool | None</code>: Boolean. Whether to channel-wise normalize the images.     If unspecified, it will be tuned automatically.</li> <li>augment <code>bool | None</code>: Boolean. Whether to do image augmentation. If unspecified,     it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#textblock","title":"TextBlock","text":"<pre><code>autokeras.TextBlock(max_tokens=None, **kwargs)\n</code></pre> <p>Block for text data.</p> <p>Arguments</p> <ul> <li>max_tokens: Int. The maximum size of the vocabulary.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#imageaugmentation","title":"ImageAugmentation","text":"<pre><code>autokeras.ImageAugmentation(\n    translation_factor=None,\n    vertical_flip=None,\n    horizontal_flip=None,\n    rotation_factor=None,\n    zoom_factor=None,\n    contrast_factor=None,\n    **kwargs\n)\n</code></pre> <p>Collection of various image augmentation methods.</p> <p>Arguments</p> <ul> <li>translation_factor <code>float | Tuple[float, float] | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: A positive float represented as fraction value, or a     tuple of 2 representing fraction for translation vertically and     horizontally, or a kerastuner.engine.hyperparameters.Choice range     of positive floats. For instance, <code>translation_factor=0.2</code> result     in a random translation factor within 20% of the width and height.     If left unspecified, it will be tuned automatically.</li> <li>vertical_flip <code>bool | None</code>: Boolean. Whether to flip the image vertically.     If left unspecified, it will be tuned automatically.</li> <li>horizontal_flip <code>bool | None</code>: Boolean. Whether to flip the image horizontally.     If left unspecified, it will be tuned automatically.</li> <li>rotation_factor <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or kerastuner.engine.hyperparameters.Choice range     between [0, 1]. A positive float represented as fraction of 2pi     upper bound for rotating clockwise and counter-clockwise. When     represented as a single float, lower = upper.     If left unspecified, it will be tuned automatically.</li> <li>zoom_factor <code>float | Tuple[float, float] | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: A positive float represented as fraction value, or a tuple     of 2 representing fraction for zooming vertically and horizontally,     or a kerastuner.engine.hyperparameters.Choice range of positive     floats.  For instance, <code>zoom_factor=0.2</code> result in a random zoom     factor from 80% to 120%. If left unspecified, it will be tuned     automatically.</li> <li>contrast_factor <code>float | Tuple[float, float] | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: A positive float represented as fraction of value, or a     tuple of size 2 representing lower and upper bound, or a     kerastuner.engine.hyperparameters.Choice range of floats to find the     optimal value. When represented as a single float, lower = upper.     The contrast factor will be randomly picked     between [1.0 - lower, 1.0 + upper]. If left unspecified, it will be     tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#normalization","title":"Normalization","text":"<pre><code>autokeras.Normalization(axis=-1, **kwargs)\n</code></pre> <p>Perform feature-wise normalization on data.</p> <p>Refer to Normalization layer in keras preprocessing layers for more information.</p> <p>Arguments</p> <ul> <li>axis <code>int</code>: Integer or tuple of integers, the axis or axes that should be     normalized (typically the features axis). We will normalize each     element in the specified axis. The default is '-1' (the innermost     axis); 0 (the batch axis) is not allowed.</li> </ul> <p>[source]</p>"},{"location":"block/#classificationhead","title":"ClassificationHead","text":"<pre><code>autokeras.ClassificationHead(\n    num_classes=None, multi_label=False, loss=None, metrics=None, dropout=None, **kwargs\n)\n</code></pre> <p>Classification Dense layers.</p> <p>Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default.</p> <p>The targets passing to the head would have to be np.ndarray. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification.</p> <p>The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found.</p> <p>Arguments</p> <ul> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | keras.losses.Loss | None</code>: A Keras loss function. Defaults to use <code>binary_crossentropy</code> or     <code>categorical_crossentropy</code> based on the number of classes.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>dropout <code>float | None</code>: Float. The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#regressionhead","title":"RegressionHead","text":"<pre><code>autokeras.RegressionHead(\n    output_dim=None, loss=\"mean_squared_error\", metrics=None, dropout=None, **kwargs\n)\n</code></pre> <p>Regression Dense layers.</p> <p>The targets passing to the head would have to be np.ndarray. It can be single-column or multi-column. The values should all be numerical.</p> <p>Arguments</p> <ul> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>multi_label: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | keras.losses.Loss</code>: A Keras loss function. Defaults to use <code>mean_squared_error</code>.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use <code>mean_squared_error</code>.</li> <li>dropout <code>float | None</code>: Float. The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Contributions are welcome, and greatly appreciated! This page is only a guide of the best practices of contributing code to AutoKeras. The best way to contribute is to join our community by reading this. We will get you started right away.</p> <p>Follow the tag of good first issue for the issues for beginner.</p>"},{"location":"contributing/#pull-request-guide","title":"Pull Request Guide","text":"<ol> <li> <p>Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project.</p> </li> <li> <p>Include \"resolves #issue_number\" in the description of the pull request if applicable. Briefly describe your contribution.</p> </li> <li> <p>Submit the pull request from the first day of your development and create it as a draft pull request. Click <code>ready for review</code> when finished and passed the all the checks.</p> </li> <li> <p>For the case of bug fixes, add new test cases which would fail before your bug fix.</p> </li> </ol>"},{"location":"contributing/#setup-environment","title":"Setup Environment","text":"<p>We introduce 3 different options: GitHub Codespaces, VS Code &amp; Dev Containers, the general setup. You can choose base on your preference.</p>"},{"location":"contributing/#option-1-github-codespaces","title":"Option 1: GitHub Codespaces","text":"<p>You can simply open the repository in GitHub Codespaces. The environment is already setup there.</p>"},{"location":"contributing/#option-2-vs-code-dev-containers","title":"Option 2: VS Code &amp; Dev Containers","text":"<p>Open VS Code. Install the <code>Dev Containers</code> extension. Press <code>F1</code> key. Enter <code>Dev Containers: Open Folder in Container...</code> to open the repository root folder. The environment is already setup there.</p>"},{"location":"contributing/#option-3-the-general-setup","title":"Option 3: The General Setup","text":"<p>Install Virtualenvwrapper. Create a new virtualenv named <code>ak</code> based on python3. <pre><code>mkvirtualenv -p python3 ak \n</code></pre> Please use this virtualenv for development.</p> <p>Clone the repo. Go to the repo directory. Run the following commands. <pre><code>workon ak\n\npip install -e \".[tests]\"\npip uninstall autokeras\nadd2virtualenv .\n</code></pre></p>"},{"location":"contributing/#run-tests","title":"Run Tests","text":""},{"location":"contributing/#github-codespaces-or-vs-code-dev-containers","title":"GitHub Codespaces or VS Code &amp; Dev Containers","text":"<p>If you are using \"GitHub Codespaces\" or \"VS Code &amp; Dev Containers\", you can simply open any <code>*_test.py</code> file under the <code>tests</code> directory, and wait a few seconds, you will see the test tab on the left of the window.</p>"},{"location":"contributing/#general-setup","title":"General Setup","text":"<p>If you are using the general setup.</p> <p>Activate the virtualenv. Go to the repo directory Run the following lines to run the tests.</p> <p>Run all the tests. <pre><code>pytest tests\n</code></pre></p> <p>Run all the unit tests. <pre><code>pytest tests/autokeras\n</code></pre></p> <p>Run all the integration tests. <pre><code>pytest tests/integration_tests\n</code></pre></p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>You can run the following manually every time you want to format your code. 1. Run <code>shell/format.sh</code> to format your code. 2. Run <code>shell/lint.sh</code> to check.</p>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Docstrings should follow our style. Just check the style of other docstrings in AutoKeras.</p>"},{"location":"image_classifier/","title":"ImageClassifier","text":"<p>[source]</p>"},{"location":"image_classifier/#imageclassifier","title":"ImageClassifier","text":"<pre><code>autokeras.ImageClassifier(\n    num_classes=None,\n    multi_label=False,\n    loss=None,\n    metrics=None,\n    project_name=\"image_classifier\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras image classification class.</p> <p>Arguments</p> <ul> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | keras.losses.Loss | None</code>: A Keras loss function. Defaults to use 'binary_crossentropy' or     'categorical_crossentropy' based on the number of classes.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'image_classifier'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs.  Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"image_classifier/#fit","title":"fit","text":"<pre><code>ImageClassifier.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x <code>numpy.ndarray | None</code>: numpy.ndarray. Training data x. The shape     of the data should be (samples, width, height) or (samples,     width, height, channels).</li> <li>y <code>numpy.ndarray | None</code>: numpy.ndarray. Training data y. It can be     raw labels, one-hot encoded if more than two classes, or binary     encoded for binary classification.</li> <li>epochs <code>int | None</code>: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks <code>List[keras.callbacks.Callback] | None</code>: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split <code>float | None</code>: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset. The best model found would be fit on the entire dataset     including the validation data.</li> <li>validation_data <code>Tuple[numpy.ndarray, numpy.ndarray] | None</code>: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training loss     values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"image_classifier/#predict","title":"predict","text":"<pre><code>ImageClassifier.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"image_classifier/#evaluate","title":"evaluate","text":"<pre><code>ImageClassifier.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"image_classifier/#export_model","title":"export_model","text":"<pre><code>ImageClassifier.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"image_regressor/","title":"ImageRegressor","text":"<p>[source]</p>"},{"location":"image_regressor/#imageregressor","title":"ImageRegressor","text":"<pre><code>autokeras.ImageRegressor(\n    output_dim=None,\n    loss=\"mean_squared_error\",\n    metrics=None,\n    project_name=\"image_regressor\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras image regression class.</p> <p>Arguments</p> <ul> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>loss <code>str | Callable | keras.losses.Loss</code>: A Keras loss function. Defaults to use 'mean_squared_error'.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'mean_squared_error'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'image_regressor'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"image_regressor/#fit","title":"fit","text":"<pre><code>ImageRegressor.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x <code>numpy.ndarray | None</code>: numpy.ndarray. Training data x. The shape     of the data should be (samples, width, height) or (samples,     width, height, channels).</li> <li>y <code>numpy.ndarray | None</code>: numpy.ndarray. Training data y. The targets     passing to the head would have to be np.ndarray. It can be     single-column or multi-column.  The values should all be     numerical.</li> <li>epochs <code>int | None</code>: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks <code>List[keras.callbacks.Callback] | None</code>: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split <code>float | None</code>: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset.  The best model found would be fit on the entire     dataset including the validation data.</li> <li>validation_data <code>numpy.ndarray | Tuple[numpy.ndarray] | None</code>: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"image_regressor/#predict","title":"predict","text":"<pre><code>ImageRegressor.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"image_regressor/#evaluate","title":"evaluate","text":"<pre><code>ImageRegressor.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"image_regressor/#export_model","title":"export_model","text":"<pre><code>ImageRegressor.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#requirements","title":"Requirements","text":"<p>Python 3: Follow the TensorFlow install steps to install Python 3.</p> <p>Pip: Follow the TensorFlow install steps to install Pip.</p> <p>PyTorch &gt;= 2.3.0: AutoKeras is based on Keras. We recommend using the PyTorch backend. Please follow this page to install PyTorch.</p>"},{"location":"install/#install-autokeras","title":"Install AutoKeras","text":"<p>AutoKeras only support Python 3. If you followed previous steps to use virtualenv to install tensorflow, you can just activate the virtualenv and use the following command to install AutoKeras.  <pre><code>pip install git+https://github.com/keras-team/keras-tuner.git\npip install autokeras\n</code></pre></p> <p>If you did not use virtualenv, and you use <code>python3</code> command to execute your python program, please use the following command to install AutoKeras. <pre><code>python3 -m pip install git+https://github.com/keras-team/keras-tuner.git\npython3 -m pip install autokeras\n</code></pre></p>"},{"location":"node/","title":"Node","text":"<p>[source]</p>"},{"location":"node/#imageinput","title":"ImageInput","text":"<pre><code>autokeras.ImageInput(name=None, **kwargs)\n</code></pre> <p>Input node for image data.</p> <p>The input data should be numpy.ndarray. The shape of the data should be should be (samples, width, height) or (samples, width, height, channels).</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul> <p>[source]</p>"},{"location":"node/#input","title":"Input","text":"<pre><code>autokeras.Input(name=None, **kwargs)\n</code></pre> <p>Input node for tensor data.</p> <p>The data should be numpy.ndarray.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul> <p>[source]</p>"},{"location":"node/#textinput","title":"TextInput","text":"<pre><code>autokeras.TextInput(name=None, **kwargs)\n</code></pre> <p>Input node for text data.</p> <p>The input data should be numpy.ndarray. The data should be one-dimensional. Each element in the data should be a string which is a full sentence.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul> <p>[source]</p>"},{"location":"node/#structureddatainput","title":"StructuredDataInput","text":"<pre><code>autokeras.StructuredDataInput(column_names=None, column_types=None, name=None, **kwargs)\n</code></pre> <p>Input node for structured data.</p> <p>The input data should be numpy.ndarray. The data should be two-dimensional with numerical or categorical values.</p> <p>Arguments</p> <ul> <li>column_names <code>List[str] | None</code>: A list of strings specifying the names of the columns. The     length of the list should be equal to the number of columns of the     data. Defaults to None.</li> <li>column_types <code>Dict[str, str] | None</code>: Dict. The keys are the column names. The values should     either be 'numerical' or 'categorical', indicating the type of that     column. Defaults to None. If not None, the column_names need to be     specified. If None, it will be inferred from the data. A column will     be judged as categorical if the number of different values is less     than 5% of the number of instances.</li> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul>"},{"location":"structured_data_classifier/","title":"StructuredDataClassifier","text":"<p>[source]</p>"},{"location":"structured_data_classifier/#structureddataclassifier","title":"StructuredDataClassifier","text":"<pre><code>autokeras.StructuredDataClassifier(\n    column_names=None,\n    column_types=None,\n    num_classes=None,\n    multi_label=False,\n    loss=None,\n    metrics=None,\n    project_name=\"structured_data_classifier\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_accuracy\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras structured data classification class.</p> <p>Arguments</p> <ul> <li>column_names <code>List[str] | None</code>: A list of strings specifying the names of the columns. The     length of the list should be equal to the number of columns of the     data excluding the target column. Defaults to None.</li> <li>column_types <code>Dict | None</code>: Dict. The keys are the column names. The values should     either be 'numerical' or 'categorical', indicating the type of that     column.  Defaults to None. If not None, the column_names need to be     specified.  If None, it will be inferred from the data.</li> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | keras.losses.Loss | None</code>: A Keras loss function. Defaults to use 'binary_crossentropy' or     'categorical_crossentropy' based on the number of classes.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel. Defaults to     'structured_data_classifier'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize. Defaults to 'val_accuracy'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"structured_data_classifier/#fit","title":"fit","text":"<pre><code>StructuredDataClassifier.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray. Training data x.</li> <li>y: numpy.ndarray. Training data y.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, we would use epochs equal to 1000 and     early stopping with patience equal to 10.</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data.  The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch.  The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data.  <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"structured_data_classifier/#predict","title":"predict","text":"<pre><code>StructuredDataClassifier.predict(x, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray.     Testing data x. It can be string or numerical data.</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"structured_data_classifier/#evaluate","title":"evaluate","text":"<pre><code>StructuredDataClassifier.evaluate(x, y=None, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray.     Testing data x. It can be string or numerical data.</li> <li>y: numpy.ndarray. Testing data y.     It can be string labels or numerical values.</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"structured_data_classifier/#export_model","title":"export_model","text":"<pre><code>StructuredDataClassifier.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"structured_data_regressor/","title":"StructuredDataRegressor","text":"<p>[source]</p>"},{"location":"structured_data_regressor/#structureddataregressor","title":"StructuredDataRegressor","text":"<pre><code>autokeras.StructuredDataRegressor(\n    column_names=None,\n    column_types=None,\n    output_dim=None,\n    loss=\"mean_squared_error\",\n    metrics=None,\n    project_name=\"structured_data_regressor\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras structured data regression class.</p> <p>Arguments</p> <ul> <li>column_names <code>List[str] | None</code>: A list of strings specifying the names of the columns. The     length of the list should be equal to the number of columns of the     data excluding the target column. Defaults to None.</li> <li>column_types <code>Dict[str, str] | None</code>: Dict. The keys are the column names. The values should     either be 'numerical' or 'categorical', indicating the type of that     column. Defaults to None. If not None, the column_names need to be     specified. If None, it will be inferred from the data.</li> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>loss <code>str | Callable | keras.losses.Loss</code>: A Keras loss function. Defaults to use 'mean_squared_error'.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'mean_squared_error'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel. Defaults to     'structured_data_regressor'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"structured_data_regressor/#fit","title":"fit","text":"<pre><code>StructuredDataRegressor.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray.     Training data x. It can be string or numerical data.</li> <li>y: numpy.ndarray. Training data y.     It can be raw labels, one-hot encoded if more than two classes,     or binary encoded for binary classification.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, we would use epochs equal to 1000 and     early stopping with patience equal to 10.</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data.  The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch.  The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"structured_data_regressor/#predict","title":"predict","text":"<pre><code>StructuredDataRegressor.predict(x, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray.     Testing data x. It can be string or numerical data.</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"structured_data_regressor/#evaluate","title":"evaluate","text":"<pre><code>StructuredDataRegressor.evaluate(x, y=None, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray.     Testing data x. It can be string or numerical data.</li> <li>y: numpy.ndarray. Testing data y.     It can be string labels or numerical values.</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"structured_data_regressor/#export_model","title":"export_model","text":"<pre><code>StructuredDataRegressor.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"text_classifier/","title":"TextClassifier","text":"<p>[source]</p>"},{"location":"text_classifier/#textclassifier","title":"TextClassifier","text":"<pre><code>autokeras.TextClassifier(\n    num_classes=None,\n    multi_label=False,\n    loss=None,\n    metrics=None,\n    project_name=\"text_classifier\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras text classification class.</p> <p>Arguments</p> <ul> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | keras.losses.Loss | None</code>: A Keras loss function. Defaults to use 'binary_crossentropy' or     'categorical_crossentropy' based on the number of classes.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'text_classifier'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"text_classifier/#fit","title":"fit","text":"<pre><code>TextClassifier.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray. Training data x. The input     data should be numpy.ndarray. The data should     be one dimensional. Each element in the data should be a string     which is a full sentence.</li> <li>y: numpy.ndarray. Training data y. It can be     raw labels, one-hot encoded if more than two classes, or binary     encoded for binary classification.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset. The best model found would be fit on the entire     dataset including the validation data.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data.  The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"text_classifier/#predict","title":"predict","text":"<pre><code>TextClassifier.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"text_classifier/#evaluate","title":"evaluate","text":"<pre><code>TextClassifier.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"text_classifier/#export_model","title":"export_model","text":"<pre><code>TextClassifier.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"text_regressor/","title":"TextRegressor","text":"<p>[source]</p>"},{"location":"text_regressor/#textregressor","title":"TextRegressor","text":"<pre><code>autokeras.TextRegressor(\n    output_dim=None,\n    loss=\"mean_squared_error\",\n    metrics=None,\n    project_name=\"text_regressor\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras text regression class.</p> <p>Arguments</p> <ul> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>loss <code>str | Callable | keras.losses.Loss</code>: A Keras loss function. Defaults to use 'mean_squared_error'.</li> <li>metrics <code>List[str | Callable | keras.metrics.Metric] | List[List[str | Callable | keras.metrics.Metric]] | Dict[str, str | Callable | keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'mean_squared_error'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'text_regressor'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"text_regressor/#fit","title":"fit","text":"<pre><code>TextRegressor.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray. Training data x. The input     data should be numpy.ndarray. The data should     be one dimensional. Each element in the data should be a string     which is a full sentence.</li> <li>y: numpy.ndarray. Training data y. The targets     passing to the head would have to be np.ndarray,. It can be     single-column or multi-column.  The values should all be     numerical.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset. The best model found would be fit on the entire     dataset including the validation data.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"text_regressor/#predict","title":"predict","text":"<pre><code>TextRegressor.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"text_regressor/#evaluate","title":"evaluate","text":"<pre><code>TextRegressor.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"text_regressor/#export_model","title":"export_model","text":"<pre><code>TextRegressor.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"examples/automodel_with_cnn/","title":"Automodel with cnn","text":"<pre><code>import keras\nimport numpy as np\n\nimport autokeras as ak\n\n# Prepare example Data - Shape 1D\nnum_instances = 100\nnum_features = 5\nx_train = np.random.rand(num_instances, num_features).astype(np.float32)\ny_train = np.zeros(num_instances).astype(np.float32)\ny_train[0 : int(num_instances / 2)] = 1\nx_test = np.random.rand(num_instances, num_features).astype(np.float32)\ny_test = np.zeros(num_instances).astype(np.float32)\ny_train[0 : int(num_instances / 2)] = 1\n\nx_train = np.expand_dims(\n    x_train, axis=2\n)  # This step it's very important an CNN will only accept this data shape\nprint(x_train.shape)\nprint(y_train.shape)\n\n\n# Prepare Automodel for search\ninput_node = ak.Input()\noutput_node = ak.ConvBlock()(input_node)\n# output_node = ak.DenseBlock()(output_node) #optional\n# output_node = ak.SpatialReduction()(output_node) #optional\noutput_node = ak.ClassificationHead(num_classes=2, multi_label=True)(\n    output_node\n)\n\nauto_model = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\n\n\n# Search\nauto_model.fit(x_train, y_train, epochs=1)\nprint(auto_model.evaluate(x_test, y_test))\n\n\n# Export as a Keras Model\nmodel = auto_model.export_model()\nprint(type(model.summary()))\n\n# print model as image\nkeras.utils.plot_model(\n    model, show_shapes=True, expand_treeed=True, to_file=\"name.png\"\n)\n</code></pre>"},{"location":"examples/celeb_age/","title":"Celeb age","text":"<p>Regression tasks estimate a numeric variable, such as the price of a house or voter turnout.</p> <p>This example is adapted from a notebook which estimates a person's age from their image, trained on the IMDB-WIKI photographs of famous people.</p> <p>First, prepare your image data in a numpy.ndarray. Each image must have the same shape, meaning each has the same width, height, and color channels as other images in the set.</p> <pre><code>Regression tasks estimate a numeric variable, such as the price of a house or\nvoter turnout.\n\nThis example is adapted from a\n[notebook](https://gist.github.com/mapmeld/98d1e9839f2d1f9c4ee197953661ed07)\nwhich estimates a person's age from their image, trained on the\n[IMDB-WIKI](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) photographs\nof famous\npeople.\n\nFirst, prepare your image data in a numpy.ndarray.\nEach image must have the same shape, meaning each has the same width, height,\nand color channels as other images in the set.\n\"\"\"\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport numpy as np\nimport pandas as pd\nfrom google.colab import drive\nfrom PIL import Image\nfrom scipy.io import loadmat\n\nimport autokeras as ak\n\n\"\"\"\n### Connect your Google Drive for Data\n\"\"\"\n\n\ndrive.mount(\"/content/drive\")\n\n\"\"\"\n### Install AutoKeras\n\nDownload the master branch to your Google Drive for this tutorial. In general,\nyou can use *pip install autokeras* .\n\"\"\"\n\n\"\"\"shell\n!pip install  -v \"/content/drive/My Drive/AutoKeras-dev/autokeras-master.zip\"\n!pip uninstall keras-tuner\n!pip install\ngit+git://github.com/keras-team/keras-tuner.git@d2d69cba21a0b482a85ce2a38893e2322e139c01\n\"\"\"\n\n\"\"\"shell\n!pip install torch\n\"\"\"\n\n\"\"\"\n###**Import IMDB Celeb images and metadata**\n\"\"\"\n\n\"\"\"shell\n!mkdir \"./drive/My Drive/mlin/celebs\"\n\"\"\"\n\n\"\"\"shell\n! wget -O \"./drive/My Drive/mlin/celebs/imdb_0.tar\"\nhttps://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_0.tar\n\"\"\"\n\n\"\"\"shell\n! cd \"./drive/My Drive/mlin/celebs\" &amp;&amp; tar -xf imdb_0.tar\n! rm \"./drive/My Drive/mlin/celebs/imdb_0.tar\"\n\"\"\"\n\n\"\"\"\nUncomment and run the below cell if you need to re-run the cells again and\nabove don't need to install everything from the beginning.\n\"\"\"\n\n# ! cd ./drive/My\\ Drive/mlin/celebs.\n\n\"\"\"shell\n! ls \"./drive/My Drive/mlin/celebs/imdb/\"\n\"\"\"\n\n\"\"\"shell\n! wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_meta.tar\n! tar -xf imdb_meta.tar\n! rm imdb_meta.tar\n\"\"\"\n\n\"\"\"\n###**Converting from MATLAB date to actual Date-of-Birth**\n\"\"\"\n\n\ndef datenum_to_datetime(datenum):\n    \"\"\"\n    Convert Matlab datenum into Python datetime.\n    \"\"\"\n    days = datenum % 1\n    hours = days % 1 * 24\n    minutes = hours % 1 * 60\n    seconds = minutes % 1 * 60\n    try:\n        return (\n            datetime.fromordinal(int(datenum))\n            + timedelta(days=int(days))\n            + timedelta(hours=int(hours))\n            + timedelta(minutes=int(minutes))\n            + timedelta(seconds=round(seconds))\n            - timedelta(days=366)\n        )\n    except Exception:\n        return datenum_to_datetime(700000)\n\n\nprint(datenum_to_datetime(734963))\n\n\"\"\"\n### **Opening MatLab file to Pandas DataFrame**\n\"\"\"\n\n\nx = loadmat(\"imdb/imdb.mat\")\n\n\nmdata = x[\"imdb\"]  # variable in mat file\nmdtype = mdata.dtype  # dtypes of structures are \"unsized objects\"\nndata = {n: mdata[n][0, 0] for n in mdtype.names}\ncolumns = [n for n, v in ndata.items()]\n\nrows = []\nfor col in range(0, 10):\n    values = list(ndata.items())[col]\n    for num, val in enumerate(values[1][0], start=0):\n        if col == 0:\n            rows.append([])\n        if num &gt; 0:\n            if columns[col] == \"dob\":\n                rows[num].append(datenum_to_datetime(int(val)))\n            elif columns[col] == \"photo_taken\":\n                rows[num].append(datetime(year=int(val), month=6, day=30))\n            else:\n                rows[num].append(val)\n\ndt = map(lambda row: np.array(row), np.array(rows[1:]))\n\ndf = pd.DataFrame(data=dt, index=range(0, len(rows) - 1), columns=columns)\nprint(df.head())\n\nprint(columns)\nprint(df[\"full_path\"])\n\n\"\"\"\n### **Calculating age at time photo was taken**\n\"\"\"\n\ndf[\"age\"] = (df[\"photo_taken\"] - df[\"dob\"]).astype(\"int\") / 31558102e9\nprint(df[\"age\"])\n\n\"\"\"\n### **Creating dataset**\n\n\n* We sample 200 of the images which were included in this first download.\n* Images are resized to 128x128 to standardize shape and conserve memory\n* RGB images are converted to grayscale to standardize shape\n* Ages are converted to ints\n\n\n\"\"\"\n\n\ndef df2numpy(train_set):\n    images = []\n    for img_path in train_set[\"full_path\"]:\n        img = (\n            Image.open(\"./drive/My Drive/mlin/celebs/imdb/\" + img_path[0])\n            .resize((128, 128))\n            .convert(\"L\")\n        )\n        images.append(np.asarray(img, dtype=\"int32\"))\n\n    image_inputs = np.array(images)\n\n    ages = train_set[\"age\"].astype(\"int\").to_numpy()\n    return image_inputs, ages\n\n\ntrain_set = df[df[\"full_path\"] &lt; \"02\"].sample(200)\ntrain_imgs, train_ages = df2numpy(train_set)\n\ntest_set = df[df[\"full_path\"] &lt; \"02\"].sample(100)\ntest_imgs, test_ages = df2numpy(test_set)\n\n\"\"\"\n### **Training using AutoKeras**\n\"\"\"\n\n\n# Initialize the image regressor\nreg = ak.ImageRegressor(max_trials=15)  # AutoKeras tries 15 different models.\n\n# Find the best model for the given training data\nreg.fit(train_imgs, train_ages)\n\n# Predict with the chosen model:\n# predict_y = reg.predict(test_images)  # Uncomment if required\n\n# Evaluate the chosen model with testing data\nprint(reg.evaluate(train_imgs, train_ages))\n\n\"\"\"\n### **Validation Data**\n\nBy default, AutoKeras use the last 20% of training data as validation data. As\nshown in the example below, you can use validation_split to specify the\npercentage.\n\"\"\"\n\nreg.fit(\n    train_imgs,\n    train_ages,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=3,\n)\n\n\"\"\"\nYou can also use your own validation set instead of splitting it from the\ntraining data with validation_data.\n\"\"\"\n\nsplit = 460000\nx_val = train_imgs[split:]\ny_val = train_ages[split:]\nx_train = train_imgs[:split]\ny_train = train_ages[:split]\nreg.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=3,\n)\n\n\"\"\"\n### **Customized Search Space**\n\nFor advanced users, you may customize your search space by using AutoModel\ninstead of ImageRegressor. You can configure the ImageBlock for some high-level\nconfigurations, e.g., block_type for the type of neural network to search,\nnormalize for whether to do data normalization, augment for whether to do data\naugmentation. You can also choose not to specify these arguments, which would\nleave the different choices to be tuned automatically. See the following\nexample for detail.\n\"\"\"\n\n\ninput_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=\"resnet\",\n    # Normalize the dataset.\n    normalize=True,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\nreg.fit(x_train, y_train, epochs=3)\n\n\"\"\"\nThe usage of AutoModel is similar to the functional API of Keras. Basically, you\nare building a graph, whose edges are blocks and the nodes are intermediate\noutputs of blocks. To add an edge from input_node to output_node with\noutput_node = ak.some_block(input_node).  You can even also use more fine\ngrained blocks to customize the search space even further. See the following\nexample.\n\"\"\"\n\n\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(translation_factor=0.3)(output_node)\noutput_node = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.RegressionHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\nclf.fit(x_train, y_train, epochs=3)\n\n\"\"\"\n## References\n\n[Main Reference\nNotebook](https://gist.github.com/mapmeld/98d1e9839f2d1f9c4ee197953661ed07),\n[Dataset](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/),\n[ImageRegressor](/image_regressor),\n[ResNetBlock](/block/#resnetblock-class),\n[ImageInput](/node/#imageinput-class),\n[AutoModel](/auto_model/#automodel-class),\n[ImageBlock](/block/#imageblock-class),\n[Normalization](/preprocessor/#normalization-class),\n[ImageAugmentation](/preprocessor/#image-augmentation-class),\n[RegressionHead](/head/#regressionhead-class).\n\n\"\"\"\n</code></pre>"},{"location":"examples/cifar10/","title":"Cifar10","text":"<pre><code>import autokeras as ak\n\n# Prepare the dataset.\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Initialize the ImageClassifier.\nclf = ak.ImageClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=5)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)[1]))\n</code></pre>"},{"location":"examples/imdb/","title":"Imdb","text":"<p>Search for a good model for the IMDB dataset.</p> <pre><code>Search for a good model for the\n[IMDB](\nhttps://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) dataset.\n\"\"\"\n\nimport keras\nimport numpy as np\n\nimport autokeras as ak\n\n\ndef imdb_raw():\n    max_features = 20000\n    index_offset = 3  # word index offset\n\n    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n        num_words=max_features, index_from=index_offset\n    )\n    x_train = x_train\n    y_train = y_train.reshape(-1, 1)\n    x_test = x_test\n    y_test = y_test.reshape(-1, 1)\n\n    word_to_id = keras.datasets.imdb.get_word_index()\n    word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\n    word_to_id[\"&lt;PAD&gt;\"] = 0\n    word_to_id[\"&lt;START&gt;\"] = 1\n    word_to_id[\"&lt;UNK&gt;\"] = 2\n\n    id_to_word = {value: key for key, value in word_to_id.items()}\n    x_train = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_train)\n    )\n    x_test = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_test)\n    )\n    x_train = np.array(x_train, dtype=np.str)\n    x_test = np.array(x_test, dtype=np.str)\n    return (x_train, y_train), (x_test, y_test)\n\n\n# Prepare the data.\n(x_train, y_train), (x_test, y_test) = imdb_raw()\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # &lt;START&gt; this film was just brilliant casting &lt;UNK&gt;\n\n# Initialize the TextClassifier\nclf = ak.TextClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=2, batch_size=8)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)))\n</code></pre>"},{"location":"examples/mnist/","title":"Mnist","text":"<p>Search for a good model for the MNIST dataset.</p> <pre><code>Search for a good model for the\n[MNIST](https://keras.io/datasets/#mnist-database-of-handwritten-digits)\ndataset.\n\"\"\"\n\nfrom keras.datasets import mnist\n\nimport autokeras as ak\n\n# Prepare the dataset.\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n# Initialize the ImageClassifier.\nclf = ak.ImageClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=10)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)))\n</code></pre>"},{"location":"examples/new_pop/","title":"New pop","text":"<p>shell pip install autokeras</p> <pre><code>pip install autokeras\n\"\"\"\n\nimport pandas as pd\n\nimport autokeras as ak\n\n\"\"\"\n## Social Media Articles Example\n\nRegression tasks estimate a numeric variable, such as the price of a house\nor a person's age.\n\nThis example estimates the view counts for an article on social media platforms,\ntrained on a\n[News Popularity](\nhttps://archive.ics.uci.edu/ml/datasets/\nNews+Popularity+in+Multiple+Social+Media+Platforms)\ndataset collected from 2015-2016.\n\nFirst, prepare your text data in a `numpy.ndarray` format.\n\"\"\"\n\n\n# converting from other formats (such as pandas) to numpy\ndf = pd.read_csv(\"./News_Final.csv\")\n\ntext_inputs = df.Title.to_numpy(dtype=\"str\")\nmedia_success_outputs = df.Facebook.to_numpy(dtype=\"int\")\n\n\"\"\"\nNext, initialize and train the [TextRegressor](/text_regressor).\n\"\"\"\n\n\n# Initialize the text regressor\nreg = ak.TextRegressor(max_trials=15)  # AutoKeras tries 15 different models.\n\n# Find the best model for the given training data\nreg.fit(text_inputs, media_success_outputs)\n\n# Predict with the chosen model:\npredict_y = reg.predict(text_inputs)\n</code></pre>"},{"location":"examples/reuters/","title":"Reuters","text":"<p>shell !pip install -q -U pip !pip install -q -U autokeras==1.0.8 !pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1</p> <pre><code>!pip install -q -U pip\n!pip install -q -U autokeras==1.0.8\n!pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1\n\"\"\"\n\nimport keras\nimport numpy as np\nfrom keras.datasets import reuters\n\nimport autokeras as ak\n\n\"\"\"\nSearch for a good model for the\n[Reuters](https://keras.io/ja/datasets/#_5) dataset.\n\"\"\"\n\n\n# Prepare the dataset.\ndef reuters_raw(max_features=20000):\n    index_offset = 3  # word index offset\n\n    (x_train, y_train), (x_test, y_test) = reuters.load_data(\n        num_words=max_features, index_from=index_offset\n    )\n    x_train = x_train\n    y_train = y_train.reshape(-1, 1)\n    x_test = x_test\n    y_test = y_test.reshape(-1, 1)\n\n    word_to_id = reuters.get_word_index()\n    word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\n    word_to_id[\"&lt;PAD&gt;\"] = 0\n    word_to_id[\"&lt;START&gt;\"] = 1\n    word_to_id[\"&lt;UNK&gt;\"] = 2\n\n    id_to_word = {value: key for key, value in word_to_id.items()}\n    x_train = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_train)\n    )\n    x_test = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_test)\n    )\n    x_train = np.array(x_train, dtype=np.str)\n    x_test = np.array(x_test, dtype=np.str)\n    return (x_train, y_train), (x_test, y_test)\n\n\n# Prepare the data.\n(x_train, y_train), (x_test, y_test) = reuters_raw()\nprint(x_train.shape)  # (8982,)\nprint(y_train.shape)  # (8982, 1)\nprint(x_train[0][:50])  # &lt;START&gt; &lt;UNK&gt; &lt;UNK&gt; said as a result of its decemb\n\n# Initialize the TextClassifier\nclf = ak.TextClassifier(\n    max_trials=5,\n    overwrite=True,\n)\n\n# Callback to avoid overfitting with the EarlyStopping.\ncbs = [\n    keras.callbacks.EarlyStopping(patience=3),\n]\n\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=10, callback=cbs)\n\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)))\n</code></pre>"},{"location":"tutorial/customized/","title":"Customized Model","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>import keras\nimport numpy as np\nimport tree\nfrom keras.datasets import mnist\n\nimport autokeras as ak\n</code></pre> <p>In this tutorial, we show how to customize your search space with AutoModel and how to implement your own block as search space.  This API is mainly for advanced users who already know what their model should look like.</p>"},{"location":"tutorial/customized/#customized-search-space","title":"Customized Search Space","text":"<p>First, let us see how we can build the following neural network using the building blocks in AutoKeras.</p>  graph LR     id1(ImageInput) --&gt; id2(Normalization)     id2 --&gt; id3(Image Augmentation)     id3 --&gt; id4(Convolutional)     id3 --&gt; id5(ResNet V2)     id4 --&gt; id6(Merge)     id5 --&gt; id6     id6 --&gt; id7(Classification Head)  <p>We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API. Since this is just a demo, we use small amount of <code>max_trials</code> and <code>epochs</code>.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node = ak.ClassificationHead()(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\n</code></pre> <p>Whild building the model, the blocks used need to follow this topology: <code>Preprocessor</code> -&gt; <code>Block</code> -&gt; <code>Head</code>. <code>Normalization</code> and <code>ImageAugmentation</code> are <code>Preprocessor</code>s. <code>ClassificationHead</code> is <code>Head</code>. The rest are <code>Block</code>s.</p> <p>In the code above, we use <code>ak.ResNetBlock(version='v2')</code> to specify the version of ResNet to use.  There are many other arguments to specify for each building block.  For most of the arguments, if not specified, they would be tuned automatically.  Please refer to the documentation links at the bottom of the page for more details.</p> <p>Then, we prepare some data to run the model.</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n# Feed the AutoModel with training data.\nauto_model.fit(x_train[:100], y_train[:100], epochs=1)\n# Predict with the best model.\npredicted_y = auto_model.predict(x_test)\n# Evaluate the best model with testing data.\nprint(auto_model.evaluate(x_test, y_test))\n</code></pre> <p>For multiple input nodes and multiple heads search space, you can refer to this section.</p>"},{"location":"tutorial/customized/#validation-data","title":"Validation Data","text":"<p>If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification, Text Classification, Structured Data Classification, Multi-task and Multiple Validation.</p>"},{"location":"tutorial/customized/#implement-new-block","title":"Implement New Block","text":"<p>You can extend the Block class to implement your own building blocks and use it with AutoModel.</p> <p>The first step is to learn how to write a build function for KerasTuner.  You need to override the build function of the block.  The following example shows how to implement a single Dense layer block whose number of neurons is tunable.</p> <pre><code>class SingleDenseLayerBlock(ak.Block):\n    def build(self, hp, inputs=None):\n        # Get the input_node from inputs.\n        input_node = tree.flatten(inputs)[0]\n        layer = keras.layers.Dense(\n            hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n        )\n        output_node = layer(input_node)\n        return output_node\n</code></pre> <p>You can connect it with other blocks and build it into an AutoModel.</p> <pre><code># Build the AutoModel\ninput_node = ak.Input()\noutput_node = SingleDenseLayerBlock()(input_node)\noutput_node = ak.RegressionHead()(output_node)\nauto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=1)\n# Prepare Data\nnum_instances = 100\nx_train = np.random.rand(num_instances, 20).astype(np.float32)\ny_train = np.random.rand(num_instances, 1).astype(np.float32)\nx_test = np.random.rand(num_instances, 20).astype(np.float32)\ny_test = np.random.rand(num_instances, 1).astype(np.float32)\n# Train the model\nauto_model.fit(x_train, y_train, epochs=1)\nprint(auto_model.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/customized/#reference","title":"Reference","text":"<p>AutoModel</p> <p>Nodes: ImageInput, Input, TextInput. StructuredDataInput,</p> <p>Preprocessors: FeatureEngineering, ImageAugmentation, LightGBM, Normalization,</p> <p>Blocks: ConvBlock, DenseBlock, Embedding, Merge, ResNetBlock, RNNBlock, SpatialReduction, TemporalReduction, XceptionBlock, ImageBlock, TextBlock. StructuredDataBlock,</p>"},{"location":"tutorial/export/","title":"Export Model","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>import numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import load_model\n\nimport autokeras as ak\n</code></pre> <p>You can easily export your model the best model found by AutoKeras as a Keras Model.</p> <p>The following example uses ImageClassifier as an example. All the tasks and the AutoModel has this export_model function.</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Initialize the image classifier.\nclf = ak.ImageClassifier(\n    overwrite=True, max_trials=1\n)  # Try only 1 model.(Increase accordingly)\n# Feed the image classifier with training data.\nclf.fit(x_train, y_train, epochs=1)  # Change no of epochs to improve the model\n# Export as a Keras Model.\nmodel = clf.export_model()\n\nprint(type(model))  # &lt;class 'keras.engine.training.Model'&gt;\n\nmodel.save(\"model_autokeras.keras\")\n\n\nloaded_model = load_model(\n    \"model_autokeras.keras\", custom_objects=ak.CUSTOM_OBJECTS\n)\n\npredicted_y = loaded_model.predict(np.expand_dims(x_test, -1))\nprint(predicted_y)\n</code></pre>"},{"location":"tutorial/faq/","title":"FAQ","text":""},{"location":"tutorial/faq/#how-to-resume-a-previously-killed-run","title":"How to resume a previously killed run?","text":"<p>This feature is controlled by the <code>overwrite</code> argument of <code>AutoModel</code> or any other task APIs. It is set to <code>False</code> by default, which means it would not overwrite the contents of the directory. In other words, it will continue the previous fit.</p> <p>You can just run the same code again. It will automatically resume the previously killed run.</p>"},{"location":"tutorial/faq/#how-to-customize-metrics-and-loss","title":"How to customize metrics and loss?","text":"<p>Please see the code example below.</p> <pre><code>import autokeras as ak\n\n\nclf = ak.ImageClassifier(\n    max_trials=3,\n    metrics=['mse'],\n    loss='mse',\n)\n</code></pre>"},{"location":"tutorial/faq/#how-to-use-customized-metrics-to-select-the-best-model","title":"How to use customized metrics to select the best model?","text":"<p>By default, AutoKeras use validation loss as the metric for selecting the best model. Below is a code example of using customized metric for selecting models. Please read the comments for the details.</p> <pre><code># Implement your customized metric according to the tutorial.\n# https://keras.io/api/metrics/#creating-custom-metrics\nimport autokeras as ak\n\n\ndef f1_score(y_true, y_pred):\n  ...\n\nclf = ak.ImageClassifier(\n    max_trials=3,\n\n    # Wrap the function into a Keras Tuner Objective \n    # and pass it to AutoKeras.\n\n    # Direction can be 'min' or 'max'\n    # meaning we want to minimize or maximize the metric.\n\n    # 'val_f1_score' is just add a 'val_' prefix\n    # to the function name or the metric name.\n\n    objective=kerastuner.Objective('val_f1_score', direction='max'),\n    # Include it as one of the metrics.\n    metrics=[f1_score],\n)\n</code></pre>"},{"location":"tutorial/image_classification/","title":"Image Classification","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>from keras.datasets import mnist\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/image_classification/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the MNIST dataset as an example</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:100]\ny_train = y_train[:100]\nx_test = x_test[:100]\ny_test = y_test[:100]\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n</code></pre> <p>The second step is to run the ImageClassifier. It is recommended have more trials for more complicated datasets. This is just a quick demo of MNIST, so we set max_trials to 1. For the same reason, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the image classifier.\nclf = ak.ImageClassifier(overwrite=True, max_trials=1)\n# Feed the image classifier with training data.\nclf.fit(x_train, y_train, epochs=1)\n\n\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\nprint(predicted_y)\n\n\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/image_classification/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage.</p> <pre><code>clf.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with validation_data.</p> <pre><code>split = 50000\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=1,\n)\n</code></pre>"},{"location":"tutorial/image_classification/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=\"resnet\",\n    # Normalize the dataset.\n    normalize=True,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nclf.fit(x_train, y_train, epochs=1)\n</code></pre> <p>The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.some_block(input_node).</p> <p>You can even also use more fine grained blocks to customize the search space even further. See the following example.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\noutput_node = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nclf.fit(x_train, y_train, epochs=1)\n</code></pre>"},{"location":"tutorial/image_classification/#reference","title":"Reference","text":"<p>ImageClassifier, AutoModel, ImageBlock, Normalization, ImageAugmentation, ResNetBlock, ImageInput, ClassificationHead.</p>"},{"location":"tutorial/image_regression/","title":"Image Regression","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>from keras.datasets import mnist\n\nimport autokeras as ak\n</code></pre> <p>To make this tutorial easy to follow, we just treat MNIST dataset as a regression dataset. It means we will treat prediction targets of MNIST dataset, which are integers ranging from 0 to 9 as numerical values, so that they can be directly used as the regression targets.</p>"},{"location":"tutorial/image_regression/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the MNIST dataset as an example</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:100]\ny_train = y_train[:100]\nx_test = x_test[:100]\ny_test = y_test[:100]\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n</code></pre> <p>The second step is to run the ImageRegressor.  It is recommended have more trials for more complicated datasets.  This is just a quick demo of MNIST, so we set max_trials to 1.  For the same reason, we set epochs to 1.  You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the image regressor.\nreg = ak.ImageRegressor(overwrite=True, max_trials=1)\n# Feed the image regressor with training data.\nreg.fit(x_train, y_train, epochs=1)\n\n\n# Predict with the best model.\npredicted_y = reg.predict(x_test)\nprint(predicted_y)\n\n\n# Evaluate the best model with testing data.\nprint(reg.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/image_regression/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage.</p> <pre><code>reg.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with validation_data.</p> <pre><code>split = 50000\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nreg.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=2,\n)\n</code></pre>"},{"location":"tutorial/image_regression/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of ImageRegressor. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=\"resnet\",\n    # Normalize the dataset.\n    normalize=False,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nreg.fit(x_train, y_train, epochs=1)\n</code></pre> <p>The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.some_block(input_node).</p> <p>You can even also use more fine grained blocks to customize the search space even further. See the following example.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\noutput_node = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nreg.fit(x_train, y_train, epochs=1)\n</code></pre>"},{"location":"tutorial/image_regression/#reference","title":"Reference","text":"<p>ImageRegressor, AutoModel, ImageBlock, Normalization, ImageAugmentation, ResNetBlock, ImageInput, RegressionHead.</p>"},{"location":"tutorial/multi/","title":"Multi-Modal and Multi-Task","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>import numpy as np\n\nimport autokeras as ak\n</code></pre> <p>In this tutorial we are making use of the AutoModel  API to show how to handle multi-modal data and multi-task.</p>"},{"location":"tutorial/multi/#what-is-multi-modal","title":"What is multi-modal?","text":"<p>Multi-modal data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as numerical data.</p>"},{"location":"tutorial/multi/#what-is-multi-task","title":"What is multi-task?","text":"<p>Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1.</p> <p>The following diagram shows an example of multi-modal and multi-task neural network model.</p>  graph TD     id1(ImageInput) --&gt; id3(Some Neural Network Model)     id2(Input) --&gt; id3     id3 --&gt; id4(ClassificationHead)     id3 --&gt; id5(RegressionHead)  <p>It has two inputs the images and the numerical input data. Each image is associated with a set of attributes in the numerical input data. From these data, we are trying to predict the classification label and the regression value at the same time.</p>"},{"location":"tutorial/multi/#data-preparation","title":"Data Preparation","text":"<p>To illustrate our idea, we generate some random image and numerical data as the multi-modal data.</p> <pre><code>num_instances = 10\n# Generate image data.\nimage_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n# Generate numerical data.\nnumerical_data = np.random.rand(num_instances, 20).astype(np.float32)\n</code></pre> <p>We also generate some multi-task targets for classification and regression.</p> <pre><code># Generate regression targets.\nregression_target = np.random.rand(num_instances, 1).astype(np.float32)\n# Generate classification labels of five classes.\nclassification_target = np.random.randint(5, size=num_instances)\n</code></pre>"},{"location":"tutorial/multi/#build-and-train-the-model","title":"Build and Train the Model","text":"<p>Then we initialize the multi-modal and multi-task model with AutoModel. Since this is just a demo, we use small amount of <code>max_trials</code> and <code>epochs</code>.</p> <pre><code># Initialize the multi with multiple inputs and outputs.\nmodel = ak.AutoModel(\n    inputs=[ak.ImageInput(), ak.Input()],\n    outputs=[\n        ak.RegressionHead(metrics=[\"mae\"]),\n        ak.ClassificationHead(\n            loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n        ),\n    ],\n    overwrite=True,\n    max_trials=2,\n)\n# Fit the model with prepared data.\nmodel.fit(\n    [image_data, numerical_data],\n    [regression_target, classification_target],\n    epochs=1,\n    batch_size=3,\n)\n</code></pre>"},{"location":"tutorial/multi/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>model.fit(\n    [image_data, numerical_data],\n    [regression_target, classification_target],\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n    batch_size=3,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 5\n\nimage_val = image_data[split:]\nnumerical_val = numerical_data[split:]\nregression_val = regression_target[split:]\nclassification_val = classification_target[split:]\n\nimage_data = image_data[:split]\nnumerical_data = numerical_data[:split]\nregression_target = regression_target[:split]\nclassification_target = classification_target[:split]\n\nmodel.fit(\n    [image_data, numerical_data],\n    [regression_target, classification_target],\n    # Use your own validation set.\n    validation_data=(\n        [image_val, numerical_val],\n        [regression_val, classification_val],\n    ),\n    epochs=1,\n    batch_size=3,\n)\n</code></pre>"},{"location":"tutorial/multi/#customized-search-space","title":"Customized Search Space","text":"<p>You can customize your search space. The following figure shows the search space we want to define.</p>  graph LR     id1(ImageInput) --&gt; id2(Normalization)     id2 --&gt; id3(Image Augmentation)     id3 --&gt; id4(Convolutional)     id3 --&gt; id5(ResNet V2)     id4 --&gt; id6(Merge)     id5 --&gt; id6     id7(Input) --&gt; id9(DenseBlock)     id6 --&gt; id10(Merge)     id9 --&gt; id10     id10 --&gt; id11(Classification Head)     id10 --&gt; id12(Regression Head)  <pre><code>input_node1 = ak.ImageInput()\noutput_node = ak.Normalization()(input_node1)\noutput_node = ak.ImageAugmentation()(output_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node1 = ak.Merge()([output_node1, output_node2])\n\ninput_node2 = ak.Input()\noutput_node2 = ak.DenseBlock()(input_node2)\n\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node1 = ak.ClassificationHead()(output_node)\noutput_node2 = ak.RegressionHead()(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=[input_node1, input_node2],\n    outputs=[output_node1, output_node2],\n    overwrite=True,\n    max_trials=2,\n)\n\nimage_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\nnumerical_data = np.random.rand(num_instances, 20).astype(np.float32)\nregression_target = np.random.rand(num_instances, 1).astype(np.float32)\nclassification_target = np.random.randint(5, size=num_instances)\n\nauto_model.fit(\n    [image_data, numerical_data],\n    [classification_target, regression_target],\n    batch_size=3,\n    epochs=1,\n)\n</code></pre>"},{"location":"tutorial/multi/#reference","title":"Reference","text":"<p>AutoModel, ImageInput, Input, DenseBlock, RegressionHead, ClassificationHead, CategoricalToNumerical.</p>"},{"location":"tutorial/overview/","title":"AutoKeras 1.0 Tutorial","text":""},{"location":"tutorial/overview/#supported-tasks","title":"Supported Tasks","text":"<p>AutoKeras supports several tasks with an extremely simple interface. You can click the links below to see the detailed tutorial for each task.</p> <p>Supported Tasks:</p> <p>Image Classification</p> <p>Image Regression</p> <p>Text Classification</p> <p>Text Regression</p> <p>Structured Data Classification</p> <p>Structured Data Regression</p>"},{"location":"tutorial/overview/#multi-task-and-multi-modal-data","title":"Multi-Task and Multi-Modal Data","text":"<p>If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details.</p>"},{"location":"tutorial/overview/#customized-model","title":"Customized Model","text":"<p>Follow this tutorial, to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras.</p> <p>Nodes:</p> <p>ImageInput</p> <p>Input</p> <p>TextInput</p> <p>StructuredDataInput</p> <p>Blocks:</p> <p>ImageAugmentation</p> <p>Normalization</p> <p>ConvBlock</p> <p>DenseBlock</p> <p>Embedding</p> <p>Merge</p> <p>ResNetBlock</p> <p>RNNBlock</p> <p>SpatialReduction</p> <p>TemporalReduction</p> <p>XceptionBlock</p> <p>ImageBlock</p> <p>TextBlock</p> <p>StructuredDataBlock</p> <p>ClassificationHead</p> <p>RegressionHead</p>"},{"location":"tutorial/overview/#export-model","title":"Export Model","text":"<p>You can follow this tutorial to export the best model.</p>"},{"location":"tutorial/structured_data_classification/","title":"Structured Data Classification","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>import keras\nimport pandas as pd\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/structured_data_classification/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the Titanic dataset as an example.</p> <pre><code>TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\nTEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n\ntrain_file_path = keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\ntest_file_path = keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n\n# Load data into numpy arrays\ntrain_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\n\ny_train = train_df[\"survived\"].values\nx_train = train_df.drop(\"survived\", axis=1).values\n\ny_test = test_df[\"survived\"].values\nx_test = test_df.drop(\"survived\", axis=1).values\n</code></pre> <p>The second step is to run the StructuredDataClassifier. As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(\n    overwrite=True, max_trials=3\n)  # It tries 3 different models.\n# Feed the structured data classifier with training data.\nclf.fit(\n    x_train,\n    y_train,\n    epochs=10,\n)\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n</code></pre> <p>You can also specify the column names and types for the data as follows.  The <code>column_names</code> is optional if the training data already have the column names, e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will be inferred from the training data.</p> <pre><code># Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(\n    column_names=[\n        \"sex\",\n        \"age\",\n        \"n_siblings_spouses\",\n        \"parch\",\n        \"fare\",\n        \"class\",\n        \"deck\",\n        \"embark_town\",\n        \"alone\",\n    ],\n    column_types={\"sex\": \"categorical\", \"fare\": \"numerical\"},\n    max_trials=10,  # It tries 10 different models.\n    overwrite=True,\n)\n</code></pre>"},{"location":"tutorial/structured_data_classification/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data.  As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>clf.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=10,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 500\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=10,\n)\n</code></pre>"},{"location":"tutorial/structured_data_classification/#reference","title":"Reference","text":"<p>StructuredDataClassifier, AutoModel, StructuredDataBlock, DenseBlock, StructuredDataInput, ClassificationHead,</p>"},{"location":"tutorial/structured_data_regression/","title":"Structured Data Regression","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>from sklearn.datasets import fetch_california_housing\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/structured_data_regression/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the California housing dataset as an example.</p> <pre><code>house_dataset = fetch_california_housing()\ntrain_size = int(house_dataset.data.shape[0] * 0.9)\n\nx_train = house_dataset.data[:train_size]\ny_train = house_dataset.target[:train_size]\nx_test = house_dataset.data[train_size:]\ny_test = house_dataset.target[train_size:]\n</code></pre> <p>The second step is to run the StructuredDataRegressor. As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the structured data regressor.\nreg = ak.StructuredDataRegressor(\n    overwrite=True, max_trials=3\n)  # It tries 3 different models.\n# Feed the structured data regressor with training data.\nreg.fit(\n    x_train,\n    y_train,\n    epochs=10,\n)\n# Predict with the best model.\npredicted_y = reg.predict(x_test)\n# Evaluate the best model with testing data.\nprint(reg.evaluate(x_test, y_test))\n</code></pre> <p>You can also specify the column names and types for the data as follows.  The <code>column_names</code> is optional if the training data already have the column names, e.g.  pandas.DataFrame, CSV file.  Any column, whose type is not specified will be inferred from the training data.</p> <pre><code># Initialize the structured data regressor.\nreg = ak.StructuredDataRegressor(\n    column_names=[\n        \"MedInc\",\n        \"HouseAge\",\n        \"AveRooms\",\n        \"AveBedrms\",\n        \"Population\",\n        \"AveOccup\",\n        \"Latitude\",\n        \"Longitude\",\n    ],\n    column_types={\"MedInc\": \"numerical\", \"Latitude\": \"numerical\"},\n    max_trials=10,  # It tries 10 different models.\n    overwrite=True,\n)\n</code></pre>"},{"location":"tutorial/structured_data_regression/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data.  As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>reg.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=10,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 500\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nreg.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=10,\n)\n</code></pre>"},{"location":"tutorial/structured_data_regression/#reference","title":"Reference","text":"<p>StructuredDataRegressor, AutoModel, StructuredDataBlock, DenseBlock, StructuredDataInput, RegressionHead,</p>"},{"location":"tutorial/text_classification/","title":"Text Classification","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>import os\n\nimport keras\nimport numpy as np\nfrom sklearn.datasets import load_files\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/text_classification/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the IMDB dataset as an example.</p> <pre><code>dataset = keras.utils.get_file(\n    fname=\"aclImdb.tar.gz\",\n    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n    extract=True,\n)\n\n# set path to dataset\nIMDB_DATADIR = os.path.join(\n    os.path.dirname(dataset), \"aclImdb_extracted\", \"aclImdb\"\n)\n\nclasses = [\"pos\", \"neg\"]\ntrain_data = load_files(\n    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n)\ntest_data = load_files(\n    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n)\n\nx_train = np.array(train_data.data)[:100]\ny_train = np.array(train_data.target)[:100]\nx_test = np.array(test_data.data)[:100]\ny_test = np.array(test_data.target)[:100]\n\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # this film was just brilliant casting\n</code></pre> <p>The second step is to run the TextClassifier.  As a quick demo, we set epochs to 2.  You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the text classifier.\nclf = ak.TextClassifier(\n    overwrite=True, max_trials=1\n)  # It only tries 1 model as a quick demo.\n# Feed the text classifier with training data.\nclf.fit(x_train, y_train, epochs=1, batch_size=2)\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/text_classification/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data.  As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>clf.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n    batch_size=2,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 5\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(\n    x_train,\n    y_train,\n    epochs=1,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    batch_size=2,\n)\n</code></pre>"},{"location":"tutorial/text_classification/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of TextClassifier. You can configure the TextBlock for some high-level configurations. You can also do not specify these arguments, which would leave the different choices to be tuned automatically.  See the following example for detail.</p> <pre><code>input_node = ak.TextInput()\noutput_node = ak.TextBlock()(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nclf.fit(x_train, y_train, epochs=1, batch_size=2)\n</code></pre>"},{"location":"tutorial/text_classification/#reference","title":"Reference","text":"<p>TextClassifier, AutoModel, ConvBlock, TextInput, ClassificationHead.</p>"},{"location":"tutorial/text_regression/","title":"Text Regression","text":"<p> View in Colab GitHub source</p> <pre><code>!export KERAS_BACKEND=\"torch\"\n!pip install autokeras\n</code></pre> <pre><code>import os\n\nimport keras\nimport numpy as np\nfrom sklearn.datasets import load_files\n\nimport autokeras as ak\n</code></pre> <p>To make this tutorial easy to follow, we just treat IMDB dataset as a regression dataset. It means we will treat prediction targets of IMDB dataset, which are 0s and 1s as numerical values, so that they can be directly used as the regression targets.</p>"},{"location":"tutorial/text_regression/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the IMDB dataset as an example.</p> <pre><code>dataset = keras.utils.get_file(\n    fname=\"aclImdb.tar.gz\",\n    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n    extract=True,\n)\n\n# set path to dataset\nIMDB_DATADIR = os.path.join(\n    os.path.dirname(dataset), \"aclImdb_extracted\", \"aclImdb\"\n)\n\nclasses = [\"pos\", \"neg\"]\ntrain_data = load_files(\n    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n)\ntest_data = load_files(\n    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n)\n\nx_train = np.array(train_data.data)[:100]\ny_train = np.array(train_data.target)[:100]\nx_test = np.array(test_data.data)[:100]\ny_test = np.array(test_data.target)[:100]\n\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # &lt;START&gt; this film was just brilliant casting &lt;UNK&gt;\n</code></pre> <p>The second step is to run the TextRegressor.  As a quick demo, we set epochs to 2.  You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the text regressor.\nreg = ak.TextRegressor(\n    overwrite=True, max_trials=1  # It tries 10 different models.\n)\n# Feed the text regressor with training data.\nreg.fit(x_train, y_train, epochs=1, batch_size=2)\n# Predict with the best model.\npredicted_y = reg.predict(x_test)\n# Evaluate the best model with testing data.\nprint(reg.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/text_regression/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data.  As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>reg.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 5\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nreg.fit(\n    x_train,\n    y_train,\n    epochs=1,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    batch_size=2,\n)\n</code></pre>"},{"location":"tutorial/text_regression/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of TextRegressor. You can configure the TextBlock for some high-level configurations. You can also do not specify these arguments, which would leave the different choices to be tuned automatically.  See the following example for detail.</p> <pre><code>input_node = ak.TextInput()\noutput_node = ak.TextBlock()(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nreg.fit(x_train, y_train, epochs=1, batch_size=2)\n</code></pre>"},{"location":"tutorial/text_regression/#reference","title":"Reference","text":"<p>TextRegressor, AutoModel, TextBlock, ConvBlock, TextInput, RegressionHead.</p>"}]}