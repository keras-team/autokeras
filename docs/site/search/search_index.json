{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible to everyone. Learning resources A short example. import autokeras as ak clf = ak . ImageClassifier () clf . fit ( x_train , y_train ) results = clf . predict ( x_test ) Official website tutorials . The book of Automated Machine Learning in Action . The LiveProjects of Image Classification with AutoKeras . Installation To install the package, please use the pip installation as follows: pip3 install autokeras Please follow the installation guide for more details. Note: Currently, AutoKeras is only compatible with Python >= 3.7 and TensorFlow >= 2.8.0 . Community Ask your questions on our GitHub Discussions . Contributing Code Here is how we manage our project. We pick the critical issues to work on from GitHub issues . They will be added to this Project . Some of the issues will then be added to the milestones , which are used to plan for the releases. Refer to our Contributing Guide to learn the best practices. Thank all the contributors! Cite this work Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings { jin2019auto , title = {Auto-Keras: An Efficient Neural Architecture Search System} , author = {Jin, Haifeng and Song, Qingquan and Hu, Xia} , booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining} , pages = {1946--1956} , year = {2019} , organization = {ACM} } Acknowledgements The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M University.","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"","title":""},{"location":"#_3","text":"AutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible to everyone.","title":""},{"location":"#learning-resources","text":"A short example. import autokeras as ak clf = ak . ImageClassifier () clf . fit ( x_train , y_train ) results = clf . predict ( x_test ) Official website tutorials . The book of Automated Machine Learning in Action . The LiveProjects of Image Classification with AutoKeras .","title":"Learning resources"},{"location":"#installation","text":"To install the package, please use the pip installation as follows: pip3 install autokeras Please follow the installation guide for more details. Note: Currently, AutoKeras is only compatible with Python >= 3.7 and TensorFlow >= 2.8.0 .","title":"Installation"},{"location":"#community","text":"Ask your questions on our GitHub Discussions .","title":"Community"},{"location":"#contributing-code","text":"Here is how we manage our project. We pick the critical issues to work on from GitHub issues . They will be added to this Project . Some of the issues will then be added to the milestones , which are used to plan for the releases. Refer to our Contributing Guide to learn the best practices. Thank all the contributors!","title":"Contributing Code"},{"location":"#cite-this-work","text":"Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings { jin2019auto , title = {Auto-Keras: An Efficient Neural Architecture Search System} , author = {Jin, Haifeng and Song, Qingquan and Hu, Xia} , booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining} , pages = {1946--1956} , year = {2019} , organization = {ACM} }","title":"Cite this work"},{"location":"#acknowledgements","text":"The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M University.","title":"Acknowledgements"},{"location":"about/","text":"This package is developed by DATA LAB at Texas A&M University, collaborating with keras-team for version 1.0 and above. Core Team Haifeng Jin : Created, designed and implemented the AutoKeras system. Maintainer. Fran\u00e7ois Chollet : The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests. Qingquan Song : Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module. Xia \"Ben\" Hu : Project lead and maintainer.","title":"About"},{"location":"about/#core-team","text":"Haifeng Jin : Created, designed and implemented the AutoKeras system. Maintainer. Fran\u00e7ois Chollet : The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests. Qingquan Song : Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module. Xia \"Ben\" Hu : Project lead and maintainer.","title":"Core Team"},{"location":"auto_model/","text":"[source] AutoModel autokeras . AutoModel ( inputs , outputs , project_name = \"auto_model\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = \"greedy\" , overwrite = False , seed = None , max_model_size = None , ** kwargs ) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API . Exampl Example # The user only specifies the input nodes and output heads. import autokeras as ak ak . AutoModel ( inputs = [ ak . ImageInput (), ak . TextInput ()], outputs = [ ak . ClassificationHead (), ak . RegressionHead ()] ) # The user specifies the high-level architecture. import autokeras as ak image_input = ak . ImageInput () image_output = ak . ImageBlock ()( image_input ) text_input = ak . TextInput () text_output = ak . TextBlock ()( text_input ) output = ak . Merge ()([ image_output , text_output ]) classification_output = ak . ClassificationHead ()( output ) regression_output = ak . RegressionHead ()( output ) ak . AutoModel ( inputs = [ image_input , text_input ], outputs = [ classification_output , regression_output ] ) Arguments inputs Union[autokeras.Input, List[autokeras.Input]] : A list of Node instances. The input node(s) of the AutoModel. outputs Union[autokeras.Head, autokeras.Node, list] : A list of Node or Head instances. The output node(s) or head(s) of the AutoModel. project_name str : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Union[str, Type[autokeras.engine.tuner.AutoTuner]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. Defaults to 'greedy'. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by keras_tuner.Tuner. [source] fit AutoModel . fit ( x = None , y = None , batch_size = 32 , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , verbose = 1 , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. batch_size : Int. Number of samples per gradient update. Defaults to 32. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. verbose : 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). Controls the verbosity of both KerasTuner search and keras.Model.fit **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict AutoModel . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate AutoModel . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model AutoModel . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"AutoModel"},{"location":"auto_model/#automodel","text":"autokeras . AutoModel ( inputs , outputs , project_name = \"auto_model\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = \"greedy\" , overwrite = False , seed = None , max_model_size = None , ** kwargs ) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API . Exampl","title":"AutoModel"},{"location":"auto_model/#example","text":"# The user only specifies the input nodes and output heads. import autokeras as ak ak . AutoModel ( inputs = [ ak . ImageInput (), ak . TextInput ()], outputs = [ ak . ClassificationHead (), ak . RegressionHead ()] ) # The user specifies the high-level architecture. import autokeras as ak image_input = ak . ImageInput () image_output = ak . ImageBlock ()( image_input ) text_input = ak . TextInput () text_output = ak . TextBlock ()( text_input ) output = ak . Merge ()([ image_output , text_output ]) classification_output = ak . ClassificationHead ()( output ) regression_output = ak . RegressionHead ()( output ) ak . AutoModel ( inputs = [ image_input , text_input ], outputs = [ classification_output , regression_output ] ) Arguments inputs Union[autokeras.Input, List[autokeras.Input]] : A list of Node instances. The input node(s) of the AutoModel. outputs Union[autokeras.Head, autokeras.Node, list] : A list of Node or Head instances. The output node(s) or head(s) of the AutoModel. project_name str : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Union[str, Type[autokeras.engine.tuner.AutoTuner]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. Defaults to 'greedy'. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by keras_tuner.Tuner. [source]","title":"Example"},{"location":"auto_model/#fit","text":"AutoModel . fit ( x = None , y = None , batch_size = 32 , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , verbose = 1 , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. batch_size : Int. Number of samples per gradient update. Defaults to 32. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. verbose : 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). Controls the verbosity of both KerasTuner search and keras.Model.fit **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"auto_model/#predict","text":"AutoModel . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"auto_model/#evaluate","text":"AutoModel . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"auto_model/#export_model","text":"AutoModel . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"base/","text":"[source] Node autokeras . Node ( ** kwargs ) The nodes in a network connecting the blocks. [source] Block autokeras . Block ( ** kwargs ) The base class for different Block. The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user. [source] build Block . build ( hp , inputs = None ) Build the Block into a real Keras Model. The subclasses should override this function and return the output node. Arguments hp : HyperParameters. The hyperparameters for building the model. inputs : A list of input node(s). [source] Head autokeras . Head ( loss = None , metrics = None , ** kwargs ) Base class for the heads, e.g. classification, regression. Arguments loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to None. If None, the loss will be inferred from the AutoModel. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to None. If None, the metrics will be inferred from the AutoModel.","title":"Base Class"},{"location":"base/#node","text":"autokeras . Node ( ** kwargs ) The nodes in a network connecting the blocks. [source]","title":"Node"},{"location":"base/#block","text":"autokeras . Block ( ** kwargs ) The base class for different Block. The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user. [source]","title":"Block"},{"location":"base/#build","text":"Block . build ( hp , inputs = None ) Build the Block into a real Keras Model. The subclasses should override this function and return the output node. Arguments hp : HyperParameters. The hyperparameters for building the model. inputs : A list of input node(s). [source]","title":"build"},{"location":"base/#head","text":"autokeras . Head ( loss = None , metrics = None , ** kwargs ) Base class for the heads, e.g. classification, regression. Arguments loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to None. If None, the loss will be inferred from the AutoModel. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to None. If None, the metrics will be inferred from the AutoModel.","title":"Head"},{"location":"benchmarks/","text":"Benchmarks We track the performance of the latest AutoKeras release on the benchmark datasets. Tested on a single NVIDIA Tesla V100 GPU. Name API Metric Results GPU Days MNIST ImageClassifier Accuracy 99.04% 0.51 CIFAR10 ImageClassifier Accuracy 97.10% 1.8 IMDB TextClassifier Accuracy 93.93% 1.2 Titanic StructuredDataClassifier Accuracy 82.20% 0.007 California Housing StructuredDataRegression MSE 0.23 0.06","title":"Benchmarks"},{"location":"benchmarks/#benchmarks","text":"We track the performance of the latest AutoKeras release on the benchmark datasets. Tested on a single NVIDIA Tesla V100 GPU. Name API Metric Results GPU Days MNIST ImageClassifier Accuracy 99.04% 0.51 CIFAR10 ImageClassifier Accuracy 97.10% 1.8 IMDB TextClassifier Accuracy 93.93% 1.2 Titanic StructuredDataClassifier Accuracy 82.20% 0.007 California Housing StructuredDataRegression MSE 0.23 0.06","title":"Benchmarks"},{"location":"block/","text":"[source] ConvBlock autokeras . ConvBlock ( kernel_size = None , num_blocks = None , num_layers = None , filters = None , max_pooling = None , separable = None , dropout = None , ** kwargs ) Block for vanilla ConvNets. Arguments kernel_size Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The size of the kernel. If left unspecified, it will be tuned automatically. num_blocks Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of conv blocks, each of which may contain convolutional, max pooling, dropout, and activation. If left unspecified, it will be tuned automatically. num_layers Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or hyperparameters.Choice. The number of convolutional layers in each block. If left unspecified, it will be tuned automatically. filters Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of filters in the convolutional layers. If left unspecified, it will be tuned automatically. max_pooling Optional[bool] : Boolean. Whether to use max pooling layer in each block. If left unspecified, it will be tuned automatically. separable Optional[bool] : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. dropout Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or kerastuner.engine.hyperparameters. Choice range Between 0 and 1. The dropout rate after convolutional layers. If left unspecified, it will be tuned automatically. [source] DenseBlock autokeras . DenseBlock ( num_layers = None , num_units = None , use_batchnorm = None , dropout = None , ** kwargs ) Block for Dense layers. Arguments num_layers Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of Dense layers in the block. If left unspecified, it will be tuned automatically. num_units Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of units in each dense layer. If left unspecified, it will be tuned automatically. use_bn : Boolean. Whether to use BatchNormalization layers. If left unspecified, it will be tuned automatically. dropout Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or keras_tuner.engine.hyperparameters.Choice. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] Embedding autokeras . Embedding ( max_features = 20001 , pretraining = None , embedding_dim = None , dropout = None , ** kwargs ) Word embedding block for sequences. The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word. Arguments max_features int : Int. Size of the vocabulary. Must be set if not using TextToIntSequence before this block. Defaults to 20001. pretraining Optional[Union[str, keras_tuner.engine.hyperparameters.Choice]] : String or keras_tuner.engine.hyperparameters.Choice. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. embedding_dim Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. Output dimension of the Attention block. If left unspecified, it will be tuned automatically. dropout Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or keras_tuner.engine.hyperparameters.Choice. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] Merge autokeras . Merge ( merge_type = None , ** kwargs ) Merge block to merge multiple nodes into one. Arguments merge_type Optional[str] : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically. [source] ResNetBlock autokeras . ResNetBlock ( version = None , pretrained = None , ** kwargs ) Block for ResNet. Arguments version Optional[str] : String. 'v1', 'v2'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pretrained Optional[bool] : Boolean. Whether to use ImageNet pretrained weights. If left unspecified, it will be tuned automatically. [source] RNNBlock autokeras . RNNBlock ( return_sequences = False , bidirectional = None , num_layers = None , layer_type = None , ** kwargs ) An RNN Block. Arguments return_sequences bool : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional Optional[Union[bool, keras_tuner.engine.hyperparameters.Boolean]] : Boolean or keras_tuner.engine.hyperparameters.Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type Optional[Union[str, keras_tuner.engine.hyperparameters.Choice]] : String or or keras_tuner.engine.hyperparameters.Choice. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source] SpatialReduction autokeras . SpatialReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a spatial tensor, e.g. image, to a vector. Arguments reduction_type Optional[str] : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source] TemporalReduction autokeras . TemporalReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector. Arguments reduction_type Optional[str] : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source] XceptionBlock autokeras . XceptionBlock ( pretrained = None , ** kwargs ) Block for XceptionNet. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357 . The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments pretrained Optional[bool] : Boolean. Whether to use ImageNet pretrained weights. If left unspecified, it will be tuned automatically. [source] ImageBlock autokeras . ImageBlock ( block_type = None , normalize = None , augment = None , ** kwargs ) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type Optional[str] : String. 'resnet', 'xception', 'vanilla'. The type of Block to use. If unspecified, it will be tuned automatically. normalize Optional[bool] : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment Optional[bool] : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source] StructuredDataBlock autokeras . StructuredDataBlock ( categorical_encoding = True , normalize = None , seed = None , ** kwargs ) Block for structured data. Arguments categorical_encoding bool : Boolean. Whether to use the CategoricalToNumerical to encode the categorical features to numerical features. Defaults to True. normalize Optional[bool] : Boolean. Whether to normalize the features. If unspecified, it will be tuned automatically. seed Optional[int] : Int. Random seed. [source] TextBlock autokeras . TextBlock ( block_type = None , max_tokens = None , pretraining = None , ** kwargs ) Block for text data. Arguments block_type Optional[str] : String. 'vanilla', 'transformer', and 'ngram'. The type of Block to use. 'vanilla' and 'transformer' use a TextToIntSequence vectorizer, whereas 'ngram' uses TextToNgramVector. If unspecified, it will be tuned automatically. max_tokens Optional[int] : Int. The maximum size of the vocabulary. If left unspecified, it will be tuned automatically. pretraining Optional[str] : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. [source] ImageAugmentation autokeras . ImageAugmentation ( translation_factor = None , vertical_flip = None , horizontal_flip = None , rotation_factor = None , zoom_factor = None , contrast_factor = None , ** kwargs ) Collection of various image augmentation methods. Arguments translation_factor Optional[Union[float, Tuple[float, float], keras_tuner.engine.hyperparameters.Choice]] : A positive float represented as fraction value, or a tuple of 2 representing fraction for translation vertically and horizontally, or a kerastuner.engine.hyperparameters.Choice range of positive floats. For instance, translation_factor=0.2 result in a random translation factor within 20% of the width and height. If left unspecified, it will be tuned automatically. vertical_flip Optional[bool] : Boolean. Whether to flip the image vertically. If left unspecified, it will be tuned automatically. horizontal_flip Optional[bool] : Boolean. Whether to flip the image horizontally. If left unspecified, it will be tuned automatically. rotation_factor Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or kerastuner.engine.hyperparameters.Choice range between [0, 1]. A positive float represented as fraction of 2pi upper bound for rotating clockwise and counter-clockwise. When represented as a single float, lower = upper. If left unspecified, it will be tuned automatically. zoom_factor Optional[Union[float, Tuple[float, float], keras_tuner.engine.hyperparameters.Choice]] : A positive float represented as fraction value, or a tuple of 2 representing fraction for zooming vertically and horizontally, or a kerastuner.engine.hyperparameters.Choice range of positive floats. For instance, zoom_factor=0.2 result in a random zoom factor from 80% to 120%. If left unspecified, it will be tuned automatically. contrast_factor Optional[Union[float, Tuple[float, float], keras_tuner.engine.hyperparameters.Choice]] : A positive float represented as fraction of value, or a tuple of size 2 representing lower and upper bound, or a kerastuner.engine.hyperparameters.Choice range of floats to find the optimal value. When represented as a single float, lower = upper. The contrast factor will be randomly picked between [1.0 - lower, 1.0 + upper]. If left unspecified, it will be tuned automatically. [source] Normalization autokeras . Normalization ( axis =- 1 , ** kwargs ) Perform feature-wise normalization on data. Refer to Normalization layer in keras preprocessing layers for more information. Arguments axis int : Integer or tuple of integers, the axis or axes that should be normalized (typically the features axis). We will normalize each element in the specified axis. The default is '-1' (the innermost axis); 0 (the batch axis) is not allowed. [source] TextToIntSequence autokeras . TextToIntSequence ( output_sequence_length = None , max_tokens = 20000 , ** kwargs ) Convert raw texts to sequences of word indices. Arguments output_sequence_length Optional[int] : Int. The maximum length of a sentence. If unspecified, it would be tuned automatically. max_tokens int : Int. The maximum size of the vocabulary. Defaults to 20000. [source] TextToNgramVector autokeras . TextToNgramVector ( max_tokens = 20000 , ngrams = None , ** kwargs ) Convert raw texts to n-gram vectors. Arguments max_tokens int : Int. The maximum size of the vocabulary. Defaults to 20000. ngrams Optional[Union[int, Tuple[int]]] : Int or tuple of ints. Passing an integer will create ngrams up to that integer, and passing a tuple of integers will create ngrams for the specified values in the tuple. If left unspecified, it will be tuned automatically. [source] CategoricalToNumerical autokeras . CategoricalToNumerical ( ** kwargs ) Encode the categorical features to numerical features. [source] ClassificationHead autokeras . ClassificationHead ( num_classes = None , multi_label = False , loss = None , metrics = None , dropout = None , ** kwargs ) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found. Arguments num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use binary_crossentropy or categorical_crossentropy based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. dropout Optional[float] : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] RegressionHead autokeras . RegressionHead ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , dropout = None , ** kwargs ) Regression Dense layers. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. Arguments output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. multi_label : Boolean. Defaults to False. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use mean_squared_error . metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use mean_squared_error . dropout Optional[float] : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"Block"},{"location":"block/#convblock","text":"autokeras . ConvBlock ( kernel_size = None , num_blocks = None , num_layers = None , filters = None , max_pooling = None , separable = None , dropout = None , ** kwargs ) Block for vanilla ConvNets. Arguments kernel_size Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The size of the kernel. If left unspecified, it will be tuned automatically. num_blocks Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of conv blocks, each of which may contain convolutional, max pooling, dropout, and activation. If left unspecified, it will be tuned automatically. num_layers Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or hyperparameters.Choice. The number of convolutional layers in each block. If left unspecified, it will be tuned automatically. filters Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of filters in the convolutional layers. If left unspecified, it will be tuned automatically. max_pooling Optional[bool] : Boolean. Whether to use max pooling layer in each block. If left unspecified, it will be tuned automatically. separable Optional[bool] : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. dropout Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or kerastuner.engine.hyperparameters. Choice range Between 0 and 1. The dropout rate after convolutional layers. If left unspecified, it will be tuned automatically. [source]","title":"ConvBlock"},{"location":"block/#denseblock","text":"autokeras . DenseBlock ( num_layers = None , num_units = None , use_batchnorm = None , dropout = None , ** kwargs ) Block for Dense layers. Arguments num_layers Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of Dense layers in the block. If left unspecified, it will be tuned automatically. num_units Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of units in each dense layer. If left unspecified, it will be tuned automatically. use_bn : Boolean. Whether to use BatchNormalization layers. If left unspecified, it will be tuned automatically. dropout Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or keras_tuner.engine.hyperparameters.Choice. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"DenseBlock"},{"location":"block/#embedding","text":"autokeras . Embedding ( max_features = 20001 , pretraining = None , embedding_dim = None , dropout = None , ** kwargs ) Word embedding block for sequences. The input should be tokenized sequences with the same length, where each element of a sequence should be the index of the word. Arguments max_features int : Int. Size of the vocabulary. Must be set if not using TextToIntSequence before this block. Defaults to 20001. pretraining Optional[Union[str, keras_tuner.engine.hyperparameters.Choice]] : String or keras_tuner.engine.hyperparameters.Choice. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. embedding_dim Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. Output dimension of the Attention block. If left unspecified, it will be tuned automatically. dropout Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or keras_tuner.engine.hyperparameters.Choice. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"Embedding"},{"location":"block/#merge","text":"autokeras . Merge ( merge_type = None , ** kwargs ) Merge block to merge multiple nodes into one. Arguments merge_type Optional[str] : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically. [source]","title":"Merge"},{"location":"block/#resnetblock","text":"autokeras . ResNetBlock ( version = None , pretrained = None , ** kwargs ) Block for ResNet. Arguments version Optional[str] : String. 'v1', 'v2'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pretrained Optional[bool] : Boolean. Whether to use ImageNet pretrained weights. If left unspecified, it will be tuned automatically. [source]","title":"ResNetBlock"},{"location":"block/#rnnblock","text":"autokeras . RNNBlock ( return_sequences = False , bidirectional = None , num_layers = None , layer_type = None , ** kwargs ) An RNN Block. Arguments return_sequences bool : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional Optional[Union[bool, keras_tuner.engine.hyperparameters.Boolean]] : Boolean or keras_tuner.engine.hyperparameters.Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers Optional[Union[int, keras_tuner.engine.hyperparameters.Choice]] : Int or keras_tuner.engine.hyperparameters.Choice. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type Optional[Union[str, keras_tuner.engine.hyperparameters.Choice]] : String or or keras_tuner.engine.hyperparameters.Choice. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source]","title":"RNNBlock"},{"location":"block/#spatialreduction","text":"autokeras . SpatialReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a spatial tensor, e.g. image, to a vector. Arguments reduction_type Optional[str] : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source]","title":"SpatialReduction"},{"location":"block/#temporalreduction","text":"autokeras . TemporalReduction ( reduction_type = None , ** kwargs ) Reduce the dimension of a temporal tensor, e.g. output of RNN, to a vector. Arguments reduction_type Optional[str] : String. 'flatten', 'global_max' or 'global_avg'. If left unspecified, it will be tuned automatically. [source]","title":"TemporalReduction"},{"location":"block/#xceptionblock","text":"autokeras . XceptionBlock ( pretrained = None , ** kwargs ) Block for XceptionNet. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357 . The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments pretrained Optional[bool] : Boolean. Whether to use ImageNet pretrained weights. If left unspecified, it will be tuned automatically. [source]","title":"XceptionBlock"},{"location":"block/#imageblock","text":"autokeras . ImageBlock ( block_type = None , normalize = None , augment = None , ** kwargs ) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type Optional[str] : String. 'resnet', 'xception', 'vanilla'. The type of Block to use. If unspecified, it will be tuned automatically. normalize Optional[bool] : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment Optional[bool] : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source]","title":"ImageBlock"},{"location":"block/#structureddatablock","text":"autokeras . StructuredDataBlock ( categorical_encoding = True , normalize = None , seed = None , ** kwargs ) Block for structured data. Arguments categorical_encoding bool : Boolean. Whether to use the CategoricalToNumerical to encode the categorical features to numerical features. Defaults to True. normalize Optional[bool] : Boolean. Whether to normalize the features. If unspecified, it will be tuned automatically. seed Optional[int] : Int. Random seed. [source]","title":"StructuredDataBlock"},{"location":"block/#textblock","text":"autokeras . TextBlock ( block_type = None , max_tokens = None , pretraining = None , ** kwargs ) Block for text data. Arguments block_type Optional[str] : String. 'vanilla', 'transformer', and 'ngram'. The type of Block to use. 'vanilla' and 'transformer' use a TextToIntSequence vectorizer, whereas 'ngram' uses TextToNgramVector. If unspecified, it will be tuned automatically. max_tokens Optional[int] : Int. The maximum size of the vocabulary. If left unspecified, it will be tuned automatically. pretraining Optional[str] : String. 'random' (use random weights instead any pretrained model), 'glove', 'fasttext' or 'word2vec'. Use pretrained word embedding. If left unspecified, it will be tuned automatically. [source]","title":"TextBlock"},{"location":"block/#imageaugmentation","text":"autokeras . ImageAugmentation ( translation_factor = None , vertical_flip = None , horizontal_flip = None , rotation_factor = None , zoom_factor = None , contrast_factor = None , ** kwargs ) Collection of various image augmentation methods. Arguments translation_factor Optional[Union[float, Tuple[float, float], keras_tuner.engine.hyperparameters.Choice]] : A positive float represented as fraction value, or a tuple of 2 representing fraction for translation vertically and horizontally, or a kerastuner.engine.hyperparameters.Choice range of positive floats. For instance, translation_factor=0.2 result in a random translation factor within 20% of the width and height. If left unspecified, it will be tuned automatically. vertical_flip Optional[bool] : Boolean. Whether to flip the image vertically. If left unspecified, it will be tuned automatically. horizontal_flip Optional[bool] : Boolean. Whether to flip the image horizontally. If left unspecified, it will be tuned automatically. rotation_factor Optional[Union[float, keras_tuner.engine.hyperparameters.Choice]] : Float or kerastuner.engine.hyperparameters.Choice range between [0, 1]. A positive float represented as fraction of 2pi upper bound for rotating clockwise and counter-clockwise. When represented as a single float, lower = upper. If left unspecified, it will be tuned automatically. zoom_factor Optional[Union[float, Tuple[float, float], keras_tuner.engine.hyperparameters.Choice]] : A positive float represented as fraction value, or a tuple of 2 representing fraction for zooming vertically and horizontally, or a kerastuner.engine.hyperparameters.Choice range of positive floats. For instance, zoom_factor=0.2 result in a random zoom factor from 80% to 120%. If left unspecified, it will be tuned automatically. contrast_factor Optional[Union[float, Tuple[float, float], keras_tuner.engine.hyperparameters.Choice]] : A positive float represented as fraction of value, or a tuple of size 2 representing lower and upper bound, or a kerastuner.engine.hyperparameters.Choice range of floats to find the optimal value. When represented as a single float, lower = upper. The contrast factor will be randomly picked between [1.0 - lower, 1.0 + upper]. If left unspecified, it will be tuned automatically. [source]","title":"ImageAugmentation"},{"location":"block/#normalization","text":"autokeras . Normalization ( axis =- 1 , ** kwargs ) Perform feature-wise normalization on data. Refer to Normalization layer in keras preprocessing layers for more information. Arguments axis int : Integer or tuple of integers, the axis or axes that should be normalized (typically the features axis). We will normalize each element in the specified axis. The default is '-1' (the innermost axis); 0 (the batch axis) is not allowed. [source]","title":"Normalization"},{"location":"block/#texttointsequence","text":"autokeras . TextToIntSequence ( output_sequence_length = None , max_tokens = 20000 , ** kwargs ) Convert raw texts to sequences of word indices. Arguments output_sequence_length Optional[int] : Int. The maximum length of a sentence. If unspecified, it would be tuned automatically. max_tokens int : Int. The maximum size of the vocabulary. Defaults to 20000. [source]","title":"TextToIntSequence"},{"location":"block/#texttongramvector","text":"autokeras . TextToNgramVector ( max_tokens = 20000 , ngrams = None , ** kwargs ) Convert raw texts to n-gram vectors. Arguments max_tokens int : Int. The maximum size of the vocabulary. Defaults to 20000. ngrams Optional[Union[int, Tuple[int]]] : Int or tuple of ints. Passing an integer will create ngrams up to that integer, and passing a tuple of integers will create ngrams for the specified values in the tuple. If left unspecified, it will be tuned automatically. [source]","title":"TextToNgramVector"},{"location":"block/#categoricaltonumerical","text":"autokeras . CategoricalToNumerical ( ** kwargs ) Encode the categorical features to numerical features. [source]","title":"CategoricalToNumerical"},{"location":"block/#classificationhead","text":"autokeras . ClassificationHead ( num_classes = None , multi_label = False , loss = None , metrics = None , dropout = None , ** kwargs ) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found. Arguments num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use binary_crossentropy or categorical_crossentropy based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. dropout Optional[float] : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"ClassificationHead"},{"location":"block/#regressionhead","text":"autokeras . RegressionHead ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , dropout = None , ** kwargs ) Regression Dense layers. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. Arguments output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. multi_label : Boolean. Defaults to False. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use mean_squared_error . metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use mean_squared_error . dropout Optional[float] : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"RegressionHead"},{"location":"contributing/","text":"Contributing Guide Contributions are welcome, and greatly appreciated! This page is only a guide of the best practices of contributing code to AutoKeras. The best way to contribute is to join our community by reading this . We will get you started right away. Follow the tag of good first issue for the issues for beginner. Pull Request Guide Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project . Include \"resolves #issue_number\" in the description of the pull request if applicable. Briefly describe your contribution. Submit the pull request from the first day of your development and create it as a draft pull request . Click ready for review when finished and passed the all the checks. For the case of bug fixes, add new test cases which would fail before your bug fix. Setup Environment We introduce 3 different options: GitHub Codespaces , VS Code & Remote-Containers , the general setup . You can choose base on your preference. Option 1: GitHub Codespaces You can simply open the repository in GitHub Codespaces. The environment is already setup there. Option 2: VS Code & Remote-Containers Open VS Code. Install the Remote-Containers extension. Press F1 key. Enter Remote-Containers: Open Folder in Container to open the repository root folder. The environment is already setup there. Option 3: The General Setup Install Virtualenvwrapper . Create a new virtualenv named ak based on python3. mkvirtualenv -p python3 ak Please use this virtualenv for development. Clone the repo. Go to the repo directory. Run the following commands. workon ak pip install -e \".[tests]\" pip uninstall autokeras add2virtualenv . Run Tests GitHub Codespaces or VS Code & Remote-Containers If you are using \"GitHub Codespaces\" or \"VS Code & Remote-Containers\", you can simply open any *_test.py file under the tests directory, and wait a few seconds, you will see the test tab on the left of the window. General Setup If you are using the general setup. Activate the virtualenv. Go to the repo directory Run the following lines to run the tests. Run all the tests. pytest tests Run all the unit tests. pytest tests/autokeras Run all the integration tests. pytest tests/integration_tests Code Style You can run the following manually every time you want to format your code. 1. Run shell/format.sh to format your code. 2. Run shell/lint.sh to check. Docstrings Docstrings should follow our style. Just check the style of other docstrings in AutoKeras.","title":"Contributing Guide"},{"location":"contributing/#contributing-guide","text":"Contributions are welcome, and greatly appreciated! This page is only a guide of the best practices of contributing code to AutoKeras. The best way to contribute is to join our community by reading this . We will get you started right away. Follow the tag of good first issue for the issues for beginner.","title":"Contributing Guide"},{"location":"contributing/#pull-request-guide","text":"Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project . Include \"resolves #issue_number\" in the description of the pull request if applicable. Briefly describe your contribution. Submit the pull request from the first day of your development and create it as a draft pull request . Click ready for review when finished and passed the all the checks. For the case of bug fixes, add new test cases which would fail before your bug fix.","title":"Pull Request Guide"},{"location":"contributing/#setup-environment","text":"We introduce 3 different options: GitHub Codespaces , VS Code & Remote-Containers , the general setup . You can choose base on your preference.","title":"Setup Environment"},{"location":"contributing/#option-1-github-codespaces","text":"You can simply open the repository in GitHub Codespaces. The environment is already setup there.","title":"Option 1: GitHub Codespaces"},{"location":"contributing/#option-2-vs-code-remote-containers","text":"Open VS Code. Install the Remote-Containers extension. Press F1 key. Enter Remote-Containers: Open Folder in Container to open the repository root folder. The environment is already setup there.","title":"Option 2: VS Code &amp; Remote-Containers"},{"location":"contributing/#option-3-the-general-setup","text":"Install Virtualenvwrapper . Create a new virtualenv named ak based on python3. mkvirtualenv -p python3 ak Please use this virtualenv for development. Clone the repo. Go to the repo directory. Run the following commands. workon ak pip install -e \".[tests]\" pip uninstall autokeras add2virtualenv .","title":"Option 3: The General Setup"},{"location":"contributing/#run-tests","text":"","title":"Run Tests"},{"location":"contributing/#github-codespaces-or-vs-code-remote-containers","text":"If you are using \"GitHub Codespaces\" or \"VS Code & Remote-Containers\", you can simply open any *_test.py file under the tests directory, and wait a few seconds, you will see the test tab on the left of the window.","title":"GitHub Codespaces or VS Code &amp; Remote-Containers"},{"location":"contributing/#general-setup","text":"If you are using the general setup. Activate the virtualenv. Go to the repo directory Run the following lines to run the tests. Run all the tests. pytest tests Run all the unit tests. pytest tests/autokeras Run all the integration tests. pytest tests/integration_tests","title":"General Setup"},{"location":"contributing/#code-style","text":"You can run the following manually every time you want to format your code. 1. Run shell/format.sh to format your code. 2. Run shell/lint.sh to check.","title":"Code Style"},{"location":"contributing/#docstrings","text":"Docstrings should follow our style. Just check the style of other docstrings in AutoKeras.","title":"Docstrings"},{"location":"docker/","text":"Auto-Keras Docker Download Auto-Keras Docker image The following command download Auto-Keras docker image to your machine. docker pull haifengjin/autokeras:latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository Start Auto-Keras Docker container docker run -it --shm-size 2G haifengjin/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference ) Run application : To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run -it -v hostDir:/app --shm-size 2G haifengjin/autokeras python file.py Example : Let's download the mnist example and run it within the container. Download the example : curl https://raw.githubusercontent.com/keras-team/autokeras/master/examples/mnist.py --output mnist.py Run the mnist example : docker run -it -v \"$(pwd)\":/app --shm-size 2G haifengjin/autokeras python /app/mnist.py","title":"Docker"},{"location":"docker/#auto-keras-docker","text":"","title":"Auto-Keras Docker"},{"location":"docker/#download-auto-keras-docker-image","text":"The following command download Auto-Keras docker image to your machine. docker pull haifengjin/autokeras:latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository","title":"Download Auto-Keras Docker image"},{"location":"docker/#start-auto-keras-docker-container","text":"docker run -it --shm-size 2G haifengjin/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference )","title":"Start Auto-Keras Docker container"},{"location":"docker/#run-application","text":"To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run -it -v hostDir:/app --shm-size 2G haifengjin/autokeras python file.py","title":"Run application :"},{"location":"docker/#example","text":"Let's download the mnist example and run it within the container. Download the example : curl https://raw.githubusercontent.com/keras-team/autokeras/master/examples/mnist.py --output mnist.py Run the mnist example : docker run -it -v \"$(pwd)\":/app --shm-size 2G haifengjin/autokeras python /app/mnist.py","title":"Example :"},{"location":"image_classifier/","text":"[source] ImageClassifier autokeras . ImageClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , project_name = \"image_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras image classification class. Arguments num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. project_name str : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source] fit ImageClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be (samples, width, height) or (samples, width, height, channels). y Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs Optional[int] : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks Optional[List[tensorflow.keras.callbacks.Callback]] : List of Keras callbacks to apply during training and validation. validation_split Optional[float] : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data Optional[Union[tensorflow.data.Dataset, Tuple[Union[numpy.ndarray, tensorflow.data.Dataset], Union[numpy.ndarray, tensorflow.data.Dataset]]]] : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict ImageClassifier . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate ImageClassifier . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model ImageClassifier . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"ImageClassifier"},{"location":"image_classifier/#imageclassifier","text":"autokeras . ImageClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , project_name = \"image_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras image classification class. Arguments num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. project_name str : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source]","title":"ImageClassifier"},{"location":"image_classifier/#fit","text":"ImageClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be (samples, width, height) or (samples, width, height, channels). y Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs Optional[int] : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks Optional[List[tensorflow.keras.callbacks.Callback]] : List of Keras callbacks to apply during training and validation. validation_split Optional[float] : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data Optional[Union[tensorflow.data.Dataset, Tuple[Union[numpy.ndarray, tensorflow.data.Dataset], Union[numpy.ndarray, tensorflow.data.Dataset]]]] : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"image_classifier/#predict","text":"ImageClassifier . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"image_classifier/#evaluate","text":"ImageClassifier . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"image_classifier/#export_model","text":"ImageClassifier . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"image_regressor/","text":"[source] ImageRegressor autokeras . ImageRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , project_name = \"image_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras image regression class. Arguments output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use 'mean_squared_error'. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'mean_squared_error'. project_name str : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source] fit ImageRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be (samples, width, height) or (samples, width, height, channels). y Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs Optional[int] : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks Optional[List[tensorflow.keras.callbacks.Callback]] : List of Keras callbacks to apply during training and validation. validation_split Optional[float] : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data Optional[Union[numpy.ndarray, tensorflow.data.Dataset, Tuple[Union[numpy.ndarray, tensorflow.data.Dataset]]]] : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict ImageRegressor . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate ImageRegressor . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model ImageRegressor . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"ImageRegressor"},{"location":"image_regressor/#imageregressor","text":"autokeras . ImageRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , project_name = \"image_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras image regression class. Arguments output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use 'mean_squared_error'. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'mean_squared_error'. project_name str : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source]","title":"ImageRegressor"},{"location":"image_regressor/#fit","text":"ImageRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data x. The shape of the data should be (samples, width, height) or (samples, width, height, channels). y Optional[Union[numpy.ndarray, tensorflow.data.Dataset]] : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs Optional[int] : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks Optional[List[tensorflow.keras.callbacks.Callback]] : List of Keras callbacks to apply during training and validation. validation_split Optional[float] : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data Optional[Union[numpy.ndarray, tensorflow.data.Dataset, Tuple[Union[numpy.ndarray, tensorflow.data.Dataset]]]] : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"image_regressor/#predict","text":"ImageRegressor . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"image_regressor/#evaluate","text":"ImageRegressor . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"image_regressor/#export_model","text":"ImageRegressor . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"install/","text":"Requirements Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.3.0 : AutoKeras is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup. Install AutoKeras AutoKeras only support Python 3 . If you followed previous steps to use virtualenv to install tensorflow, you can just activate the virtualenv and use the following command to install AutoKeras. pip install git+https://github.com/keras-team/keras-tuner.git pip install autokeras If you did not use virtualenv, and you use python3 command to execute your python program, please use the following command to install AutoKeras. python3 -m pip install git+https://github.com/keras-team/keras-tuner.git python3 -m pip install autokeras","title":"Installation"},{"location":"install/#requirements","text":"Python 3 : Follow the TensorFlow install steps to install Python 3. Pip : Follow the TensorFlow install steps to install Pip. Tensorflow >= 2.3.0 : AutoKeras is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3. GPU Setup (Optional) : If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup.","title":"Requirements"},{"location":"install/#install-autokeras","text":"AutoKeras only support Python 3 . If you followed previous steps to use virtualenv to install tensorflow, you can just activate the virtualenv and use the following command to install AutoKeras. pip install git+https://github.com/keras-team/keras-tuner.git pip install autokeras If you did not use virtualenv, and you use python3 command to execute your python program, please use the following command to install AutoKeras. python3 -m pip install git+https://github.com/keras-team/keras-tuner.git python3 -m pip install autokeras","title":"Install AutoKeras"},{"location":"node/","text":"[source] ImageInput autokeras . ImageInput ( name = None , ** kwargs ) Input node for image data. The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be should be (samples, width, height) or (samples, width, height, channels). Arguments name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name. [source] Input autokeras . Input ( name = None , ** kwargs ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. Arguments name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name. [source] StructuredDataInput autokeras . StructuredDataInput ( column_names = None , column_types = None , name = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. The data should be two-dimensional with numerical or categorical values. Arguments column_names Optional[List[str]] : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will be obtained from the header of the csv file or the pandas.DataFrame. column_types Optional[Dict[str, str]] : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name. [source] TextInput autokeras . TextInput ( name = None , ** kwargs ) Input node for text data. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence. Arguments name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name.","title":"Node"},{"location":"node/#imageinput","text":"autokeras . ImageInput ( name = None , ** kwargs ) Input node for image data. The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be should be (samples, width, height) or (samples, width, height, channels). Arguments name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name. [source]","title":"ImageInput"},{"location":"node/#input","text":"autokeras . Input ( name = None , ** kwargs ) Input node for tensor data. The data should be numpy.ndarray or tf.data.Dataset. Arguments name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name. [source]","title":"Input"},{"location":"node/#structureddatainput","text":"autokeras . StructuredDataInput ( column_names = None , column_types = None , name = None , ** kwargs ) Input node for structured data. The input data should be numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. The data should be two-dimensional with numerical or categorical values. Arguments column_names Optional[List[str]] : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data. Defaults to None. If None, it will be obtained from the header of the csv file or the pandas.DataFrame. column_types Optional[Dict[str, str]] : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. A column will be judged as categorical if the number of different values is less than 5% of the number of instances. name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name. [source]","title":"StructuredDataInput"},{"location":"node/#textinput","text":"autokeras . TextInput ( name = None , ** kwargs ) Input node for text data. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence. Arguments name Optional[str] : String. The name of the input node. If unspecified, it will be set automatically with the class name.","title":"TextInput"},{"location":"structured_data_classifier/","text":"[source] StructuredDataClassifier autokeras . StructuredDataClassifier ( column_names = None , column_types = None , num_classes = None , multi_label = False , loss = None , metrics = None , project_name = \"structured_data_classifier\" , max_trials = 100 , directory = None , objective = \"val_accuracy\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras structured data classification class. Arguments column_names Optional[List[str]] : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types Optional[Dict] : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. project_name str : String. The name of the AutoModel. Defaults to 'structured_data_classifier'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source] fit StructuredDataClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict StructuredDataClassifier . predict ( x , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate StructuredDataClassifier . evaluate ( x , y = None , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model StructuredDataClassifier . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"StructuredDataClassifier"},{"location":"structured_data_classifier/#structureddataclassifier","text":"autokeras . StructuredDataClassifier ( column_names = None , column_types = None , num_classes = None , multi_label = False , loss = None , metrics = None , project_name = \"structured_data_classifier\" , max_trials = 100 , directory = None , objective = \"val_accuracy\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras structured data classification class. Arguments column_names Optional[List[str]] : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types Optional[Dict] : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. project_name str : String. The name of the AutoModel. Defaults to 'structured_data_classifier'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize. Defaults to 'val_accuracy'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source]","title":"StructuredDataClassifier"},{"location":"structured_data_classifier/#fit","text":"StructuredDataClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"structured_data_classifier/#predict","text":"StructuredDataClassifier . predict ( x , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"structured_data_classifier/#evaluate","text":"StructuredDataClassifier . evaluate ( x , y = None , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"structured_data_classifier/#export_model","text":"StructuredDataClassifier . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"structured_data_regressor/","text":"[source] StructuredDataRegressor autokeras . StructuredDataRegressor ( column_names = None , column_types = None , output_dim = None , loss = \"mean_squared_error\" , metrics = None , project_name = \"structured_data_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras structured data regression class. Arguments column_names Optional[List[str]] : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types Optional[Dict[str, str]] : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use 'mean_squared_error'. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'mean_squared_error'. project_name str : String. The name of the AutoModel. Defaults to 'structured_data_regressor'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source] fit StructuredDataRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, it can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict StructuredDataRegressor . predict ( x , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate StructuredDataRegressor . evaluate ( x , y = None , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model StructuredDataRegressor . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"StructuredDataRegressor"},{"location":"structured_data_regressor/#structureddataregressor","text":"autokeras . StructuredDataRegressor ( column_names = None , column_types = None , output_dim = None , loss = \"mean_squared_error\" , metrics = None , project_name = \"structured_data_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras structured data regression class. Arguments column_names Optional[List[str]] : A list of strings specifying the names of the columns. The length of the list should be equal to the number of columns of the data excluding the target column. Defaults to None. If None, it will obtained from the header of the csv file or the pandas.DataFrame. column_types Optional[Dict[str, str]] : Dict. The keys are the column names. The values should either be 'numerical' or 'categorical', indicating the type of that column. Defaults to None. If not None, the column_names need to be specified. If None, it will be inferred from the data. output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use 'mean_squared_error'. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'mean_squared_error'. project_name str : String. The name of the AutoModel. Defaults to 'structured_data_regressor'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source]","title":"StructuredDataRegressor"},{"location":"structured_data_regressor/#fit","text":"StructuredDataRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Training data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the training data. y : String, numpy.ndarray, or tensorflow.Dataset. Training data y. If the data is from a csv file, it should be a string, which is the name of the target column. Otherwise, it can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, we would use epochs equal to 1000 and early stopping with patience equal to 30. callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"structured_data_regressor/#predict","text":"StructuredDataRegressor . predict ( x , ** kwargs ) Predict the output for a given testing data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"structured_data_regressor/#evaluate","text":"StructuredDataRegressor . evaluate ( x , y = None , ** kwargs ) Evaluate the best model for the given data. Arguments x : String, numpy.ndarray, pandas.DataFrame or tensorflow.Dataset. Testing data x. If the data is from a csv file, it should be a string specifying the path of the csv file of the testing data. y : String, numpy.ndarray, or tensorflow.Dataset. Testing data y. If the data is from a csv file, it should be a string corresponding to the label column. **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"structured_data_regressor/#export_model","text":"StructuredDataRegressor . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"text_classifier/","text":"[source] TextClassifier autokeras . TextClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , project_name = \"text_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras text classification class. Arguments num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. project_name str : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source] fit TextClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict TextClassifier . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate TextClassifier . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model TextClassifier . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"TextClassifier"},{"location":"text_classifier/#textclassifier","text":"autokeras . TextClassifier ( num_classes = None , multi_label = False , loss = None , metrics = None , project_name = \"text_classifier\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras text classification class. Arguments num_classes Optional[int] : Int. Defaults to None. If None, it will be inferred from the data. multi_label bool : Boolean. Defaults to False. loss Optional[Union[str, Callable, tensorflow.keras.losses.Loss]] : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'accuracy'. project_name str : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source]","title":"TextClassifier"},{"location":"text_classifier/#fit","text":"TextClassifier . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"text_classifier/#predict","text":"TextClassifier . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"text_classifier/#evaluate","text":"TextClassifier . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"text_classifier/#export_model","text":"TextClassifier . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"text_regressor/","text":"[source] TextRegressor autokeras . TextRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , project_name = \"text_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras text regression class. Arguments output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use 'mean_squared_error'. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'mean_squared_error'. project_name str : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source] fit TextRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source] predict TextRegressor . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source] evaluate TextRegressor . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source] export_model TextRegressor . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"TextRegressor"},{"location":"text_regressor/#textregressor","text":"autokeras . TextRegressor ( output_dim = None , loss = \"mean_squared_error\" , metrics = None , project_name = \"text_regressor\" , max_trials = 100 , directory = None , objective = \"val_loss\" , tuner = None , overwrite = False , seed = None , max_model_size = None , ** kwargs ) AutoKeras text regression class. Arguments output_dim Optional[int] : Int. The number of output dimensions. Defaults to None. If None, it will be inferred from the data. loss Union[str, Callable, tensorflow.keras.losses.Loss] : A Keras loss function. Defaults to use 'mean_squared_error'. metrics Optional[Union[List[Union[str, Callable, tensorflow.keras.metrics.Metric]], List[List[Union[str, Callable, tensorflow.keras.metrics.Metric]]], Dict[str, Union[str, Callable, tensorflow.keras.metrics.Metric]]]] : A list of Keras metrics. Defaults to use 'mean_squared_error'. project_name str : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials int : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory Optional[Union[str, pathlib.Path]] : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. objective str : String. Name of model metric to minimize or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'. tuner Optional[Union[str, Type[autokeras.engine.tuner.AutoTuner]]] : String or subclass of AutoTuner. If string, it should be one of 'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a subclass of AutoTuner. If left unspecified, it uses a task specific tuner, which first evaluates the most commonly used models for the task before exploring other models. overwrite bool : Boolean. Defaults to False . If False , reloads an existing project of the same name if one is found. Otherwise, overwrites the project. seed Optional[int] : Int. Random seed. max_model_size Optional[int] : Int. Maximum number of scalars in the parameters of a model. Models larger than this are rejected. **kwargs : Any arguments supported by AutoModel. [source]","title":"TextRegressor"},{"location":"text_regressor/#fit","text":"TextRegressor . fit ( x = None , y = None , epochs = None , callbacks = None , validation_split = 0.2 , validation_data = None , ** kwargs ) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. The input data should be numpy.ndarray or tf.data.Dataset. The data should be one dimensional. Each element in the data should be a string which is a full sentence. y : numpy.ndarray or tensorflow.Dataset. Training data y. The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical. epochs : Int. The number of epochs to train each model during the search. If unspecified, by default we train for a maximum of 1000 epochs, but we stop training if the validation loss stops improving for 10 epochs (unless you specified an EarlyStopping callback as part of the callbacks argument, in which case the EarlyStopping callback you specified will determine early stopping). callbacks : List of Keras callbacks to apply during training and validation. validation_split : Float between 0 and 1. Defaults to 0.2. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. The best model found would be fit on the entire dataset including the validation data. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . The type of the validation data should be the same as the training data. The best model found would be fit on the training dataset without the validation data. **kwargs : Any arguments supported by keras.Model.fit . Returns history: A Keras History object corresponding to the best model. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). [source]","title":"fit"},{"location":"text_regressor/#predict","text":"TextRegressor . predict ( x , batch_size = 32 , verbose = 1 , ** kwargs ) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.predict **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results. [source]","title":"predict"},{"location":"text_regressor/#evaluate","text":"TextRegressor . evaluate ( x , y = None , batch_size = 32 , verbose = 1 , ** kwargs ) Evaluate the best model for the given data. Arguments x : Any allowed types according to the input node. Testing data. y : Any allowed types according to the head. Testing targets. Defaults to None. batch_size : Number of samples per batch. If unspecified, batch_size will default to 32. verbose : Verbosity mode. 0 = silent, 1 = progress bar. Controls the verbosity of keras.Model.evaluate **kwargs : Any arguments supported by keras.Model.evaluate. Returns Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. [source]","title":"evaluate"},{"location":"text_regressor/#export_model","text":"TextRegressor . export_model () Export the best Keras Model. Returns keras.Model instance. The best model found during the search, loaded with trained weights.","title":"export_model"},{"location":"utils/","text":"[source] image_dataset_from_directory autokeras . image_dataset_from_directory ( directory , batch_size = 32 , color_mode = \"rgb\" , image_size = ( 256 , 256 ), interpolation = \"bilinear\" , shuffle = True , seed = None , validation_split = None , subset = None , ) Generates a tf.data.Dataset from image files in a directory. If your directory structure is: main_directory/ ...class_a/ ......a_image_1.jpg ......a_image_2.jpg ...class_b/ ......b_image_1.jpg ......b_image_2.jpg Then calling image_dataset_from_directory(main_directory) will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b , together with labels 'class_a' and 'class_b'. Supported image formats: jpeg, png, bmp, gif. Animated gifs are truncated to the first frame. Arguments directory str : Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing images for a class. Otherwise, the directory structure is ignored. batch_size int : Size of the batches of data. Default: 32. color_mode str : One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels. image_size Tuple[int, int] : Size to resize images to after they are read from disk. Defaults to (256, 256) . Since the pipeline processes batches of images that must all have the same size, this must be provided. interpolation str : String, the interpolation method used when resizing images. Defaults to bilinear . Supports bilinear , nearest , bicubic , area , lanczos3 , lanczos5 , gaussian , mitchellcubic . shuffle bool : Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order. seed Optional[int] : Optional random seed for shuffling and transformations. validation_split Optional[float] : Optional float between 0 and 1, fraction of data to reserve for validation. subset Optional[str] : One of \"training\" or \"validation\". Only used if validation_split is set. Returns A tf.data.Dataset object, which yields a tuple (texts, labels) , where images has shape (batch_size, image_size[0], image_size[1], num_channels) where labels has shape (batch_size,) and type of tf.string. - if color_mode is grayscale , there's 1 channel in the image tensors. - if color_mode is rgb , there are 3 channel in the image tensors. - if color_mode is rgba , there are 4 channel in the image tensors. [source] text_dataset_from_directory autokeras . text_dataset_from_directory ( directory , batch_size = 32 , max_length = None , shuffle = True , seed = None , validation_split = None , subset = None ) Generates a tf.data.Dataset from text files in a directory. If your directory structure is: main_directory/ ...class_a/ ......a_text_1.txt ......a_text_2.txt ...class_b/ ......b_text_1.txt ......b_text_2.txt Then calling text_dataset_from_directory(main_directory) will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b , together with labels 'class_a' and 'class_b'. Only .txt files are supported at this time. Arguments directory str : Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored. batch_size int : Size of the batches of data. Defaults to 32. max_length Optional[int] : Maximum size of a text string. Texts longer than this will be truncated to max_length . shuffle bool : Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order. seed Optional[int] : Optional random seed for shuffling and transformations. validation_split Optional[float] : Optional float between 0 and 1, fraction of data to reserve for validation. subset Optional[str] : One of \"training\" or \"validation\". Only used if validation_split is set. Returns A tf.data.Dataset object, which yields a tuple (texts, labels) , where both has shape (batch_size,) and type of tf.string.","title":"Utils"},{"location":"utils/#image_dataset_from_directory","text":"autokeras . image_dataset_from_directory ( directory , batch_size = 32 , color_mode = \"rgb\" , image_size = ( 256 , 256 ), interpolation = \"bilinear\" , shuffle = True , seed = None , validation_split = None , subset = None , ) Generates a tf.data.Dataset from image files in a directory. If your directory structure is: main_directory/ ...class_a/ ......a_image_1.jpg ......a_image_2.jpg ...class_b/ ......b_image_1.jpg ......b_image_2.jpg Then calling image_dataset_from_directory(main_directory) will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b , together with labels 'class_a' and 'class_b'. Supported image formats: jpeg, png, bmp, gif. Animated gifs are truncated to the first frame. Arguments directory str : Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing images for a class. Otherwise, the directory structure is ignored. batch_size int : Size of the batches of data. Default: 32. color_mode str : One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1, 3, or 4 channels. image_size Tuple[int, int] : Size to resize images to after they are read from disk. Defaults to (256, 256) . Since the pipeline processes batches of images that must all have the same size, this must be provided. interpolation str : String, the interpolation method used when resizing images. Defaults to bilinear . Supports bilinear , nearest , bicubic , area , lanczos3 , lanczos5 , gaussian , mitchellcubic . shuffle bool : Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order. seed Optional[int] : Optional random seed for shuffling and transformations. validation_split Optional[float] : Optional float between 0 and 1, fraction of data to reserve for validation. subset Optional[str] : One of \"training\" or \"validation\". Only used if validation_split is set. Returns A tf.data.Dataset object, which yields a tuple (texts, labels) , where images has shape (batch_size, image_size[0], image_size[1], num_channels) where labels has shape (batch_size,) and type of tf.string. - if color_mode is grayscale , there's 1 channel in the image tensors. - if color_mode is rgb , there are 3 channel in the image tensors. - if color_mode is rgba , there are 4 channel in the image tensors. [source]","title":"image_dataset_from_directory"},{"location":"utils/#text_dataset_from_directory","text":"autokeras . text_dataset_from_directory ( directory , batch_size = 32 , max_length = None , shuffle = True , seed = None , validation_split = None , subset = None ) Generates a tf.data.Dataset from text files in a directory. If your directory structure is: main_directory/ ...class_a/ ......a_text_1.txt ......a_text_2.txt ...class_b/ ......b_text_1.txt ......b_text_2.txt Then calling text_dataset_from_directory(main_directory) will return a tf.data.Dataset that yields batches of texts from the subdirectories class_a and class_b , together with labels 'class_a' and 'class_b'. Only .txt files are supported at this time. Arguments directory str : Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing text files for a class. Otherwise, the directory structure is ignored. batch_size int : Size of the batches of data. Defaults to 32. max_length Optional[int] : Maximum size of a text string. Texts longer than this will be truncated to max_length . shuffle bool : Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order. seed Optional[int] : Optional random seed for shuffling and transformations. validation_split Optional[float] : Optional float between 0 and 1, fraction of data to reserve for validation. subset Optional[str] : One of \"training\" or \"validation\". Only used if validation_split is set. Returns A tf.data.Dataset object, which yields a tuple (texts, labels) , where both has shape (batch_size,) and type of tf.string.","title":"text_dataset_from_directory"},{"location":"examples/automodel_with_cnn/","text":"import numpy as np import tensorflow as tf import autokeras as ak # Prepare example Data - Shape 1D num_instances = 100 num_features = 5 x_train = np . random . rand ( num_instances , num_features ) . astype ( np . float32 ) y_train = np . zeros ( num_instances ) . astype ( np . float32 ) y_train [ 0 : int ( num_instances / 2 )] = 1 x_test = np . random . rand ( num_instances , num_features ) . astype ( np . float32 ) y_test = np . zeros ( num_instances ) . astype ( np . float32 ) y_train [ 0 : int ( num_instances / 2 )] = 1 x_train = np . expand_dims ( x_train , axis = 2 ) # This step it's very important an CNN will only accept this data shape print ( x_train . shape ) print ( y_train . shape ) # Prepare Automodel for search input_node = ak . Input () output_node = ak . ConvBlock ()( input_node ) # output_node = ak.DenseBlock()(output_node) #optional # output_node = ak.SpatialReduction()(output_node) #optional output_node = ak . ClassificationHead ( num_classes = 2 , multi_label = True )( output_node ) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) # Search auto_model . fit ( x_train , y_train , epochs = 1 ) print ( auto_model . evaluate ( x_test , y_test )) # Export as a Keras Model model = auto_model . export_model () print ( type ( model . summary ())) # print model as image tf . keras . utils . plot_model ( model , show_shapes = True , expand_nested = True , to_file = \"name.png\" )","title":"Automodel with cnn"},{"location":"examples/celeb_age/","text":"Regression tasks estimate a numeric variable, such as the price of a house or voter turnout. This example is adapted from a notebook which estimates a person's age from their image, trained on the IMDB-WIKI photographs of famous people. First, prepare your image data in a numpy.ndarray or tensorflow.Dataset format. Each image must have the same shape, meaning each has the same width, height, and color channels as other images in the set. Regression tasks estimate a numeric variable , such as the price of a house or voter turnout . This example is adapted from a [ notebook ]( https : // gist . github . com / mapmeld / 98 d1e9839f2d1f9c4ee197953661ed07 ) which estimates a person 's age from their image, trained on the [ IMDB - WIKI ]( https : // data . vision . ee . ethz . ch / cvl / rrothe / imdb - wiki / ) photographs of famous people . First , prepare your image data in a numpy . ndarray or tensorflow . Dataset format . Each image must have the same shape , meaning each has the same width , height , and color channels as other images in the set . \"\"\" from datetime import datetime from datetime import timedelta import numpy as np import pandas as pd import tensorflow as tf from google.colab import drive from PIL import Image from scipy.io import loadmat import autokeras as ak \"\"\" ### Connect your Google Drive for Data \"\"\" drive.mount(\"/content/drive\") \"\"\" ### Install AutoKeras and TensorFlow Download the master branch to your Google Drive for this tutorial . In general , you can use * pip install autokeras * . \"\"\" \"\"\" shell ! pip install - v \"/content/drive/My Drive/AutoKeras-dev/autokeras-master.zip\" ! pip uninstall keras - tuner ! pip install git + git : // github . com / keras - team / keras - tuner . git @d2d69cba21a0b482a85ce2a38893e2322e139c01 \"\"\" \"\"\" shell ! pip install tensorflow == 2.2.0 \"\"\" \"\"\" ###**Import IMDB Celeb images and metadata** \"\"\" \"\"\" shell ! mkdir \"./drive/My Drive/mlin/celebs\" \"\"\" \"\"\" shell ! wget - O \"./drive/My Drive/mlin/celebs/imdb_0.tar\" https : // data . vision . ee . ethz . ch / cvl / rrothe / imdb - wiki / static / imdb_0 . tar \"\"\" \"\"\" shell ! cd \"./drive/My Drive/mlin/celebs\" && tar - xf imdb_0 . tar ! rm \"./drive/My Drive/mlin/celebs/imdb_0.tar\" \"\"\" \"\"\" Uncomment and run the below cell if you need to re - run the cells again and above don 't need to install everything from the beginning. \"\"\" # ! cd ./drive/My\\ Drive/mlin/celebs. \"\"\" shell ! ls \"./drive/My Drive/mlin/celebs/imdb/\" \"\"\" \"\"\" shell ! wget https : // data . vision . ee . ethz . ch / cvl / rrothe / imdb - wiki / static / imdb_meta . tar ! tar - xf imdb_meta . tar ! rm imdb_meta . tar \"\"\" \"\"\" ###**Converting from MATLAB date to actual Date-of-Birth** \"\"\" def datenum_to_datetime(datenum): \"\"\" Convert Matlab datenum into Python datetime . \"\"\" days = datenum % 1 hours = days % 1 * 24 minutes = hours % 1 * 60 seconds = minutes % 1 * 60 try: return ( datetime.fromordinal(int(datenum)) + timedelta(days=int(days)) + timedelta(hours=int(hours)) + timedelta(minutes=int(minutes)) + timedelta(seconds=round(seconds)) - timedelta(days=366) ) except Exception: return datenum_to_datetime(700000) print(datenum_to_datetime(734963)) \"\"\" ### **Opening MatLab file to Pandas DataFrame** \"\"\" x = loadmat(\"imdb/imdb.mat\") mdata = x[\"imdb\"] # variable in mat file mdtype = mdata.dtype # dtypes of structures are \"unsized objects\" ndata = {n: mdata[n][0, 0] for n in mdtype.names} columns = [n for n, v in ndata.items()] rows = [] for col in range(0, 10): values = list(ndata.items())[col] for num, val in enumerate(values[1][0], start=0): if col == 0: rows.append([]) if num > 0: if columns[col] == \"dob\": rows[num].append(datenum_to_datetime(int(val))) elif columns[col] == \"photo_taken\": rows[num].append(datetime(year=int(val), month=6, day=30)) else: rows[num].append(val) dt = map(lambda row: np.array(row), np.array(rows[1:])) df = pd.DataFrame(data=dt, index=range(0, len(rows) - 1), columns=columns) print(df.head()) print(columns) print(df[\"full_path\"]) \"\"\" ### **Calculating age at time photo was taken** \"\"\" df[\"age\"] = (df[\"photo_taken\"] - df[\"dob\"]).astype(\"int\") / 31558102e9 print(df[\"age\"]) \"\"\" ### **Creating dataset** * We sample 200 of the images which were included in this first download . * Images are resized to 128 x128 to standardize shape and conserve memory * RGB images are converted to grayscale to standardize shape * Ages are converted to ints \"\"\" def df2numpy(train_set): images = [] for img_path in train_set[\"full_path\"]: img = ( Image.open(\"./drive/My Drive/mlin/celebs/imdb/\" + img_path[0]) .resize((128, 128)) .convert(\"L\") ) images.append(np.asarray(img, dtype=\"int32\")) image_inputs = np.array(images) ages = train_set[\"age\"].astype(\"int\").to_numpy() return image_inputs, ages train_set = df[df[\"full_path\"] < \"02\"].sample(200) train_imgs, train_ages = df2numpy(train_set) test_set = df[df[\"full_path\"] < \"02\"].sample(100) test_imgs, test_ages = df2numpy(test_set) \"\"\" ### **Training using AutoKeras** \"\"\" # Initialize the image regressor reg = ak.ImageRegressor(max_trials=15) # AutoKeras tries 15 different models. # Find the best model for the given training data reg.fit(train_imgs, train_ages) # Predict with the chosen model: # predict_y = reg.predict(test_images) # Uncomment if required # Evaluate the chosen model with testing data print(reg.evaluate(train_imgs, train_ages)) \"\"\" ### **Validation Data** By default , AutoKeras use the last 20 % of training data as validation data . As shown in the example below , you can use validation_split to specify the percentage . \"\"\" reg.fit( train_imgs, train_ages, # Split the training data and use the last 15% as validation data. validation_split=0.15, epochs=3, ) \"\"\" You can also use your own validation set instead of splitting it from the training data with validation_data . \"\"\" split = 460000 x_val = train_imgs[split:] y_val = train_ages[split:] x_train = train_imgs[:split] y_train = train_ages[:split] reg.fit( x_train, y_train, # Use your own validation set. validation_data=(x_val, y_val), epochs=3, ) \"\"\" ### **Customized Search Space** For advanced users , you may customize your search space by using AutoModel instead of ImageRegressor . You can configure the ImageBlock for some high - level configurations , e . g . , block_type for the type of neural network to search , normalize for whether to do data normalization , augment for whether to do data augmentation . You can also choose not to specify these arguments , which would leave the different choices to be tuned automatically . See the following example for detail . \"\"\" input_node = ak.ImageInput() output_node = ak.ImageBlock( # Only search ResNet architectures. block_type=\"resnet\", # Normalize the dataset. normalize=True, # Do not do data augmentation. augment=False, )(input_node) output_node = ak.RegressionHead()(output_node) reg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10) reg.fit(x_train, y_train, epochs=3) \"\"\" The usage of AutoModel is similar to the functional API of Keras . Basically , you are building a graph , whose edges are blocks and the nodes are intermediate outputs of blocks . To add an edge from input_node to output_node with output_node = ak . some_block ( input_node ) . You can even also use more fine grained blocks to customize the search space even further . See the following example . \"\"\" input_node = ak.ImageInput() output_node = ak.Normalization()(input_node) output_node = ak.ImageAugmentation(translation_factor=0.3)(output_node) output_node = ak.ResNetBlock(version=\"v2\")(output_node) output_node = ak.RegressionHead()(output_node) clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10) clf.fit(x_train, y_train, epochs=3) \"\"\" ### **Data Format** \"\"\" \"\"\" The AutoKeras ImageClassifier is quite flexible for the data format . For the image , it accepts data formats both with and without the channel dimension . The images in the IMDB - Wiki dataset do not have a channel dimension . Each image is a matrix with shape ( 128 , 128 ) . AutoKeras also accepts images with a channel dimension at last , e . g . , ( 32 , 32 , 3 ), ( 28 , 28 , 1 ) . For the classification labels , AutoKeras accepts both plain labels , i . e . strings or integers , and one - hot encoded labels , i . e . vectors of 0 s and 1 s . So if you prepare your data in the following way , the ImageClassifier should still work . \"\"\" # Reshape the images to have the channel dimension. train_imgs = train_imgs.reshape(train_imgs.shape + (1,)) test_imgs = test_imgs.reshape(test_imgs.shape + (1,)) print(train_imgs.shape) # (200, 128, 128, 1) print(test_imgs.shape) # (100, 128, 128, 1) print(train_ages[:3]) \"\"\" We also support using tf . data . Dataset format for the training data . In this case , the images would have to be 3 - dimentional . The labels have to be one - hot encoded for multi - class classification to be wrapped into tensorflow Dataset . \"\"\" train_set = tf.data.Dataset.from_tensor_slices(((train_imgs,), (train_ages,))) test_set = tf.data.Dataset.from_tensor_slices(((test_imgs,), (test_ages,))) reg = ak.ImageRegressor(max_trials=15) # Feed the tensorflow Dataset to the classifier. reg.fit(train_set) # Predict with the best model. predicted_y = clf.predict(test_set) # Evaluate the best model with testing data. print(clf.evaluate(test_set)) \"\"\" ## References [ Main Reference Notebook ]( https : // gist . github . com / mapmeld / 98 d1e9839f2d1f9c4ee197953661ed07 ), [ Dataset ]( https : // data . vision . ee . ethz . ch / cvl / rrothe / imdb - wiki / ), [ ImageRegressor ]( / image_regressor ), [ ResNetBlock ]( / block / #resnetblock-class), [ ImageInput ]( / node / #imageinput-class), [ AutoModel ]( / auto_model / #automodel-class), [ ImageBlock ]( / block / #imageblock-class), [ Normalization ]( / preprocessor / #normalization-class), [ ImageAugmentation ]( / preprocessor / #image-augmentation-class), [ RegressionHead ]( / head / #regressionhead-class). \"\"\"","title":"Celeb age"},{"location":"examples/cifar10/","text":"import autokeras as ak # Prepare the dataset. ( x_train , y_train ), ( x_test , y_test ) = cifar10 . load_data () # Initialize the ImageClassifier. clf = ak . ImageClassifier ( max_trials = 3 ) # Search for the best model. clf . fit ( x_train , y_train , epochs = 5 ) # Evaluate on the testing data. print ( \"Accuracy: {accuracy} \" . format ( accuracy = clf . evaluate ( x_test , y_test )[ 1 ]))","title":"Cifar10"},{"location":"examples/imdb/","text":"Search for a good model for the IMDB dataset. Search for a good model for the [ IMDB ]( https : // keras . io / datasets / #imdb-movie-reviews-sentiment-classification) dataset. \"\"\" import numpy as np import tensorflow as tf import autokeras as ak def imdb_raw(): max_features = 20000 index_offset = 3 # word index offset (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data( num_words=max_features, index_from=index_offset ) x_train = x_train y_train = y_train.reshape(-1, 1) x_test = x_test y_test = y_test.reshape(-1, 1) word_to_id = tf.keras.datasets.imdb.get_word_index() word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()} word_to_id[\"<PAD>\"] = 0 word_to_id[\"<START>\"] = 1 word_to_id[\"<UNK>\"] = 2 id_to_word = {value: key for key, value in word_to_id.items()} x_train = list( map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_train) ) x_test = list( map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_test) ) x_train = np.array(x_train, dtype=np.str) x_test = np.array(x_test, dtype=np.str) return (x_train, y_train), (x_test, y_test) # Prepare the data. (x_train, y_train), (x_test, y_test) = imdb_raw() print(x_train.shape) # (25000,) print(y_train.shape) # (25000, 1) print(x_train[0][:50]) # <START> this film was just brilliant casting <UNK> # Initialize the TextClassifier clf = ak.TextClassifier(max_trials=3) # Search for the best model. clf.fit(x_train, y_train, epochs=2, batch_size=8) # Evaluate on the testing data. print(\"Accuracy: {accuracy} \".format(accuracy=clf.evaluate(x_test, y_test)))","title":"Imdb"},{"location":"examples/iris/","text":"shell pip install -q -U autokeras==1.0.5 pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1 pip install - q - U autokeras == 1.0.5 pip install - q git + https : // github . com / keras - team / keras - tuner . git @ 1.0.2 rc1 \"\"\" import os import pandas as pd import tensorflow as tf import autokeras as ak \"\"\" Search for a good model for the [ iris ]( https : // www . tensorflow . org / datasets / catalog / iris ) dataset . \"\"\" # Prepare the dataset. train_dataset_url = ( \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\" ) train_dataset_fp = tf.keras.utils.get_file( fname=os.path.basename(train_dataset_url), origin=train_dataset_url ) test_dataset_url = ( \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\" ) test_dataset_fp = tf.keras.utils.get_file( fname=os.path.basename(test_dataset_url), origin=test_dataset_url ) column_names = [ \"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\", ] feature_names = column_names[:-1] label_name = column_names[-1] class_names = [\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"] train = pd.read_csv(train_dataset_fp, names=column_names, header=0) test = pd.read_csv(test_dataset_fp, names=column_names, header=0) print(train.shape) # (120, 5) print(test.shape) # (30, 5) # Initialize the StructuredDataClassifier. clf = ak.StructuredDataClassifier( max_trials=5, overwrite=True, ) # Search for the best model with EarlyStopping. cbs = [ tf.keras.callbacks.EarlyStopping(patience=3), ] clf.fit( x=train[feature_names], y=train[label_name], epochs=200, callbacks=cbs, ) # Evaluate on the testing data. print( \"Accuracy: {accuracy} \".format( accuracy=clf.evaluate(x=test[feature_names], y=test[label_name]) ) )","title":"Iris"},{"location":"examples/mnist/","text":"Search for a good model for the MNIST dataset. Search for a good model for the [ MNIST ]( https : // keras . io / datasets / #mnist-database-of-handwritten-digits) dataset. \"\"\" from tensorflow.keras.datasets import mnist import autokeras as ak # Prepare the dataset. (x_train, y_train), (x_test, y_test) = mnist.load_data() print(x_train.shape) # (60000, 28, 28) print(y_train.shape) # (60000,) print(y_train[:3]) # array([7, 2, 1], dtype=uint8) # Initialize the ImageClassifier. clf = ak.ImageClassifier(max_trials=3) # Search for the best model. clf.fit(x_train, y_train, epochs=10) # Evaluate on the testing data. print(\"Accuracy: {accuracy} \".format(accuracy=clf.evaluate(x_test, y_test)))","title":"Mnist"},{"location":"examples/new_pop/","text":"shell pip install autokeras pip install autokeras \"\"\" import pandas as pd import autokeras as ak \"\"\" ## Social Media Articles Example Regression tasks estimate a numeric variable , such as the price of a house or a person 's age. This example estimates the view counts for an article on social media platforms , trained on a [ News Popularity ]( https : // archive . ics . uci . edu / ml / datasets / News + Popularity + in + Multiple + Social + Media + Platforms ) dataset collected from 2015 - 2016. First , prepare your text data in a ` numpy . ndarray ` or ` tensorflow . Dataset ` format . \"\"\" # converting from other formats (such as pandas) to numpy df = pd.read_csv(\"./News_Final.csv\") text_inputs = df.Title.to_numpy(dtype=\"str\") media_success_outputs = df.Facebook.to_numpy(dtype=\"int\") \"\"\" Next , initialize and train the [ TextRegressor ]( / text_regressor ) . \"\"\" # Initialize the text regressor reg = ak.TextRegressor(max_trials=15) # AutoKeras tries 15 different models. # Find the best model for the given training data reg.fit(text_inputs, media_success_outputs) # Predict with the chosen model: predict_y = reg.predict(text_inputs) \"\"\" If your text source has a larger vocabulary ( number of distinct words ), you may need to create a custom pipeline in AutoKeras to increase the ` max_tokens ` parameter . \"\"\" text_input = (df.Title + \" \" + df.Headline).to_numpy(dtype=\"str\") # text input and tokenization input_node = ak.TextInput() output_node = ak.TextToIntSequence(max_tokens=20000)(input_node) # regression output output_node = ak.RegressionHead()(output_node) # initialize AutoKeras and find the best model reg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=15) reg.fit(text_input, media_success_outputs) \"\"\" Measure the accuracy of the regressor on an independent test set : \"\"\" print(reg.evaluate(text_input, media_success_outputs))","title":"New pop"},{"location":"examples/reuters/","text":"shell !pip install -q -U pip !pip install -q -U autokeras==1.0.8 !pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1 ! pip install - q - U pip ! pip install - q - U autokeras == 1.0.8 ! pip install - q git + https : // github . com / keras - team / keras - tuner . git @ 1.0.2 rc1 \"\"\" import numpy as np import tensorflow as tf from tensorflow.keras.datasets import reuters import autokeras as ak \"\"\" Search for a good model for the [ Reuters ]( https : // keras . io / ja / datasets / #_5) dataset. \"\"\" # Prepare the dataset. def reuters_raw(max_features=20000): index_offset = 3 # word index offset (x_train, y_train), (x_test, y_test) = reuters.load_data( num_words=max_features, index_from=index_offset ) x_train = x_train y_train = y_train.reshape(-1, 1) x_test = x_test y_test = y_test.reshape(-1, 1) word_to_id = reuters.get_word_index() word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()} word_to_id[\"<PAD>\"] = 0 word_to_id[\"<START>\"] = 1 word_to_id[\"<UNK>\"] = 2 id_to_word = {value: key for key, value in word_to_id.items()} x_train = list( map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_train) ) x_test = list( map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_test) ) x_train = np.array(x_train, dtype=np.str) x_test = np.array(x_test, dtype=np.str) return (x_train, y_train), (x_test, y_test) # Prepare the data. (x_train, y_train), (x_test, y_test) = reuters_raw() print(x_train.shape) # (8982,) print(y_train.shape) # (8982, 1) print(x_train[0][:50]) # <START> <UNK> <UNK> said as a result of its decemb # Initialize the TextClassifier clf = ak.TextClassifier( max_trials=5, overwrite=True, ) # Callback to avoid overfitting with the EarlyStopping. cbs = [ tf.keras.callbacks.EarlyStopping(patience=3), ] # Search for the best model. clf.fit(x_train, y_train, epochs=10, callback=cbs) # Evaluate on the testing data. print(\"Accuracy: {accuracy} \".format(accuracy=clf.evaluate(x_test, y_test)))","title":"Reuters"},{"location":"examples/titanic/","text":"Search for a good model for the Titanic dataset. Search for a good model for the [ Titanic ]( https : // www . kaggle . com / c / titanic ) dataset . \"\"\" import timeit import tensorflow as tf import autokeras as ak TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\" TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\" def main(): # Initialize the classifier. train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL) test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL) clf = ak.StructuredDataClassifier( max_trials=10, directory=\"tmp_dir\", overwrite=True ) start_time = timeit.default_timer() # x is the path to the csv file. y is the column name of the column to predict. clf.fit(train_file_path, \"survived\") stop_time = timeit.default_timer() # Evaluate the accuracy of the found model. accuracy = clf.evaluate(test_file_path, \"survived\")[1] print(\"Accuracy: {accuracy} %\".format(accuracy=round(accuracy * 100, 2))) print( \"Total time: {time} seconds.\".format(time=round(stop_time - start_time, 2)) ) if __name__ == \"__main__\": main()","title":"Titanic"},{"location":"examples/wine/","text":"Run the following commands first pip3 install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1 pip3 install autokeras==1.0.5 This Script searches for a model for the wine dataset Source and Description of data: Run the following commands first pip3 install git + https : // github . com / keras - team / keras - tuner . git @ 1.0.2 rc1 pip3 install autokeras == 1.0.5 This Script searches for a model for the wine dataset Source and Description of data : \"\"\" import os import pandas as pd import tensorflow as tf import autokeras as ak dataset_url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\" ) # save data data_file_path = tf.keras.utils.get_file( fname=os.path.basename(dataset_url), origin=dataset_url ) column_names = [ \"Wine\", \"Alcohol\", \"Malic.acid\", \"Ash\", \"Acl\", \"Mg\", \"Phenols\", \"Flavanoids\", \"Nonflavanoid.phenols\", \"Proanth\", \"Color.int\", \"Hue\", \"OD\", \"Proline\", ] feature_names = column_names[1:] label_name = column_names[0] # Wine data = pd.read_csv(data_file_path, header=0, names=column_names) # Shuffling data = data.sample(frac=1) split_length = int(data.shape[0] * 0.8) # 141 # train and test train_data = data.iloc[:split_length] test_data = data.iloc[split_length:] # Initialize the classifier. clf = ak.StructuredDataClassifier(max_trials=5) # Evaluate clf.fit(x=train_data[feature_names], y=train_data[label_name]) print( \"Accuracy: {accuracy} \".format( accuracy=clf.evaluate(x=test_data[feature_names], y=test_data[label_name]) ) )","title":"Wine"},{"location":"extensions/tf_cloud/","text":"TensorFlow Cloud TensorFlow Cloud allows you to run your TensorFlow program leveraging the computing power on Google Cloud easily. Please follow the instructions to setup your account. AutoKeras has successfully integrated with this service. Now you can run your program on Google Cloud only by inserting a few more lines of code. Please see the example below. import argparse import os import autokeras as ak import tensorflow_cloud as tfc from tensorflow.keras.datasets import mnist parser = argparse . ArgumentParser ( description = \"Model save path arguments.\" ) parser . add_argument ( \"--path\" , required = True , type = str , help = \"Keras model save path\" ) args = parser . parse_args () tfc . run ( chief_config = tfc . COMMON_MACHINE_CONFIGS [ \"V100_1X\" ], docker_base_image = \"haifengjin/autokeras:1.0.3\" , ) # Prepare the dataset. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Initialize the ImageClassifier. clf = ak . ImageClassifier ( max_trials = 2 ) # Search for the best model. clf . fit ( x_train , y_train , epochs = 10 ) # Evaluate on the testing data. print ( \"Accuracy: {accuracy} \" . format ( accuracy = clf . evaluate ( x_test , y_test )[ 1 ])) clf . export_model () . save ( os . path . join ( args . path , \"model.h5\" )) You can find the code above here","title":"TensorFlow Cloud"},{"location":"extensions/tf_cloud/#tensorflow-cloud","text":"TensorFlow Cloud allows you to run your TensorFlow program leveraging the computing power on Google Cloud easily. Please follow the instructions to setup your account. AutoKeras has successfully integrated with this service. Now you can run your program on Google Cloud only by inserting a few more lines of code. Please see the example below. import argparse import os import autokeras as ak import tensorflow_cloud as tfc from tensorflow.keras.datasets import mnist parser = argparse . ArgumentParser ( description = \"Model save path arguments.\" ) parser . add_argument ( \"--path\" , required = True , type = str , help = \"Keras model save path\" ) args = parser . parse_args () tfc . run ( chief_config = tfc . COMMON_MACHINE_CONFIGS [ \"V100_1X\" ], docker_base_image = \"haifengjin/autokeras:1.0.3\" , ) # Prepare the dataset. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Initialize the ImageClassifier. clf = ak . ImageClassifier ( max_trials = 2 ) # Search for the best model. clf . fit ( x_train , y_train , epochs = 10 ) # Evaluate on the testing data. print ( \"Accuracy: {accuracy} \" . format ( accuracy = clf . evaluate ( x_test , y_test )[ 1 ])) clf . export_model () . save ( os . path . join ( args . path , \"model.h5\" )) You can find the code above here","title":"TensorFlow Cloud"},{"location":"extensions/trains/","text":"Trains Integration Allegro Trains is a full system open source ML / DL experiment manager and ML-Ops solution. It enables data scientists and data engineers to effortlessly track, manage, compare and collaborate on their experiments as well as easily manage their training workloads on remote machines. Trains is a suite of open source Python packages and plugins, including: Trains Python Client package - Integrate Trains into your AutoKeras tasks with just two lines of code, and get all of Trains robust features. Trains Server - The Trains backend infrastructure and web UI. Use the public Trains demo server , or deploy your own. Trains Agent - The Trains DevOps component for experiment execution, resource control, and autoML.. Additional integrations - Integrate Trains with PyCharm and Jupyter Notebook . Setting up Trains To integrate Trains into your AutoKeras project, do the following: Install the Trains Python Client package. pip install trains Add the short Trains initialization code to your task. from trains import Task task = Task . init ( project_name = \"autokeras\" , task_name = \"autokeras imdb example with scalars\" ) Run your task. The console output will include the URL of the task's RESULTS page. TRAINS Task: overwriting (reusing) task id=60763e04c0ba45ea9fe3cfe79f3f06a3 TRAINS results page: https://demoapp.trains.allegro.ai/projects/21643e0f1c4a4c99953302fc88a1a84c/experiments/60763e04c0ba45ea9fe3cfe79f3f06a3/output/log </code></pre> See an example script here . Tracking your AutoKeras tasks Visualizing Task Results Trains automatically logs comprehensive information about your AutoKeras task: code source control, execution environment, hyperparameters and more. It also automatically records any scalars, histograms and images reported to Tensorboard/Matplotlib or Seaborn. For example, making use of Tensorboard in your task will make all recorded information available in Trains as well: from tensorflow import keras tensorboard_callback_train = keras . callbacks . TensorBoard ( log_dir = 'log' ) tensorboard_callback_test = keras . callbacks . TensorBoard ( log_dir = 'log' ) clf . fit ( x_train , y_train , epochs = 2 , callbacks = [ tensorboard_callback_train ]) clf . fit ( x_test , y_test , epochs = 2 , callbacks = [ tensorboard_callback_test ]) When your task runs, you can follow its results, including any collected metrics through the Trains web UI. View your task results in the Trains web UI, by clicking on it in the EXPERIMENTS table. Find the EXPERIMENT table under the specified project listed in the HOME or PROJECTS page: Detailed description Trains Web UI experiment information can be obtained here . Additional information on Trains logging capabilities can be obtained in the relevant Trains Documentation Task Models Trains automatically tracks models produced by your AutoKeras tasks. To upload models, specify the output_uri parameter when calling Task.init to provide the upload destination: task = Task.init(project_name=\"autokeras\", task_name=\"autokeras imdb example with scalars\", output_uri=\"http://localhost:8081/\") View models information in the experiment details panel, ARTIFACTS tab: Tracking Model Performance Use the Trains web UI to easily create experiment leaderboards and quickly identify best performing models. Customize your board adding any valuable metric or hyperparameter. Additional information on customizing Trains experiment and model tables can be obtained in the relevant Trains Documentation Model Development Insights Use the Trains web UI to view side-by-side comparison of experiments: Easily locate the differences and impact of experiment configuration parameters, metrics, scalars etc. Compare multiple experiments, by selecting two or more experiments in the EXPERIMENTS table, and clicking COMPARE . The following image shows how two experiments compare in their epoch_accuracy and epoch_loss behaviour:","title":"TRAINS"},{"location":"extensions/trains/#trains-integration","text":"Allegro Trains is a full system open source ML / DL experiment manager and ML-Ops solution. It enables data scientists and data engineers to effortlessly track, manage, compare and collaborate on their experiments as well as easily manage their training workloads on remote machines. Trains is a suite of open source Python packages and plugins, including: Trains Python Client package - Integrate Trains into your AutoKeras tasks with just two lines of code, and get all of Trains robust features. Trains Server - The Trains backend infrastructure and web UI. Use the public Trains demo server , or deploy your own. Trains Agent - The Trains DevOps component for experiment execution, resource control, and autoML.. Additional integrations - Integrate Trains with PyCharm and Jupyter Notebook .","title":"Trains Integration"},{"location":"extensions/trains/#setting-up-trains","text":"To integrate Trains into your AutoKeras project, do the following: Install the Trains Python Client package. pip install trains Add the short Trains initialization code to your task. from trains import Task task = Task . init ( project_name = \"autokeras\" , task_name = \"autokeras imdb example with scalars\" ) Run your task. The console output will include the URL of the task's RESULTS page. TRAINS Task: overwriting (reusing) task id=60763e04c0ba45ea9fe3cfe79f3f06a3 TRAINS results page: https://demoapp.trains.allegro.ai/projects/21643e0f1c4a4c99953302fc88a1a84c/experiments/60763e04c0ba45ea9fe3cfe79f3f06a3/output/log </code></pre> See an example script here .","title":"Setting up Trains"},{"location":"extensions/trains/#tracking-your-autokeras-tasks","text":"","title":"Tracking your AutoKeras tasks"},{"location":"extensions/trains/#visualizing-task-results","text":"Trains automatically logs comprehensive information about your AutoKeras task: code source control, execution environment, hyperparameters and more. It also automatically records any scalars, histograms and images reported to Tensorboard/Matplotlib or Seaborn. For example, making use of Tensorboard in your task will make all recorded information available in Trains as well: from tensorflow import keras tensorboard_callback_train = keras . callbacks . TensorBoard ( log_dir = 'log' ) tensorboard_callback_test = keras . callbacks . TensorBoard ( log_dir = 'log' ) clf . fit ( x_train , y_train , epochs = 2 , callbacks = [ tensorboard_callback_train ]) clf . fit ( x_test , y_test , epochs = 2 , callbacks = [ tensorboard_callback_test ]) When your task runs, you can follow its results, including any collected metrics through the Trains web UI. View your task results in the Trains web UI, by clicking on it in the EXPERIMENTS table. Find the EXPERIMENT table under the specified project listed in the HOME or PROJECTS page: Detailed description Trains Web UI experiment information can be obtained here . Additional information on Trains logging capabilities can be obtained in the relevant Trains Documentation","title":"Visualizing Task Results"},{"location":"extensions/trains/#task-models","text":"Trains automatically tracks models produced by your AutoKeras tasks. To upload models, specify the output_uri parameter when calling Task.init to provide the upload destination: task = Task.init(project_name=\"autokeras\", task_name=\"autokeras imdb example with scalars\", output_uri=\"http://localhost:8081/\") View models information in the experiment details panel, ARTIFACTS tab:","title":"Task Models"},{"location":"extensions/trains/#tracking-model-performance","text":"Use the Trains web UI to easily create experiment leaderboards and quickly identify best performing models. Customize your board adding any valuable metric or hyperparameter. Additional information on customizing Trains experiment and model tables can be obtained in the relevant Trains Documentation","title":"Tracking Model Performance"},{"location":"extensions/trains/#model-development-insights","text":"Use the Trains web UI to view side-by-side comparison of experiments: Easily locate the differences and impact of experiment configuration parameters, metrics, scalars etc. Compare multiple experiments, by selecting two or more experiments in the EXPERIMENTS table, and clicking COMPARE . The following image shows how two experiments compare in their epoch_accuracy and epoch_loss behaviour:","title":"Model Development Insights"},{"location":"tutorial/customized/","text":"View in Colab GitHub source ! pip install autokeras import numpy as np import tensorflow as tf from tensorflow.keras.datasets import mnist import autokeras as ak In this tutorial, we show how to customize your search space with AutoModel and how to implement your own block as search space. This API is mainly for advanced users who already know what their model should look like. Customized Search Space First, let us see how we can build the following neural network using the building blocks in AutoKeras. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id6 --> id7(Classification Head) We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API . Since this is just a demo, we use small amount of max_trials and epochs . input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node = ak . ClassificationHead ()( output_node ) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) Whild building the model, the blocks used need to follow this topology: Preprocessor -> Block -> Head . Normalization and ImageAugmentation are Preprocessor s. ClassificationHead is Head . The rest are Block s. In the code above, we use ak.ResNetBlock(version='v2') to specify the version of ResNet to use. There are many other arguments to specify for each building block. For most of the arguments, if not specified, they would be tuned automatically. Please refer to the documentation links at the bottom of the page for more details. Then, we prepare some data to run the model. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Feed the AutoModel with training data. auto_model . fit ( x_train [: 100 ], y_train [: 100 ], epochs = 1 ) # Predict with the best model. predicted_y = auto_model . predict ( x_test ) # Evaluate the best model with testing data. print ( auto_model . evaluate ( x_test , y_test )) For multiple input nodes and multiple heads search space, you can refer to this section . Validation Data If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification , Text Classification , Structured Data Classification , Multi-task and Multiple Validation . Data Format You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification . Implement New Block You can extend the Block class to implement your own building blocks and use it with AutoModel . The first step is to learn how to write a build function for KerasTuner . You need to override the build function of the block. The following example shows how to implement a single Dense layer block whose number of neurons is tunable. class SingleDenseLayerBlock ( ak . Block ): def build ( self , hp , inputs = None ): # Get the input_node from inputs. input_node = tf . nest . flatten ( inputs )[ 0 ] layer = tf . keras . layers . Dense ( hp . Int ( \"num_units\" , min_value = 32 , max_value = 512 , step = 32 ) ) output_node = layer ( input_node ) return output_node You can connect it with other blocks and build it into an AutoModel . # Build the AutoModel input_node = ak . Input () output_node = SingleDenseLayerBlock ()( input_node ) output_node = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( input_node , output_node , overwrite = True , max_trials = 1 ) # Prepare Data num_instances = 100 x_train = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_train = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) x_test = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_test = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Train the model auto_model . fit ( x_train , y_train , epochs = 1 ) print ( auto_model . evaluate ( x_test , y_test )) Reference AutoModel Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , Embedding , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock .","title":"Customized Model"},{"location":"tutorial/customized/#customized-search-space","text":"First, let us see how we can build the following neural network using the building blocks in AutoKeras. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id6 --> id7(Classification Head) We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API . Since this is just a demo, we use small amount of max_trials and epochs . input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node = ak . ClassificationHead ()( output_node ) auto_model = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) Whild building the model, the blocks used need to follow this topology: Preprocessor -> Block -> Head . Normalization and ImageAugmentation are Preprocessor s. ClassificationHead is Head . The rest are Block s. In the code above, we use ak.ResNetBlock(version='v2') to specify the version of ResNet to use. There are many other arguments to specify for each building block. For most of the arguments, if not specified, they would be tuned automatically. Please refer to the documentation links at the bottom of the page for more details. Then, we prepare some data to run the model. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) # Feed the AutoModel with training data. auto_model . fit ( x_train [: 100 ], y_train [: 100 ], epochs = 1 ) # Predict with the best model. predicted_y = auto_model . predict ( x_test ) # Evaluate the best model with testing data. print ( auto_model . evaluate ( x_test , y_test )) For multiple input nodes and multiple heads search space, you can refer to this section .","title":"Customized Search Space"},{"location":"tutorial/customized/#validation-data","text":"If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification , Text Classification , Structured Data Classification , Multi-task and Multiple Validation .","title":"Validation Data"},{"location":"tutorial/customized/#data-format","text":"You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification .","title":"Data Format"},{"location":"tutorial/customized/#implement-new-block","text":"You can extend the Block class to implement your own building blocks and use it with AutoModel . The first step is to learn how to write a build function for KerasTuner . You need to override the build function of the block. The following example shows how to implement a single Dense layer block whose number of neurons is tunable. class SingleDenseLayerBlock ( ak . Block ): def build ( self , hp , inputs = None ): # Get the input_node from inputs. input_node = tf . nest . flatten ( inputs )[ 0 ] layer = tf . keras . layers . Dense ( hp . Int ( \"num_units\" , min_value = 32 , max_value = 512 , step = 32 ) ) output_node = layer ( input_node ) return output_node You can connect it with other blocks and build it into an AutoModel . # Build the AutoModel input_node = ak . Input () output_node = SingleDenseLayerBlock ()( input_node ) output_node = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( input_node , output_node , overwrite = True , max_trials = 1 ) # Prepare Data num_instances = 100 x_train = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_train = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) x_test = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) y_test = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Train the model auto_model . fit ( x_train , y_train , epochs = 1 ) print ( auto_model . evaluate ( x_test , y_test ))","title":"Implement New Block"},{"location":"tutorial/customized/#reference","text":"AutoModel Nodes : ImageInput , Input , StructuredDataInput , TextInput . Preprocessors : FeatureEngineering , ImageAugmentation , LightGBM , Normalization , TextToIntSequence , TextToNgramVector . Blocks : ConvBlock , DenseBlock , Embedding , Merge , ResNetBlock , RNNBlock , SpatialReduction , TemporalReduction , XceptionBlock , ImageBlock , StructuredDataBlock , TextBlock .","title":"Reference"},{"location":"tutorial/export/","text":"View in Colab GitHub source ! pip install autokeras import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import load_model import autokeras as ak You can easily export your model the best model found by AutoKeras as a Keras Model. The following example uses ImageClassifier as an example. All the tasks and the AutoModel has this export_model function. print ( tf . __version__ ) ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Initialize the image classifier. clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) # Try only 1 model.(Increase accordingly) # Feed the image classifier with training data. clf . fit ( x_train , y_train , epochs = 1 ) # Change no of epochs to improve the model # Export as a Keras Model. model = clf . export_model () print ( type ( model )) # <class 'tensorflow.python.keras.engine.training.Model'> try : model . save ( \"model_autokeras\" , save_format = \"tf\" ) except Exception : model . save ( \"model_autokeras.h5\" ) loaded_model = load_model ( \"model_autokeras\" , custom_objects = ak . CUSTOM_OBJECTS ) predicted_y = loaded_model . predict ( tf . expand_dims ( x_test , - 1 )) print ( predicted_y )","title":"Export Model"},{"location":"tutorial/faq/","text":"How to resume a previously killed run? This feature is controlled by the overwrite argument of AutoModel or any other task APIs. It is set to False by default, which means it would not overwrite the contents of the directory. In other words, it will continue the previous fit. You can just run the same code again. It will automatically resume the previously killed run. How to customize metrics and loss? Please see the code example below. import autokeras as ak clf = ak . ImageClassifier ( max_trials = 3 , metrics = [ 'mse' ], loss = 'mse' , ) How to use customized metrics to select the best model? By default, AutoKeras use validation loss as the metric for selecting the best model. Below is a code example of using customized metric for selecting models. Please read the comments for the details. # Implement your customized metric according to the tutorial. # https://keras.io/api/metrics/#creating-custom-metrics import autokeras as ak def f1_score ( y_true , y_pred ): ... clf = ak . ImageClassifier ( max_trials = 3 , # Wrap the function into a Keras Tuner Objective # and pass it to AutoKeras. # Direction can be 'min' or 'max' # meaning we want to minimize or maximize the metric. # 'val_f1_score' is just add a 'val_' prefix # to the function name or the metric name. objective = kerastuner . Objective ( 'val_f1_score' , direction = 'max' ), # Include it as one of the metrics. metrics = [ f1_score ], ) How to use multiple GPUs? You can use the distribution_strategy argument when initializing any model you created with AutoKeras, like AutoModel, ImageClassifier, StructuredDataRegressor and so on. This argument is supported by Keras Tuner. AutoKeras supports the arguments supported by Keras Tuner. Please see the discription of the argument here . import tensorflow as tf import autokeras as ak auto_model = ak . ImageClassifier ( max_trials = 3 , distribution_strategy = tf . distribute . MirroredStrategy (), ) How to constrain the model size? You can use the max_model_size argument for any model in AutoKeras. import autokeras as ak auto_model = ak . ImageClassifier ( max_trials = 3 , max_model_size = 1000000000 , )","title":"FAQ"},{"location":"tutorial/faq/#how-to-resume-a-previously-killed-run","text":"This feature is controlled by the overwrite argument of AutoModel or any other task APIs. It is set to False by default, which means it would not overwrite the contents of the directory. In other words, it will continue the previous fit. You can just run the same code again. It will automatically resume the previously killed run.","title":"How to resume a previously killed run?"},{"location":"tutorial/faq/#how-to-customize-metrics-and-loss","text":"Please see the code example below. import autokeras as ak clf = ak . ImageClassifier ( max_trials = 3 , metrics = [ 'mse' ], loss = 'mse' , )","title":"How to customize metrics and loss?"},{"location":"tutorial/faq/#how-to-use-customized-metrics-to-select-the-best-model","text":"By default, AutoKeras use validation loss as the metric for selecting the best model. Below is a code example of using customized metric for selecting models. Please read the comments for the details. # Implement your customized metric according to the tutorial. # https://keras.io/api/metrics/#creating-custom-metrics import autokeras as ak def f1_score ( y_true , y_pred ): ... clf = ak . ImageClassifier ( max_trials = 3 , # Wrap the function into a Keras Tuner Objective # and pass it to AutoKeras. # Direction can be 'min' or 'max' # meaning we want to minimize or maximize the metric. # 'val_f1_score' is just add a 'val_' prefix # to the function name or the metric name. objective = kerastuner . Objective ( 'val_f1_score' , direction = 'max' ), # Include it as one of the metrics. metrics = [ f1_score ], )","title":"How to use customized metrics to select the best model?"},{"location":"tutorial/faq/#how-to-use-multiple-gpus","text":"You can use the distribution_strategy argument when initializing any model you created with AutoKeras, like AutoModel, ImageClassifier, StructuredDataRegressor and so on. This argument is supported by Keras Tuner. AutoKeras supports the arguments supported by Keras Tuner. Please see the discription of the argument here . import tensorflow as tf import autokeras as ak auto_model = ak . ImageClassifier ( max_trials = 3 , distribution_strategy = tf . distribute . MirroredStrategy (), )","title":"How to use multiple GPUs?"},{"location":"tutorial/faq/#how-to-constrain-the-model-size","text":"You can use the max_model_size argument for any model in AutoKeras. import autokeras as ak auto_model = ak . ImageClassifier ( max_trials = 3 , max_model_size = 1000000000 , )","title":"How to constrain the model size?"},{"location":"tutorial/image_classification/","text":"View in Colab GitHub source ! pip install autokeras import numpy as np import tensorflow as tf from tensorflow.keras.datasets import mnist import autokeras as ak A Simple Example The first step is to prepare your data. Here we use the MNIST dataset as an example ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageClassifier. It is recommended have more trials for more complicated datasets. This is just a quick demo of MNIST, so we set max_trials to 1. For the same reason, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the image classifier. clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) # Feed the image classifier with training data. clf . fit ( x_train , y_train , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( x_test ) print ( predicted_y ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 10 , ) You can also use your own validation set instead of splitting it from the training data with validation_data. split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 10 , ) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = \"resnet\" , # Normalize the dataset. normalize = True , # Do not do data augmentation. augment = False , )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 10 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak. some_block (input_node). You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( horizontal_flip = False )( output_node ) output_node = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 10 ) Data Format The AutoKeras ImageClassifier is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. So if you prepare your data in the following way, the ImageClassifier should still work. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) # One-hot encode the labels. eye = np . eye ( 10 ) y_train = eye [ y_train ] y_test = eye [ y_test ] print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) print ( y_train [: 3 ]) # array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]) We also support using tf.data.Dataset format for the training data. train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) Reference ImageClassifier , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , ClassificationHead .","title":"Image Classification"},{"location":"tutorial/image_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the MNIST dataset as an example ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageClassifier. It is recommended have more trials for more complicated datasets. This is just a quick demo of MNIST, so we set max_trials to 1. For the same reason, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the image classifier. clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) # Feed the image classifier with training data. clf . fit ( x_train , y_train , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( x_test ) print ( predicted_y ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/image_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 10 , ) You can also use your own validation set instead of splitting it from the training data with validation_data. split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 10 , )","title":"Validation Data"},{"location":"tutorial/image_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = \"resnet\" , # Normalize the dataset. normalize = True , # Do not do data augmentation. augment = False , )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 10 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak. some_block (input_node). You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( horizontal_flip = False )( output_node ) output_node = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 10 )","title":"Customized Search Space"},{"location":"tutorial/image_classification/#data-format","text":"The AutoKeras ImageClassifier is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. So if you prepare your data in the following way, the ImageClassifier should still work. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) # One-hot encode the labels. eye = np . eye ( 10 ) y_train = eye [ y_train ] y_test = eye [ y_test ] print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) print ( y_train [: 3 ]) # array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], # [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]) We also support using tf.data.Dataset format for the training data. train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/image_classification/#reference","text":"ImageClassifier , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/image_regression/","text":"View in Colab GitHub source ! pip install autokeras import tensorflow as tf from tensorflow.keras.datasets import mnist import autokeras as ak To make this tutorial easy to follow, we just treat MNIST dataset as a regression dataset. It means we will treat prediction targets of MNIST dataset, which are integers ranging from 0 to 9 as numerical values, so that they can be directly used as the regression targets. A Simple Example The first step is to prepare your data. Here we use the MNIST dataset as an example ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train = x_train [: 100 ] y_train = y_train [: 100 ] print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageRegressor. It is recommended have more trials for more complicated datasets. This is just a quick demo of MNIST, so we set max_trials to 1. For the same reason, we set epochs to 2. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the image regressor. reg = ak . ImageRegressor ( overwrite = True , max_trials = 1 ) # Feed the image regressor with training data. reg . fit ( x_train , y_train , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( x_test ) print ( predicted_y ) # Evaluate the best model with testing data. print ( reg . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. reg . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 2 , ) You can also use your own validation set instead of splitting it from the training data with validation_data. split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] reg . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 2 , ) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of ImageRegressor. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = \"resnet\" , # Normalize the dataset. normalize = False , # Do not do data augmentation. augment = False , )( input_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak. some_block (input_node). You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( horizontal_flip = False )( output_node ) output_node = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 ) Data Format The AutoKeras ImageRegressor is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray. We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) y_train = y_train . reshape ( y_train . shape + ( 1 ,)) y_test = y_test . reshape ( y_test . shape + ( 1 ,)) print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) reg = ak . ImageRegressor ( overwrite = True , max_trials = 1 ) # Feed the tensorflow Dataset to the regressor. reg . fit ( train_set , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( test_set ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_set )) Reference ImageRegressor , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , RegressionHead .","title":"Image Regression"},{"location":"tutorial/image_regression/#a-simple-example","text":"The first step is to prepare your data. Here we use the MNIST dataset as an example ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train = x_train [: 100 ] y_train = y_train [: 100 ] print ( x_train . shape ) # (60000, 28, 28) print ( y_train . shape ) # (60000,) print ( y_train [: 3 ]) # array([7, 2, 1], dtype=uint8) The second step is to run the ImageRegressor. It is recommended have more trials for more complicated datasets. This is just a quick demo of MNIST, so we set max_trials to 1. For the same reason, we set epochs to 2. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the image regressor. reg = ak . ImageRegressor ( overwrite = True , max_trials = 1 ) # Feed the image regressor with training data. reg . fit ( x_train , y_train , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( x_test ) print ( predicted_y ) # Evaluate the best model with testing data. print ( reg . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/image_regression/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. reg . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 2 , ) You can also use your own validation set instead of splitting it from the training data with validation_data. split = 50000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] reg . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 2 , )","title":"Validation Data"},{"location":"tutorial/image_regression/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of ImageRegressor. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . ImageInput () output_node = ak . ImageBlock ( # Only search ResNet architectures. block_type = \"resnet\" , # Normalize the dataset. normalize = False , # Do not do data augmentation. augment = False , )( input_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak. some_block (input_node). You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . ImageInput () output_node = ak . Normalization ()( input_node ) output_node = ak . ImageAugmentation ( horizontal_flip = False )( output_node ) output_node = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 )","title":"Customized Search Space"},{"location":"tutorial/image_regression/#data-format","text":"The AutoKeras ImageRegressor is quite flexible for the data format. For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1). For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray. We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional. ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () # Reshape the images to have the channel dimension. x_train = x_train . reshape ( x_train . shape + ( 1 ,)) x_test = x_test . reshape ( x_test . shape + ( 1 ,)) y_train = y_train . reshape ( y_train . shape + ( 1 ,)) y_test = y_test . reshape ( y_test . shape + ( 1 ,)) print ( x_train . shape ) # (60000, 28, 28, 1) print ( y_train . shape ) # (60000, 10) train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) reg = ak . ImageRegressor ( overwrite = True , max_trials = 1 ) # Feed the tensorflow Dataset to the regressor. reg . fit ( train_set , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( test_set ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/image_regression/#reference","text":"ImageRegressor , AutoModel , ImageBlock , Normalization , ImageAugmentation , ResNetBlock , ImageInput , RegressionHead .","title":"Reference"},{"location":"tutorial/load/","text":"View in Colab GitHub source ! pip install autokeras import os import shutil import numpy as np import tensorflow as tf import autokeras as ak Load Images from Disk If the data is too large to put in memory all at once, we can load it batch by batch into memory from disk with tf.data.Dataset. This function can help you build such a tf.data.Dataset for image data. First, we download the data and extract the files. dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\" # noqa: E501 local_file_path = tf . keras . utils . get_file ( origin = dataset_url , fname = \"image_data\" , extract = True ) # The file is extracted in the same directory as the downloaded file. local_dir_path = os . path . dirname ( local_file_path ) # After check mannually, we know the extracted data is in 'flower_photos'. data_dir = os . path . join ( local_dir_path , \"flower_photos\" ) print ( data_dir ) The directory should look like this. Each folder contains the images in the same class. flowers_photos/ daisy/ dandelion/ roses/ sunflowers/ tulips/ We can split the data into training and testing as we load them. batch_size = 32 img_height = 180 img_width = 180 train_data = ak . image_dataset_from_directory ( data_dir , # Use 20% data as testing data. validation_split = 0.2 , subset = \"training\" , # Set seed to ensure the same split when loading testing data. seed = 123 , image_size = ( img_height , img_width ), batch_size = batch_size , ) test_data = ak . image_dataset_from_directory ( data_dir , validation_split = 0.2 , subset = \"validation\" , seed = 123 , image_size = ( img_height , img_width ), batch_size = batch_size , ) Then we just do one quick demo of AutoKeras to make sure the dataset works. clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) clf . fit ( train_data , epochs = 1 ) print ( clf . evaluate ( test_data )) Load Texts from Disk You can also load text datasets in the same way. dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" local_file_path = tf . keras . utils . get_file ( fname = \"text_data\" , origin = dataset_url , extract = True , ) # The file is extracted in the same directory as the downloaded file. local_dir_path = os . path . dirname ( local_file_path ) # After check mannually, we know the extracted data is in 'aclImdb'. data_dir = os . path . join ( local_dir_path , \"aclImdb\" ) # Remove the unused data folder. shutil . rmtree ( os . path . join ( data_dir , \"train/unsup\" )) For this dataset, the data is already split into train and test. We just load them separately. print ( data_dir ) train_data = ak . text_dataset_from_directory ( os . path . join ( data_dir , \"train\" ), batch_size = batch_size ) test_data = ak . text_dataset_from_directory ( os . path . join ( data_dir , \"test\" ), shuffle = False , batch_size = batch_size ) clf = ak . TextClassifier ( overwrite = True , max_trials = 1 ) clf . fit ( train_data , epochs = 2 ) print ( clf . evaluate ( test_data )) Load Data with Python Generators If you want to use generators, you can refer to the following code. N_BATCHES = 30 BATCH_SIZE = 100 N_FEATURES = 10 def get_data_generator ( n_batches , batch_size , n_features ): \"\"\"Get a generator returning n_batches random data. The shape of the data is (batch_size, n_features). \"\"\" def data_generator (): for _ in range ( n_batches * batch_size ): x = np . random . randn ( n_features ) y = x . sum ( axis = 0 ) / n_features > 0.5 yield x , y return data_generator dataset = tf . data . Dataset . from_generator ( get_data_generator ( N_BATCHES , BATCH_SIZE , N_FEATURES ), output_types = ( tf . float32 , tf . float32 ), output_shapes = (( N_FEATURES ,), tuple ()), ) . batch ( BATCH_SIZE ) clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 1 , seed = 5 ) clf . fit ( x = dataset , validation_data = dataset , batch_size = BATCH_SIZE ) print ( clf . evaluate ( dataset )) Reference image_dataset_from_directory text_dataset_from_directory","title":"Load Data from Disk"},{"location":"tutorial/load/#load-images-from-disk","text":"If the data is too large to put in memory all at once, we can load it batch by batch into memory from disk with tf.data.Dataset. This function can help you build such a tf.data.Dataset for image data. First, we download the data and extract the files. dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\" # noqa: E501 local_file_path = tf . keras . utils . get_file ( origin = dataset_url , fname = \"image_data\" , extract = True ) # The file is extracted in the same directory as the downloaded file. local_dir_path = os . path . dirname ( local_file_path ) # After check mannually, we know the extracted data is in 'flower_photos'. data_dir = os . path . join ( local_dir_path , \"flower_photos\" ) print ( data_dir ) The directory should look like this. Each folder contains the images in the same class. flowers_photos/ daisy/ dandelion/ roses/ sunflowers/ tulips/ We can split the data into training and testing as we load them. batch_size = 32 img_height = 180 img_width = 180 train_data = ak . image_dataset_from_directory ( data_dir , # Use 20% data as testing data. validation_split = 0.2 , subset = \"training\" , # Set seed to ensure the same split when loading testing data. seed = 123 , image_size = ( img_height , img_width ), batch_size = batch_size , ) test_data = ak . image_dataset_from_directory ( data_dir , validation_split = 0.2 , subset = \"validation\" , seed = 123 , image_size = ( img_height , img_width ), batch_size = batch_size , ) Then we just do one quick demo of AutoKeras to make sure the dataset works. clf = ak . ImageClassifier ( overwrite = True , max_trials = 1 ) clf . fit ( train_data , epochs = 1 ) print ( clf . evaluate ( test_data ))","title":"Load Images from Disk"},{"location":"tutorial/load/#load-texts-from-disk","text":"You can also load text datasets in the same way. dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" local_file_path = tf . keras . utils . get_file ( fname = \"text_data\" , origin = dataset_url , extract = True , ) # The file is extracted in the same directory as the downloaded file. local_dir_path = os . path . dirname ( local_file_path ) # After check mannually, we know the extracted data is in 'aclImdb'. data_dir = os . path . join ( local_dir_path , \"aclImdb\" ) # Remove the unused data folder. shutil . rmtree ( os . path . join ( data_dir , \"train/unsup\" )) For this dataset, the data is already split into train and test. We just load them separately. print ( data_dir ) train_data = ak . text_dataset_from_directory ( os . path . join ( data_dir , \"train\" ), batch_size = batch_size ) test_data = ak . text_dataset_from_directory ( os . path . join ( data_dir , \"test\" ), shuffle = False , batch_size = batch_size ) clf = ak . TextClassifier ( overwrite = True , max_trials = 1 ) clf . fit ( train_data , epochs = 2 ) print ( clf . evaluate ( test_data ))","title":"Load Texts from Disk"},{"location":"tutorial/load/#load-data-with-python-generators","text":"If you want to use generators, you can refer to the following code. N_BATCHES = 30 BATCH_SIZE = 100 N_FEATURES = 10 def get_data_generator ( n_batches , batch_size , n_features ): \"\"\"Get a generator returning n_batches random data. The shape of the data is (batch_size, n_features). \"\"\" def data_generator (): for _ in range ( n_batches * batch_size ): x = np . random . randn ( n_features ) y = x . sum ( axis = 0 ) / n_features > 0.5 yield x , y return data_generator dataset = tf . data . Dataset . from_generator ( get_data_generator ( N_BATCHES , BATCH_SIZE , N_FEATURES ), output_types = ( tf . float32 , tf . float32 ), output_shapes = (( N_FEATURES ,), tuple ()), ) . batch ( BATCH_SIZE ) clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 1 , seed = 5 ) clf . fit ( x = dataset , validation_data = dataset , batch_size = BATCH_SIZE ) print ( clf . evaluate ( dataset ))","title":"Load Data with Python Generators"},{"location":"tutorial/load/#reference","text":"image_dataset_from_directory text_dataset_from_directory","title":"Reference"},{"location":"tutorial/multi/","text":"View in Colab GitHub source ! pip install autokeras import numpy as np import autokeras as ak In this tutorial we are making use of the AutoModel API to show how to handle multi-modal data and multi-task. What is multi-modal? Multi-modal data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data. What is multi-task? Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1. The following diagram shows an example of multi-modal and multi-task neural network model. graph TD id1(ImageInput) --> id3(Some Neural Network Model) id2(StructuredDataInput) --> id3 id3 --> id4(ClassificationHead) id3 --> id5(RegressionHead) It has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time. Data Preparation To illustrate our idea, we generate some random image and structured data as the multi-modal data. num_instances = 100 # Generate image data. image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) # Generate structured data. structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) We also generate some multi-task targets for classification and regression. # Generate regression targets. regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Generate classification labels of five classes. classification_target = np . random . randint ( 5 , size = num_instances ) Build and Train the Model Then we initialize the multi-modal and multi-task model with AutoModel . Since this is just a demo, we use small amount of max_trials and epochs . # Initialize the multi with multiple inputs and outputs. model = ak . AutoModel ( inputs = [ ak . ImageInput (), ak . StructuredDataInput ()], outputs = [ ak . RegressionHead ( metrics = [ \"mae\" ]), ak . ClassificationHead ( loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ]), ], overwrite = True , max_trials = 2 , ) # Fit the model with prepared data. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], epochs = 3 , ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 2 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 20 image_val = image_data [ split :] structured_val = structured_data [ split :] regression_val = regression_target [ split :] classification_val = classification_target [ split :] image_data = image_data [: split ] structured_data = structured_data [: split ] regression_target = regression_target [: split ] classification_target = classification_target [: split ] model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Use your own validation set. validation_data = ( [ image_val , structured_val ], [ regression_val , classification_val ], ), epochs = 2 , ) Customized Search Space You can customize your search space. The following figure shows the search space we want to define. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id7(StructuredDataInput) --> id8(CategoricalToNumerical) id8 --> id9(DenseBlock) id6 --> id10(Merge) id9 --> id10 id10 --> id11(Classification Head) id10 --> id12(Regression Head) input_node1 = ak . ImageInput () output_node = ak . Normalization ()( input_node1 ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node1 = ak . Merge ()([ output_node1 , output_node2 ]) input_node2 = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node2 ) output_node2 = ak . DenseBlock ()( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node1 = ak . ClassificationHead ()( output_node ) output_node2 = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( inputs = [ input_node1 , input_node2 ], outputs = [ output_node1 , output_node2 ], overwrite = True , max_trials = 2 , ) image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) classification_target = np . random . randint ( 5 , size = num_instances ) auto_model . fit ( [ image_data , structured_data ], [ classification_target , regression_target ], batch_size = 32 , epochs = 3 , ) Data Format You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification . Reference AutoModel , ImageInput , StructuredDataInput , DenseBlock , RegressionHead , ClassificationHead , CategoricalToNumerical .","title":"Multi-Modal and Multi-Task"},{"location":"tutorial/multi/#what-is-multi-modal","text":"Multi-modal data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as structured data.","title":"What is multi-modal?"},{"location":"tutorial/multi/#what-is-multi-task","text":"Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1. The following diagram shows an example of multi-modal and multi-task neural network model. graph TD id1(ImageInput) --> id3(Some Neural Network Model) id2(StructuredDataInput) --> id3 id3 --> id4(ClassificationHead) id3 --> id5(RegressionHead) It has two inputs the images and the structured data. Each image is associated with a set of attributes in the structured data. From these data, we are trying to predict the classification label and the regression value at the same time.","title":"What is multi-task?"},{"location":"tutorial/multi/#data-preparation","text":"To illustrate our idea, we generate some random image and structured data as the multi-modal data. num_instances = 100 # Generate image data. image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) # Generate structured data. structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) We also generate some multi-task targets for classification and regression. # Generate regression targets. regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) # Generate classification labels of five classes. classification_target = np . random . randint ( 5 , size = num_instances )","title":"Data Preparation"},{"location":"tutorial/multi/#build-and-train-the-model","text":"Then we initialize the multi-modal and multi-task model with AutoModel . Since this is just a demo, we use small amount of max_trials and epochs . # Initialize the multi with multiple inputs and outputs. model = ak . AutoModel ( inputs = [ ak . ImageInput (), ak . StructuredDataInput ()], outputs = [ ak . RegressionHead ( metrics = [ \"mae\" ]), ak . ClassificationHead ( loss = \"categorical_crossentropy\" , metrics = [ \"accuracy\" ]), ], overwrite = True , max_trials = 2 , ) # Fit the model with prepared data. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], epochs = 3 , )","title":"Build and Train the Model"},{"location":"tutorial/multi/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 2 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 20 image_val = image_data [ split :] structured_val = structured_data [ split :] regression_val = regression_target [ split :] classification_val = classification_target [ split :] image_data = image_data [: split ] structured_data = structured_data [: split ] regression_target = regression_target [: split ] classification_target = classification_target [: split ] model . fit ( [ image_data , structured_data ], [ regression_target , classification_target ], # Use your own validation set. validation_data = ( [ image_val , structured_val ], [ regression_val , classification_val ], ), epochs = 2 , )","title":"Validation Data"},{"location":"tutorial/multi/#customized-search-space","text":"You can customize your search space. The following figure shows the search space we want to define. graph LR id1(ImageInput) --> id2(Normalization) id2 --> id3(Image Augmentation) id3 --> id4(Convolutional) id3 --> id5(ResNet V2) id4 --> id6(Merge) id5 --> id6 id7(StructuredDataInput) --> id8(CategoricalToNumerical) id8 --> id9(DenseBlock) id6 --> id10(Merge) id9 --> id10 id10 --> id11(Classification Head) id10 --> id12(Regression Head) input_node1 = ak . ImageInput () output_node = ak . Normalization ()( input_node1 ) output_node = ak . ImageAugmentation ()( output_node ) output_node1 = ak . ConvBlock ()( output_node ) output_node2 = ak . ResNetBlock ( version = \"v2\" )( output_node ) output_node1 = ak . Merge ()([ output_node1 , output_node2 ]) input_node2 = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node2 ) output_node2 = ak . DenseBlock ()( output_node ) output_node = ak . Merge ()([ output_node1 , output_node2 ]) output_node1 = ak . ClassificationHead ()( output_node ) output_node2 = ak . RegressionHead ()( output_node ) auto_model = ak . AutoModel ( inputs = [ input_node1 , input_node2 ], outputs = [ output_node1 , output_node2 ], overwrite = True , max_trials = 2 , ) image_data = np . random . rand ( num_instances , 32 , 32 , 3 ) . astype ( np . float32 ) structured_data = np . random . rand ( num_instances , 20 ) . astype ( np . float32 ) regression_target = np . random . rand ( num_instances , 1 ) . astype ( np . float32 ) classification_target = np . random . randint ( 5 , size = num_instances ) auto_model . fit ( [ image_data , structured_data ], [ classification_target , regression_target ], batch_size = 32 , epochs = 3 , )","title":"Customized Search Space"},{"location":"tutorial/multi/#data-format","text":"You can refer to the documentation of ImageInput , StructuredDataInput , TextInput , RegressionHead , ClassificationHead , for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification , Text Classification , Structured Data Classification .","title":"Data Format"},{"location":"tutorial/multi/#reference","text":"AutoModel , ImageInput , StructuredDataInput , DenseBlock , RegressionHead , ClassificationHead , CategoricalToNumerical .","title":"Reference"},{"location":"tutorial/overview/","text":"AutoKeras 1.0 Tutorial Supported Tasks AutoKeras supports several tasks with an extremely simple interface. You can click the links below to see the detailed tutorial for each task. Supported Tasks : Image Classification Image Regression Text Classification Text Regression Structured Data Classification Structured Data Regression Coming Soon : Time Series Forecasting, Object Detection, Image Segmentation. Multi-Task and Multi-Modal Data If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details. Customized Model Follow this tutorial , to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras. Nodes : ImageInput Input StructuredDataInput TextInput Blocks : ImageAugmentation Normalization TextToIntSequence TextToNgramVector CategoricalToNumerical ConvBlock DenseBlock Embedding Merge ResNetBlock RNNBlock SpatialReduction TemporalReduction XceptionBlock ImageBlock StructuredDataBlock TextBlock ClassificationHead RegressionHead Export Model You can follow this tutorial to export the best model.","title":"Overview"},{"location":"tutorial/overview/#autokeras-10-tutorial","text":"","title":"AutoKeras 1.0 Tutorial"},{"location":"tutorial/overview/#supported-tasks","text":"AutoKeras supports several tasks with an extremely simple interface. You can click the links below to see the detailed tutorial for each task. Supported Tasks : Image Classification Image Regression Text Classification Text Regression Structured Data Classification Structured Data Regression Coming Soon : Time Series Forecasting, Object Detection, Image Segmentation.","title":"Supported Tasks"},{"location":"tutorial/overview/#multi-task-and-multi-modal-data","text":"If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details.","title":"Multi-Task and Multi-Modal Data"},{"location":"tutorial/overview/#customized-model","text":"Follow this tutorial , to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras. Nodes : ImageInput Input StructuredDataInput TextInput Blocks : ImageAugmentation Normalization TextToIntSequence TextToNgramVector CategoricalToNumerical ConvBlock DenseBlock Embedding Merge ResNetBlock RNNBlock SpatialReduction TemporalReduction XceptionBlock ImageBlock StructuredDataBlock TextBlock ClassificationHead RegressionHead","title":"Customized Model"},{"location":"tutorial/overview/#export-model","text":"You can follow this tutorial to export the best model.","title":"Export Model"},{"location":"tutorial/structured_data_classification/","text":"View in Colab GitHub source ! pip install autokeras import pandas as pd import tensorflow as tf import autokeras as ak A Simple Example The first step is to prepare your data. Here we use the Titanic dataset as an example. TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\" TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\" train_file_path = tf . keras . utils . get_file ( \"train.csv\" , TRAIN_DATA_URL ) test_file_path = tf . keras . utils . get_file ( \"eval.csv\" , TEST_DATA_URL ) The second step is to run the StructuredDataClassifier . As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 3 ) # It tries 3 different models. # Feed the structured data classifier with training data. clf . fit ( # The path to the train.csv file. train_file_path , # The name of the label column. \"survived\" , epochs = 10 , ) # Predict with the best model. predicted_y = clf . predict ( test_file_path ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_file_path , \"survived\" )) Data Format The AutoKeras StructuredDataClassifier is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( train_file_path ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( \"survived\" ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( test_file_path ) y_test = x_test . pop ( \"survived\" ) # It tries 10 different models. clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 3 ) # Feed the structured data classifier with training data. clf . fit ( x_train , y_train , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. train_set = tf . data . Dataset . from_tensor_slices (( x_train . astype ( str ), y_train )) test_set = tf . data . Dataset . from_tensor_slices ( ( x_test . to_numpy () . astype ( str ), y_test ) ) clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 3 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( column_names = [ \"sex\" , \"age\" , \"n_siblings_spouses\" , \"parch\" , \"fare\" , \"class\" , \"deck\" , \"embark_town\" , \"alone\" , ], column_types = { \"sex\" : \"categorical\" , \"fare\" : \"numerical\" }, max_trials = 10 , # It tries 10 different models. overwrite = True , ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 10 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 10 , ) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier . You can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( categorical_encoding = True )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 3 ) clf . fit ( x_train , y_train , epochs = 10 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 1 ) clf . predict ( x_train ) You can also export the best model found by AutoKeras as a Keras Model. model = clf . export_model () model . summary () print ( x_train . dtype ) # numpy array in object (mixed type) is not supported. # convert it to unicode. model . predict ( x_train . astype ( str )) Reference StructuredDataClassifier , AutoModel , StructuredDataBlock , DenseBlock , StructuredDataInput , ClassificationHead , CategoricalToNumerical .","title":"Structured Data Classification"},{"location":"tutorial/structured_data_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the Titanic dataset as an example. TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\" TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\" train_file_path = tf . keras . utils . get_file ( \"train.csv\" , TRAIN_DATA_URL ) test_file_path = tf . keras . utils . get_file ( \"eval.csv\" , TEST_DATA_URL ) The second step is to run the StructuredDataClassifier . As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 3 ) # It tries 3 different models. # Feed the structured data classifier with training data. clf . fit ( # The path to the train.csv file. train_file_path , # The name of the label column. \"survived\" , epochs = 10 , ) # Predict with the best model. predicted_y = clf . predict ( test_file_path ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_file_path , \"survived\" ))","title":"A Simple Example"},{"location":"tutorial/structured_data_classification/#data-format","text":"The AutoKeras StructuredDataClassifier is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. The labels can be numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( train_file_path ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( \"survived\" ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( test_file_path ) y_test = x_test . pop ( \"survived\" ) # It tries 10 different models. clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 3 ) # Feed the structured data classifier with training data. clf . fit ( x_train , y_train , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. train_set = tf . data . Dataset . from_tensor_slices (( x_train . astype ( str ), y_train )) test_set = tf . data . Dataset . from_tensor_slices ( ( x_test . to_numpy () . astype ( str ), y_test ) ) clf = ak . StructuredDataClassifier ( overwrite = True , max_trials = 3 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set , epochs = 10 ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data classifier. clf = ak . StructuredDataClassifier ( column_names = [ \"sex\" , \"age\" , \"n_siblings_spouses\" , \"parch\" , \"fare\" , \"class\" , \"deck\" , \"embark_town\" , \"alone\" , ], column_types = { \"sex\" : \"categorical\" , \"fare\" : \"numerical\" }, max_trials = 10 , # It tries 10 different models. overwrite = True , )","title":"Data Format"},{"location":"tutorial/structured_data_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 10 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 10 , )","title":"Validation Data"},{"location":"tutorial/structured_data_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of StructuredDataClassifier . You can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( categorical_encoding = True )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 3 ) clf . fit ( x_train , y_train , epochs = 10 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 1 ) clf . predict ( x_train ) You can also export the best model found by AutoKeras as a Keras Model. model = clf . export_model () model . summary () print ( x_train . dtype ) # numpy array in object (mixed type) is not supported. # convert it to unicode. model . predict ( x_train . astype ( str ))","title":"Customized Search Space"},{"location":"tutorial/structured_data_classification/#reference","text":"StructuredDataClassifier , AutoModel , StructuredDataBlock , DenseBlock , StructuredDataInput , ClassificationHead , CategoricalToNumerical .","title":"Reference"},{"location":"tutorial/structured_data_regression/","text":"View in Colab GitHub source ! pip install autokeras import numpy as np import pandas as pd import tensorflow as tf from sklearn.datasets import fetch_california_housing import autokeras as ak A Simple Example The first step is to prepare your data. Here we use the California housing dataset as an example. house_dataset = fetch_california_housing () df = pd . DataFrame ( np . concatenate ( ( house_dataset . data , house_dataset . target . reshape ( - 1 , 1 )), axis = 1 ), columns = house_dataset . feature_names + [ \"Price\" ], ) train_size = int ( df . shape [ 0 ] * 0.9 ) df [: train_size ] . to_csv ( \"train.csv\" , index = False ) df [ train_size :] . to_csv ( \"eval.csv\" , index = False ) train_file_path = \"train.csv\" test_file_path = \"eval.csv\" The second step is to run the StructuredDataRegressor . As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the structured data regressor. reg = ak . StructuredDataRegressor ( overwrite = True , max_trials = 3 ) # It tries 3 different models. # Feed the structured data regressor with training data. reg . fit ( # The path to the train.csv file. train_file_path , # The name of the label column. \"Price\" , epochs = 10 , ) # Predict with the best model. predicted_y = reg . predict ( test_file_path ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_file_path , \"Price\" )) Data Format The AutoKeras StructuredDataRegressor is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( train_file_path ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( \"Price\" ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( test_file_path ) y_test = x_test . pop ( \"Price\" ) # It tries 10 different models. reg = ak . StructuredDataRegressor ( max_trials = 3 , overwrite = True ) # Feed the structured data regressor with training data. reg . fit ( x_train , y_train , epochs = 10 ) # Predict with the best model. predicted_y = reg . predict ( x_test ) # Evaluate the best model with testing data. print ( reg . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. train_set = tf . data . Dataset . from_tensor_slices (( x_train , y_train )) test_set = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) reg = ak . StructuredDataRegressor ( max_trials = 3 , overwrite = True ) # Feed the tensorflow Dataset to the regressor. reg . fit ( train_set , epochs = 10 ) # Predict with the best model. predicted_y = reg . predict ( test_set ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data regressor. reg = ak . StructuredDataRegressor ( column_names = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" , \"AveBedrms\" , \"Population\" , \"AveOccup\" , \"Latitude\" , \"Longitude\" , ], column_types = { \"MedInc\" : \"numerical\" , \"Latitude\" : \"numerical\" }, max_trials = 10 , # It tries 10 different models. overwrite = True , ) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. reg . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 10 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] reg . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 10 , ) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of StructuredDataRegressor . You can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( categorical_encoding = True )( input_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 3 ) reg . fit ( x_train , y_train , epochs = 10 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 3 , overwrite = True ) reg . fit ( x_train , y_train , epochs = 10 ) You can also export the best model found by AutoKeras as a Keras Model. model = reg . export_model () model . summary () # numpy array in object (mixed type) is not supported. # you need convert it to unicode or float first. model . predict ( x_train ) Reference StructuredDataRegressor , AutoModel , StructuredDataBlock , DenseBlock , StructuredDataInput , RegressionHead , CategoricalToNumerical .","title":"Structured Data Regression"},{"location":"tutorial/structured_data_regression/#a-simple-example","text":"The first step is to prepare your data. Here we use the California housing dataset as an example. house_dataset = fetch_california_housing () df = pd . DataFrame ( np . concatenate ( ( house_dataset . data , house_dataset . target . reshape ( - 1 , 1 )), axis = 1 ), columns = house_dataset . feature_names + [ \"Price\" ], ) train_size = int ( df . shape [ 0 ] * 0.9 ) df [: train_size ] . to_csv ( \"train.csv\" , index = False ) df [ train_size :] . to_csv ( \"eval.csv\" , index = False ) train_file_path = \"train.csv\" test_file_path = \"eval.csv\" The second step is to run the StructuredDataRegressor . As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the structured data regressor. reg = ak . StructuredDataRegressor ( overwrite = True , max_trials = 3 ) # It tries 3 different models. # Feed the structured data regressor with training data. reg . fit ( # The path to the train.csv file. train_file_path , # The name of the label column. \"Price\" , epochs = 10 , ) # Predict with the best model. predicted_y = reg . predict ( test_file_path ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_file_path , \"Price\" ))","title":"A Simple Example"},{"location":"tutorial/structured_data_regression/#data-format","text":"The AutoKeras StructuredDataRegressor is quite flexible for the data format. The example above shows how to use the CSV files directly. Besides CSV files, it also supports numpy.ndarray, pandas.DataFrame or tf.data.Dataset . The data should be two-dimensional with numerical or categorical values. For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray, pandas.DataFrame, or pandas.Series. The following examples show how the data can be prepared with numpy.ndarray, pandas.DataFrame, and tensorflow.data.Dataset. # x_train as pandas.DataFrame, y_train as pandas.Series x_train = pd . read_csv ( train_file_path ) print ( type ( x_train )) # pandas.DataFrame y_train = x_train . pop ( \"Price\" ) print ( type ( y_train )) # pandas.Series # You can also use pandas.DataFrame for y_train. y_train = pd . DataFrame ( y_train ) print ( type ( y_train )) # pandas.DataFrame # You can also use numpy.ndarray for x_train and y_train. x_train = x_train . to_numpy () y_train = y_train . to_numpy () print ( type ( x_train )) # numpy.ndarray print ( type ( y_train )) # numpy.ndarray # Preparing testing data. x_test = pd . read_csv ( test_file_path ) y_test = x_test . pop ( \"Price\" ) # It tries 10 different models. reg = ak . StructuredDataRegressor ( max_trials = 3 , overwrite = True ) # Feed the structured data regressor with training data. reg . fit ( x_train , y_train , epochs = 10 ) # Predict with the best model. predicted_y = reg . predict ( x_test ) # Evaluate the best model with testing data. print ( reg . evaluate ( x_test , y_test )) The following code shows how to convert numpy.ndarray to tf.data.Dataset. train_set = tf . data . Dataset . from_tensor_slices (( x_train , y_train )) test_set = tf . data . Dataset . from_tensor_slices (( x_test , y_test )) reg = ak . StructuredDataRegressor ( max_trials = 3 , overwrite = True ) # Feed the tensorflow Dataset to the regressor. reg . fit ( train_set , epochs = 10 ) # Predict with the best model. predicted_y = reg . predict ( test_set ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_set )) You can also specify the column names and types for the data as follows. The column_names is optional if the training data already have the column names, e.g. pandas.DataFrame, CSV file. Any column, whose type is not specified will be inferred from the training data. # Initialize the structured data regressor. reg = ak . StructuredDataRegressor ( column_names = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" , \"AveBedrms\" , \"Population\" , \"AveOccup\" , \"Latitude\" , \"Longitude\" , ], column_types = { \"MedInc\" : \"numerical\" , \"Latitude\" : \"numerical\" }, max_trials = 10 , # It tries 10 different models. overwrite = True , )","title":"Data Format"},{"location":"tutorial/structured_data_regression/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. reg . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , epochs = 10 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 500 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] reg . fit ( x_train , y_train , # Use your own validation set. validation_data = ( x_val , y_val ), epochs = 10 , )","title":"Validation Data"},{"location":"tutorial/structured_data_regression/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of StructuredDataRegressor . You can configure the StructuredDataBlock for some high-level configurations, e.g., categorical_encoding for whether to use the CategoricalToNumerical . You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . StructuredDataInput () output_node = ak . StructuredDataBlock ( categorical_encoding = True )( input_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 3 ) reg . fit ( x_train , y_train , epochs = 10 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . StructuredDataInput () output_node = ak . CategoricalToNumerical ()( input_node ) output_node = ak . DenseBlock ()( output_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , max_trials = 3 , overwrite = True ) reg . fit ( x_train , y_train , epochs = 10 ) You can also export the best model found by AutoKeras as a Keras Model. model = reg . export_model () model . summary () # numpy array in object (mixed type) is not supported. # you need convert it to unicode or float first. model . predict ( x_train )","title":"Customized Search Space"},{"location":"tutorial/structured_data_regression/#reference","text":"StructuredDataRegressor , AutoModel , StructuredDataBlock , DenseBlock , StructuredDataInput , RegressionHead , CategoricalToNumerical .","title":"Reference"},{"location":"tutorial/text_classification/","text":"View in Colab GitHub source ! pip install autokeras import os import numpy as np import tensorflow as tf from sklearn.datasets import load_files import autokeras as ak A Simple Example The first step is to prepare your data. Here we use the IMDB dataset as an example. dataset = tf . keras . utils . get_file ( fname = \"aclImdb.tar.gz\" , origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" , extract = True , ) # set path to dataset IMDB_DATADIR = os . path . join ( os . path . dirname ( dataset ), \"aclImdb\" ) classes = [ \"pos\" , \"neg\" ] train_data = load_files ( os . path . join ( IMDB_DATADIR , \"train\" ), shuffle = True , categories = classes ) test_data = load_files ( os . path . join ( IMDB_DATADIR , \"test\" ), shuffle = False , categories = classes ) x_train = np . array ( train_data . data ) y_train = np . array ( train_data . target ) x_test = np . array ( test_data . data ) y_test = np . array ( test_data . target ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # this film was just brilliant casting The second step is to run the TextClassifier . As a quick demo, we set epochs to 2. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the text classifier. clf = ak . TextClassifier ( overwrite = True , max_trials = 1 ) # It only tries 1 model as a quick demo. # Feed the text classifier with training data. clf . fit ( x_train , y_train , epochs = 2 ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , epochs = 2 , # Use your own validation set. validation_data = ( x_val , y_val ), ) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of TextClassifier . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use Embedding for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . TextInput () output_node = ak . TextBlock ( block_type = \"ngram\" )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 2 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . Embedding ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 2 ) Data Format The AutoKeras TextClassifier is quite flexible for the data format. For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. We also support using tf.data.Dataset format for the training data. train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) . batch ( 32 ) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) . batch ( 32 ) clf = ak . TextClassifier ( overwrite = True , max_trials = 2 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set , epochs = 2 ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set )) Reference TextClassifier , AutoModel , TextBlock , TextToInteSequence , Embedding , TextToNgramVector , ConvBlock , TextInput , ClassificationHead .","title":"Text Classification"},{"location":"tutorial/text_classification/#a-simple-example","text":"The first step is to prepare your data. Here we use the IMDB dataset as an example. dataset = tf . keras . utils . get_file ( fname = \"aclImdb.tar.gz\" , origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" , extract = True , ) # set path to dataset IMDB_DATADIR = os . path . join ( os . path . dirname ( dataset ), \"aclImdb\" ) classes = [ \"pos\" , \"neg\" ] train_data = load_files ( os . path . join ( IMDB_DATADIR , \"train\" ), shuffle = True , categories = classes ) test_data = load_files ( os . path . join ( IMDB_DATADIR , \"test\" ), shuffle = False , categories = classes ) x_train = np . array ( train_data . data ) y_train = np . array ( train_data . target ) x_test = np . array ( test_data . data ) y_test = np . array ( test_data . target ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # this film was just brilliant casting The second step is to run the TextClassifier . As a quick demo, we set epochs to 2. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the text classifier. clf = ak . TextClassifier ( overwrite = True , max_trials = 1 ) # It only tries 1 model as a quick demo. # Feed the text classifier with training data. clf . fit ( x_train , y_train , epochs = 2 ) # Predict with the best model. predicted_y = clf . predict ( x_test ) # Evaluate the best model with testing data. print ( clf . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/text_classification/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. clf . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] clf . fit ( x_train , y_train , epochs = 2 , # Use your own validation set. validation_data = ( x_val , y_val ), )","title":"Validation Data"},{"location":"tutorial/text_classification/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of TextClassifier . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use Embedding for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . TextInput () output_node = ak . TextBlock ( block_type = \"ngram\" )( input_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 2 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . Embedding ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . ClassificationHead ()( output_node ) clf = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) clf . fit ( x_train , y_train , epochs = 2 )","title":"Customized Search Space"},{"location":"tutorial/text_classification/#data-format","text":"The AutoKeras TextClassifier is quite flexible for the data format. For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s. We also support using tf.data.Dataset format for the training data. train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) . batch ( 32 ) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) . batch ( 32 ) clf = ak . TextClassifier ( overwrite = True , max_trials = 2 ) # Feed the tensorflow Dataset to the classifier. clf . fit ( train_set , epochs = 2 ) # Predict with the best model. predicted_y = clf . predict ( test_set ) # Evaluate the best model with testing data. print ( clf . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/text_classification/#reference","text":"TextClassifier , AutoModel , TextBlock , TextToInteSequence , Embedding , TextToNgramVector , ConvBlock , TextInput , ClassificationHead .","title":"Reference"},{"location":"tutorial/text_regression/","text":"View in Colab GitHub source ! pip install autokeras import os import numpy as np import tensorflow as tf from sklearn.datasets import load_files import autokeras as ak To make this tutorial easy to follow, we just treat IMDB dataset as a regression dataset. It means we will treat prediction targets of IMDB dataset, which are 0s and 1s as numerical values, so that they can be directly used as the regression targets. A Simple Example The first step is to prepare your data. Here we use the IMDB dataset as an example. dataset = tf . keras . utils . get_file ( fname = \"aclImdb.tar.gz\" , origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" , extract = True , ) # set path to dataset IMDB_DATADIR = os . path . join ( os . path . dirname ( dataset ), \"aclImdb\" ) classes = [ \"pos\" , \"neg\" ] train_data = load_files ( os . path . join ( IMDB_DATADIR , \"train\" ), shuffle = True , categories = classes ) test_data = load_files ( os . path . join ( IMDB_DATADIR , \"test\" ), shuffle = False , categories = classes ) x_train = np . array ( train_data . data ) y_train = np . array ( train_data . target ) x_test = np . array ( test_data . data ) y_test = np . array ( test_data . target ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> The second step is to run the TextRegressor . As a quick demo, we set epochs to 2. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the text regressor. reg = ak . TextRegressor ( overwrite = True , max_trials = 10 # It tries 10 different models. ) # Feed the text regressor with training data. reg . fit ( x_train , y_train , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( x_test ) # Evaluate the best model with testing data. print ( reg . evaluate ( x_test , y_test )) Validation Data By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. reg . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] reg . fit ( x_train , y_train , epochs = 2 , # Use your own validation set. validation_data = ( x_val , y_val ), ) Customized Search Space For advanced users, you may customize your search space by using AutoModel instead of TextRegressor . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use Embedding for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . TextInput () output_node = ak . TextBlock ( block_type = \"ngram\" )( input_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . Embedding ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 ) Data Format The AutoKeras TextRegressor is quite flexible for the data format. For the text, the input data should be one-dimensional For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray. We also support using tf.data.Dataset format for the training data. train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) . batch ( 32 ) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) . batch ( 32 ) reg = ak . TextRegressor ( overwrite = True , max_trials = 2 ) # Feed the tensorflow Dataset to the regressor. reg . fit ( train_set , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( test_set ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_set )) Reference TextRegressor , AutoModel , TextBlock , TextToInteSequence , Embedding , TextToNgramVector , ConvBlock , TextInput , RegressionHead .","title":"Text Regression"},{"location":"tutorial/text_regression/#a-simple-example","text":"The first step is to prepare your data. Here we use the IMDB dataset as an example. dataset = tf . keras . utils . get_file ( fname = \"aclImdb.tar.gz\" , origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" , extract = True , ) # set path to dataset IMDB_DATADIR = os . path . join ( os . path . dirname ( dataset ), \"aclImdb\" ) classes = [ \"pos\" , \"neg\" ] train_data = load_files ( os . path . join ( IMDB_DATADIR , \"train\" ), shuffle = True , categories = classes ) test_data = load_files ( os . path . join ( IMDB_DATADIR , \"test\" ), shuffle = False , categories = classes ) x_train = np . array ( train_data . data ) y_train = np . array ( train_data . target ) x_test = np . array ( test_data . data ) y_test = np . array ( test_data . target ) print ( x_train . shape ) # (25000,) print ( y_train . shape ) # (25000, 1) print ( x_train [ 0 ][: 50 ]) # <START> this film was just brilliant casting <UNK> The second step is to run the TextRegressor . As a quick demo, we set epochs to 2. You can also leave the epochs unspecified for an adaptive number of epochs. # Initialize the text regressor. reg = ak . TextRegressor ( overwrite = True , max_trials = 10 # It tries 10 different models. ) # Feed the text regressor with training data. reg . fit ( x_train , y_train , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( x_test ) # Evaluate the best model with testing data. print ( reg . evaluate ( x_test , y_test ))","title":"A Simple Example"},{"location":"tutorial/text_regression/#validation-data","text":"By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage. reg . fit ( x_train , y_train , # Split the training data and use the last 15% as validation data. validation_split = 0.15 , ) You can also use your own validation set instead of splitting it from the training data with validation_data . split = 5000 x_val = x_train [ split :] y_val = y_train [ split :] x_train = x_train [: split ] y_train = y_train [: split ] reg . fit ( x_train , y_train , epochs = 2 , # Use your own validation set. validation_data = ( x_val , y_val ), )","title":"Validation Data"},{"location":"tutorial/text_regression/#customized-search-space","text":"For advanced users, you may customize your search space by using AutoModel instead of TextRegressor . You can configure the TextBlock for some high-level configurations, e.g., vectorizer for the type of text vectorization method to use. You can use 'sequence', which uses TextToInteSequence to convert the words to integers and use Embedding for embedding the integer sequences, or you can use 'ngram', which uses TextToNgramVector to vectorize the sentences. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail. input_node = ak . TextInput () output_node = ak . TextBlock ( block_type = \"ngram\" )( input_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 ) The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.[some_block]([block_args])(input_node) . You can even also use more fine grained blocks to customize the search space even further. See the following example. input_node = ak . TextInput () output_node = ak . TextToIntSequence ()( input_node ) output_node = ak . Embedding ()( output_node ) # Use separable Conv layers in Keras. output_node = ak . ConvBlock ( separable = True )( output_node ) output_node = ak . RegressionHead ()( output_node ) reg = ak . AutoModel ( inputs = input_node , outputs = output_node , overwrite = True , max_trials = 1 ) reg . fit ( x_train , y_train , epochs = 2 )","title":"Customized Search Space"},{"location":"tutorial/text_regression/#data-format","text":"The AutoKeras TextRegressor is quite flexible for the data format. For the text, the input data should be one-dimensional For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray. We also support using tf.data.Dataset format for the training data. train_set = tf . data . Dataset . from_tensor_slices ((( x_train ,), ( y_train ,))) . batch ( 32 ) test_set = tf . data . Dataset . from_tensor_slices ((( x_test ,), ( y_test ,))) . batch ( 32 ) reg = ak . TextRegressor ( overwrite = True , max_trials = 2 ) # Feed the tensorflow Dataset to the regressor. reg . fit ( train_set , epochs = 2 ) # Predict with the best model. predicted_y = reg . predict ( test_set ) # Evaluate the best model with testing data. print ( reg . evaluate ( test_set ))","title":"Data Format"},{"location":"tutorial/text_regression/#reference","text":"TextRegressor , AutoModel , TextBlock , TextToInteSequence , Embedding , TextToNgramVector , ConvBlock , TextInput , RegressionHead .","title":"Reference"},{"location":"tutorial/timeseries_forecaster/","text":"View in Colab GitHub source ! pip install autokeras import pandas as pd import tensorflow as tf import autokeras as ak To make this tutorial easy to follow, we use the UCI Airquality dataset, and try to forecast the AH value at the different timesteps. Some basic preprocessing has also been performed on the dataset as it required cleanup. A Simple Example The first step is to prepare your data. Here we use the UCI Airquality dataset as an example. dataset = tf . keras . utils . get_file ( fname = \"AirQualityUCI.csv\" , origin = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/\" \"AirQualityUCI.zip\" , extract = True , ) dataset = pd . read_csv ( dataset , sep = \";\" ) dataset = dataset [ dataset . columns [: - 2 ]] dataset = dataset . dropna () dataset = dataset . replace ( \",\" , \".\" , regex = True ) val_split = int ( len ( dataset ) * 0.7 ) data_train = dataset [: val_split ] validation_data = dataset [ val_split :] data_x = data_train [ [ \"CO(GT)\" , \"PT08.S1(CO)\" , \"NMHC(GT)\" , \"C6H6(GT)\" , \"PT08.S2(NMHC)\" , \"NOx(GT)\" , \"PT08.S3(NOx)\" , \"NO2(GT)\" , \"PT08.S4(NO2)\" , \"PT08.S5(O3)\" , \"T\" , \"RH\" , ] ] . astype ( \"float64\" ) data_x_val = validation_data [ [ \"CO(GT)\" , \"PT08.S1(CO)\" , \"NMHC(GT)\" , \"C6H6(GT)\" , \"PT08.S2(NMHC)\" , \"NOx(GT)\" , \"PT08.S3(NOx)\" , \"NO2(GT)\" , \"PT08.S4(NO2)\" , \"PT08.S5(O3)\" , \"T\" , \"RH\" , ] ] . astype ( \"float64\" ) # Data with train data and the unseen data from subsequent time steps. data_x_test = dataset [ [ \"CO(GT)\" , \"PT08.S1(CO)\" , \"NMHC(GT)\" , \"C6H6(GT)\" , \"PT08.S2(NMHC)\" , \"NOx(GT)\" , \"PT08.S3(NOx)\" , \"NO2(GT)\" , \"PT08.S4(NO2)\" , \"PT08.S5(O3)\" , \"T\" , \"RH\" , ] ] . astype ( \"float64\" ) data_y = data_train [ \"AH\" ] . astype ( \"float64\" ) data_y_val = validation_data [ \"AH\" ] . astype ( \"float64\" ) print ( data_x . shape ) # (6549, 12) print ( data_y . shape ) # (6549,) The second step is to run the TimeSeriesForecaster . As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. predict_from = 1 predict_until = 10 lookback = 3 clf = ak . TimeseriesForecaster ( lookback = lookback , predict_from = predict_from , predict_until = predict_until , max_trials = 1 , objective = \"val_loss\" , ) # Train the TimeSeriesForecaster with train data clf . fit ( x = data_x , y = data_y , validation_data = ( data_x_val , data_y_val ), batch_size = 32 , epochs = 10 , ) # Predict with the best model(includes original training data). predictions = clf . predict ( data_x_test ) print ( predictions . shape ) # Evaluate the best model with testing data. print ( clf . evaluate ( data_x_val , data_y_val ))","title":"TimeSeriesForecaster"},{"location":"tutorial/timeseries_forecaster/#a-simple-example","text":"The first step is to prepare your data. Here we use the UCI Airquality dataset as an example. dataset = tf . keras . utils . get_file ( fname = \"AirQualityUCI.csv\" , origin = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/\" \"AirQualityUCI.zip\" , extract = True , ) dataset = pd . read_csv ( dataset , sep = \";\" ) dataset = dataset [ dataset . columns [: - 2 ]] dataset = dataset . dropna () dataset = dataset . replace ( \",\" , \".\" , regex = True ) val_split = int ( len ( dataset ) * 0.7 ) data_train = dataset [: val_split ] validation_data = dataset [ val_split :] data_x = data_train [ [ \"CO(GT)\" , \"PT08.S1(CO)\" , \"NMHC(GT)\" , \"C6H6(GT)\" , \"PT08.S2(NMHC)\" , \"NOx(GT)\" , \"PT08.S3(NOx)\" , \"NO2(GT)\" , \"PT08.S4(NO2)\" , \"PT08.S5(O3)\" , \"T\" , \"RH\" , ] ] . astype ( \"float64\" ) data_x_val = validation_data [ [ \"CO(GT)\" , \"PT08.S1(CO)\" , \"NMHC(GT)\" , \"C6H6(GT)\" , \"PT08.S2(NMHC)\" , \"NOx(GT)\" , \"PT08.S3(NOx)\" , \"NO2(GT)\" , \"PT08.S4(NO2)\" , \"PT08.S5(O3)\" , \"T\" , \"RH\" , ] ] . astype ( \"float64\" ) # Data with train data and the unseen data from subsequent time steps. data_x_test = dataset [ [ \"CO(GT)\" , \"PT08.S1(CO)\" , \"NMHC(GT)\" , \"C6H6(GT)\" , \"PT08.S2(NMHC)\" , \"NOx(GT)\" , \"PT08.S3(NOx)\" , \"NO2(GT)\" , \"PT08.S4(NO2)\" , \"PT08.S5(O3)\" , \"T\" , \"RH\" , ] ] . astype ( \"float64\" ) data_y = data_train [ \"AH\" ] . astype ( \"float64\" ) data_y_val = validation_data [ \"AH\" ] . astype ( \"float64\" ) print ( data_x . shape ) # (6549, 12) print ( data_y . shape ) # (6549,) The second step is to run the TimeSeriesForecaster . As a quick demo, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs. predict_from = 1 predict_until = 10 lookback = 3 clf = ak . TimeseriesForecaster ( lookback = lookback , predict_from = predict_from , predict_until = predict_until , max_trials = 1 , objective = \"val_loss\" , ) # Train the TimeSeriesForecaster with train data clf . fit ( x = data_x , y = data_y , validation_data = ( data_x_val , data_y_val ), batch_size = 32 , epochs = 10 , ) # Predict with the best model(includes original training data). predictions = clf . predict ( data_x_test ) print ( predictions . shape ) # Evaluate the best model with testing data. print ( clf . evaluate ( data_x_val , data_y_val ))","title":"A Simple Example"}]}